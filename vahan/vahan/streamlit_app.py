# =====================================================
# üöó VAHAN ULTRA MASTER DASHBOARD ‚Äî ALL MAXED EDITION
# =====================================================
import os, sys, io, json, time, logging, platform, traceback, random
from datetime import date, datetime, timedelta
from functools import wraps
from zoneinfo import ZoneInfo

import numpy as np
import pandas as pd
import streamlit as st

# =============================
# üìö Cleaned & Consolidated Imports
# =============================

# ---------- Standard Library ----------
import io
import json
import time
import random
import traceback
from datetime import date, timedelta

# ---------- Third-Party Libraries ----------
import numpy as np
import pandas as pd
import requests
import altair as alt
from dotenv import load_dotenv

# ============================================================
# üöÄ VAHAN ALL-MAXED MODE ‚Äî Full Power Imports (No Limits)
# ============================================================

# ---------- Standard Library ----------
import os
import sys
import io
import re
import math
import csv
import json
import time
import random
import string
import socket
import base64
import hashlib
import logging
import zipfile
import traceback
import datetime
import statistics
import itertools
import concurrent.futures
from functools import lru_cache, partial, reduce
from datetime import date, datetime, timedelta
from urllib.parse import urlencode, quote, unquote

# ---------- Third-Party Core ----------
import numpy as np
import pandas as pd
import requests
import altair as alt
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from dotenv import load_dotenv

# ---------- Machine Learning / Forecasting ----------
from sklearn.ensemble import IsolationForest, RandomForestRegressor
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from prophet import Prophet

# ---------- Excel / OpenPyXL ----------
from openpyxl import Workbook, load_workbook
from openpyxl.styles import Font, Alignment, PatternFill, Border, Side
from openpyxl.utils import get_column_letter
from openpyxl.chart import (
    LineChart, BarChart, PieChart, Reference, Series
)

# ---------- Visualization Addons ----------
import plotly.io as pio
pio.templates.default = "plotly_white"
sns.set_theme(style="whitegrid")

# ---------- Utilities ----------
from colorama import Fore, Style, init as colorama_init
from rich.console import Console
from rich.table import Table
from rich.progress import Progress

import os, sys

# ---- Safe Colorama initialization ----
try:
    # On Streamlit Cloud or restricted environments, disable colorama wrapping
    if "STREAMLIT_SERVER" in os.environ or "streamlit" in sys.modules:
        # Avoid recursion by forcing no wrap
        import colorama
        colorama.deinit()  # reset any existing wrapping
        sys.stdout = sys.__stdout__
        sys.stderr = sys.__stderr__
        colorama.init(strip=True, convert=False, autoreset=True)
    else:
        # Normal local mode
        import colorama
        colorama.init(autoreset=True)

except Exception as e:
    # Disable colorama completely if it fails
    sys.stdout = sys.__stdout__
    sys.stderr = sys.__stderr__
    print("‚ö†Ô∏è Console color initialization disabled:", e)

# ---------- Local VAHAN Package (ALL IMPORTS) ----------
from vahan.api import *
from vahan.parsing import *
from vahan.metrics import *
from vahan.charts import *

# ============================================================
# üöÄ ALL-MAXED GLOBAL INITIALIZATION BLOCK (v2.0)
# ============================================================import os
import sys
import random
import time
import json
import math
import logging
import requests
import numpy as np
import pandas as pd
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
from datetime import date, datetime, timedelta
from dotenv import load_dotenv
from colorama import Fore, Style, init as colorama_init

# Optional AI + ML + Forecasting Modules
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import r2_score, mean_squared_error
from prophet import Prophet

# Excel + Visualization + Utils
from openpyxl import load_workbook
from io import BytesIO


# Initialize color output for cross-platform logs
colorama_init(autoreset=True)

# ---------- Warnings + Pandas Config ----------
warnings_filter = __import__("warnings")
warnings_filter.filterwarnings("ignore")
pd.set_option("display.max_columns", None)
pd.set_option("display.max_rows", 1000)
pd.set_option("display.width", 200)
pd.set_option("display.float_format", "{:,.4f}".format)
np.set_printoptions(suppress=True)

# ---------- Load Environment ----------
load_dotenv()
TODAY = date.today()
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)

# ---------- Logging Setup ----------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)-8s | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    handlers=[logging.StreamHandler(sys.stdout)]
)
# ============================================================
# üöÄ Streamlit Page Configuration (Must Come First)
# ============================================================
# ---------- Streamlit Config (MUST COME FIRST) ----------
st.set_page_config(
    page_title="üöó Vahan Master Ultra ‚Äî ALL-MAXED Mode",
    page_icon="üöÄ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ---------- Print Startup Summary ----------
print(f"\n{Fore.GREEN}‚úÖ ALL-MAXED ENVIRONMENT READY ‚Äî FULL FEATURE SET ENABLED{Style.RESET_ALL}")
print(f"{Fore.CYAN}üì¶ Modules Loaded: numpy, pandas, streamlit, plotly, sklearn, prophet, openpyxl, requests, dotenv, logging{Style.RESET_ALL}")
print(f"{Fore.YELLOW}üß† Mode: Developer | Analyst | Research ‚Äî unrestricted ALL-MAXED imports{Style.RESET_ALL}")
print(f"{Fore.MAGENTA}üïí Today: {TODAY} | Seed: {RANDOM_SEED}{Style.RESET_ALL}")
print(f"{Fore.BLUE}üìÅ Working Dir: {os.getcwd()}{Style.RESET_ALL}\n")

# ---------- Initialize Empty DataFrame ----------
df = pd.DataFrame()

# =====================================================
# ‚ö° VAHAN ALL-MAXED ULTRA+ BOOT ENGINE (v4.0)
# =====================================================
import os, sys, time, platform, logging, threading, psutil
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
import streamlit as st
from colorama import Fore, Style, init as colorama_init
from pathlib import Path
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler


# =====================================================
# üåè COLOR + GLOBAL INIT
# =====================================================
colorama_init(autoreset=True)
APP_NAME = "üöó Parivahan Analytics"
APP_VERSION = "vMAX-ULTRA"
APP_ENV = "Production"
LOG_DIR = "logs"
os.makedirs(LOG_DIR, exist_ok=True)

# =====================================================
# ‚öôÔ∏è LOGGING ‚Äî ULTRA-MAXED (IST + Colors + Rotation + Compatibility)
# =====================================================
import os
import sys
import logging
from datetime import datetime
from colorama import Fore, Style, init as colorama_init
from zoneinfo import ZoneInfo

# Initialize colorama for Windows terminals
colorama_init(autoreset=True)

# Ensure log directory exists
LOG_DIR = os.path.join(os.getcwd(), "logs")
os.makedirs(LOG_DIR, exist_ok=True)

# =====================================================
# üïí IST LOGGING HELPERS
# =====================================================
def log_ist(msg: str, level: str = "INFO", color: str = Fore.CYAN):
    """Print a timestamped message in IST timezone with color."""
    try:
        ist_time = datetime.now(ZoneInfo("Asia/Kolkata")).strftime("%Y-%m-%d %I:%M:%S %p")
        print(f"{color}[IST {ist_time}] [{level.upper()}] {msg}{Style.RESET_ALL}")
    except Exception as e:
        print(f"[IST Logging Fallback] {msg} (Error: {e})")

class ISTFormatter(logging.Formatter):
    """Custom logging formatter that formats times in IST."""
    def converter(self, timestamp):
        return datetime.fromtimestamp(timestamp, ZoneInfo("Asia/Kolkata"))
    def formatTime(self, record, datefmt=None):
        dt = self.converter(record.created)
        return dt.strftime(datefmt or "%Y-%m-%d %H:%M:%S")

# =====================================================
# üß† GLOBAL LOGGING CONFIGURATION (Dual Channel)
# =====================================================
def setup_global_logging():
    """Configure root logger with color console + rotating file (daily)."""
    log_file = os.path.join(LOG_DIR, "vahan_ultra.log")
    formatter = ISTFormatter("%(asctime)s | %(levelname)-8s | %(message)s")

    root = logging.getLogger()
    root.setLevel(logging.INFO)

    # Clear existing handlers
    for h in list(root.handlers):
        root.removeHandler(h)

    # --- Console Handler
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(formatter)
    root.addHandler(ch)

    # --- File Handler (daily rotation)
    from logging.handlers import TimedRotatingFileHandler
    fh = TimedRotatingFileHandler(log_file, when="midnight", backupCount=10, encoding="utf-8")
    fh.setFormatter(formatter)
    root.addHandler(fh)

    # Notify success
    log_ist("‚úÖ Logging initialized with daily rotation", "INFO", Fore.GREEN)

# Initialize logging on import
setup_global_logging()

def _get_ist_now():
    """Return current IST timestamp string safely."""
    try:
        return datetime.now(ZoneInfo("Asia/Kolkata")).strftime("%Y-%m-%d %I:%M:%S %p")
    except Exception:
        return datetime.now().strftime("%Y-%m-%d %I:%M:%S")

def log(msg: str, level: str = "INFO"):
    """
    Safe, backward-compatible logger.
    Uses internal _get_ist_now() to avoid name collision with user vars.
    """
    try:
        ts = _get_ist_now()
    except Exception:
        ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    level = str(level).upper().strip()
    prefix = f"[IST {ts}] [{level}]"
    color_map = {
        "INFO": Fore.CYAN,
        "WARNING": Fore.YELLOW,
        "ERROR": Fore.RED,
        "DEBUG": Fore.BLUE,
        "SUCCESS": Fore.GREEN,
    }
    color = color_map.get(level, Fore.WHITE)

    try:
        print(f"{color}{prefix} {msg}{Style.RESET_ALL}")
        logging.log(getattr(logging, level, logging.INFO), msg)
    except Exception as e:
        print(f"[LOGGING FAILSAFE] {msg} (Error: {e})")
        
# =====================================================
# üíª SYSTEM DIAGNOSTICS
# =====================================================
def system_diagnostics():
    """Collect and print system-level metrics."""
    try:
        cpu_usage = psutil.cpu_percent(interval=0.8)
        mem = psutil.virtual_memory()
        boot_time = datetime.fromtimestamp(psutil.boot_time(), ZoneInfo("Asia/Kolkata"))
        uptime = datetime.now(ZoneInfo("Asia/Kolkata")) - boot_time
        log_ist(f"üíª OS: {platform.system()} {platform.release()} | Python {platform.python_version()}", "INFO", Fore.MAGENTA)
        log_ist(f"üß† CPU: {cpu_usage:.1f}% | Memory: {mem.percent}% of {round(mem.total / (1024**3), 2)} GB", "INFO", Fore.MAGENTA)
        log_ist(f"‚è±Ô∏è Uptime: {str(timedelta(seconds=int(uptime.total_seconds())))}", "INFO", Fore.MAGENTA)
    except Exception as e:
        log_ist(f"‚ö†Ô∏è Diagnostics failed: {e}", "ERROR", Fore.RED)

system_diagnostics()

# =====================================================
# üöÄ BOOT BANNER
# =====================================================
def app_boot_banner(app_name=APP_NAME, version=APP_VERSION, env=APP_ENV):
    """Display an animated startup banner in Streamlit."""
    ist_time = datetime.now(ZoneInfo("Asia/Kolkata")).strftime("%Y-%m-%d %I:%M:%S %p")
    python_ver = platform.python_version()
    streamlit_ver = st.__version__
    sys_os = platform.system()
    cpu_count = os.cpu_count()
    cwd = os.getcwd()

    gradient = "linear-gradient(90deg,#0072ff,#00c6ff)"
    shadow = "0 4px 20px rgba(0,0,0,0.25)"
    st.markdown(f"""
    <div style='background:{gradient};color:white;padding:18px 26px;
         border-radius:18px;margin:20px 0 30px 0;box-shadow:{shadow};
         font-family:monospace;line-height:1.6;'>
        <h3>üåç {app_name} ‚Äî {version}</h3>
        üïí <b>Boot Time:</b> {ist_time} (IST)<br>
        ‚öôÔ∏è <b>Environment:</b> {env} | Python {python_ver} | Streamlit {streamlit_ver}<br>
        üíª OS: {sys_os} | CPU Cores: {cpu_count}<br>
        üìÅ Working Dir: {cwd}
    </div>
    """, unsafe_allow_html=True)

app_boot_banner()

# =====================================================
# üéâ FIRST LAUNCH EVENT
# =====================================================
if "launched" not in st.session_state:
    st.session_state.launched = True
    st.toast("üöÄ Welcome to VAHAN ULTRA MAX Edition!", icon="üåç")
    st.balloons()
    log_ist("üéâ Streamlit App Launched Successfully", "INFO", Fore.GREEN)

# =====================================================
# üîÅ AUTO FILE WATCHER (DEV MODE)
# =====================================================
class AutoReloadHandler(FileSystemEventHandler):
    """Auto-refresh Streamlit when .py files change (Dev mode only)."""
    def __init__(self, watch_paths):
        self.watch_paths = watch_paths
        self.last_reload = time.time()

    def on_any_event(self, event):
        if not event.src_path.endswith(".py"):
            return
        if time.time() - self.last_reload < 2:  # avoid storm
            return
        self.last_reload = time.time()
        file_name = os.path.basename(event.src_path)
        log_ist(f"‚ôªÔ∏è Detected change in {file_name} ‚Äî triggering rerun", "INFO", Fore.YELLOW)
        try:
            st.toast(f"üîÑ Auto-reloading due to {file_name}", icon="‚öôÔ∏è")
            st.rerun()
        except Exception as e:
            log_ist(f"‚ö†Ô∏è Reload failed: {e}", "ERROR", Fore.RED)

import os
from colorama import Fore

def start_auto_reload(watch_dir="."):
    """Start a background observer for live reload in development."""
    try:
        handler = AutoReloadHandler([watch_dir])
        observer = Observer()
        observer.schedule(handler, watch_dir, recursive=True)
        observer.daemon = True
        observer.start()
        log_ist(f"üëÄ Auto-reload active ‚Äî watching {os.path.abspath(watch_dir)}", "INFO", Fore.CYAN)
        return observer
    except Exception as e:
        log_ist(f"‚ö†Ô∏è Auto-reload disabled: {e}", "WARNING", Fore.YELLOW)
        return None

# --- Prevent re-init ---
if "watchdog_started" not in st.session_state:
    observer = start_auto_reload(".")
    st.session_state.watchdog_started = True

# =====================================================
# üß© HEARTBEAT MONITOR THREAD
# =====================================================
def start_heartbeat(interval=600):
    """Thread that logs periodic heartbeat messages."""
    def _loop():
        while True:
            log_ist("üíì Heartbeat: System running fine.", "INFO", Fore.BLUE)
            time.sleep(interval)

    t = threading.Thread(target=_loop, daemon=True)
    t.start()
    log_ist("üíì Heartbeat thread started", "INFO", Fore.BLUE)

if "heartbeat" not in st.session_state:
    start_heartbeat(interval=900)  # every 15 minutes
    st.session_state.heartbeat = True

# =====================================================
# ‚úÖ FINAL BOOT MESSAGE
# =====================================================
log_ist("‚úÖ VAHAN ALL-MAXED ULTRA+ Boot Complete", "INFO", Fore.GREEN)
st.toast("‚úÖ All Systems Ready ‚Äî Full Power Mode üîã", icon="‚úÖ")

# =====================================================
# ‚öôÔ∏è DYNAMIC SIDEBAR ‚Äî ALL-MAXED ULTRA (v3.0)
# =====================================================
import os
import json
import time
import random
import platform
from datetime import date, datetime
from pathlib import Path
import streamlit as st
import pandas as pd

# ---------- SAFE DEFAULTS ----------
STORAGE_DIR = Path.home() / ".vahan"
STORAGE_DIR.mkdir(parents=True, exist_ok=True)
PRESETS_FILE = STORAGE_DIR / "sidebar_presets.json"

if "init_done" not in st.session_state:
    st.session_state["init_done"] = True
    st.session_state.setdefault("session_seed", random.randint(100000, 999999))
    st.session_state.setdefault("user_cookie", f"user_{random.randint(1000,9999)}")
    # sidebar defaults
    st.session_state.setdefault("from_year", None)
    st.session_state.setdefault("to_year", None)
    st.session_state.setdefault("time_period", "Yearly")
    st.session_state.setdefault("fitness_check", False)
    st.session_state.setdefault("enable_forecast", True)
    st.session_state.setdefault("enable_anomaly", True)
    st.session_state.setdefault("enable_cluster", True)
    st.session_state.setdefault("enable_ai", False)
    st.session_state.setdefault("auto_refresh", True)
    st.session_state.setdefault("watchdog_enabled", True)

# Toggle to hide/show entire sidebar UI (keeps state)
SHOW_SIDEBAR_UI = st.session_state.get("show_sidebar", False)

# ---------- small helper utilities ----------
def safe_load_presets():
    try:
        if PRESETS_FILE.exists():
            with PRESETS_FILE.open("r", encoding="utf-8") as fh:
                return json.load(fh)
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Could not load presets: {e}")
    return {}

def safe_save_presets(presets: dict):
    try:
        with PRESETS_FILE.open("w", encoding="utf-8") as fh:
            json.dump(presets, fh, indent=2)
        return True
    except Exception as e:
        st.error(f"üí• Failed to save presets: {e}")
        return False

def to_df(json_obj, label_keys=("label",), value_key="value"):
    # Compatible with many API shapes (keeps original behavior)
    if isinstance(json_obj, dict) and "labels" in json_obj and "data" in json_obj:
        labels = json_obj["labels"]
        values = json_obj["data"]
        min_len = min(len(labels), len(values))
        return pd.DataFrame({"label": labels[:min_len], "value": values[:min_len]})
    data = json_obj.get("data", json_obj) if isinstance(json_obj, dict) else json_obj
    if isinstance(data, dict):
        data = [data]
    rows = []
    for item in data:
        label = None
        for k in label_keys:
            if k in item:
                label = item[k]
                break
        if label is None and label_keys:
            label = item.get(label_keys[0], None)
        value = item.get(value_key, None)
        if value is None:
            for vk in ("count", "value", "total"):
                if vk in item:
                    value = item[vk]
                    break
        if label is not None and value is not None:
            rows.append({"label": label, "value": value})
    return pd.DataFrame(rows)

# ---------- compact CSS (keeps app neat when sidebar hidden) ----------
HIDE_SIDEBAR_CSS = """
<style>
section[data-testid="stSidebar"] {display: none !important;}
div[data-testid="collapsedControl"] {display: none !important;}
.stButton>button {min-height: 36px;}
.sidebar-title {font-weight:700; font-size:16px; margin-bottom:6px;}
.compact-input .stNumberInput>div>div>input {height:34px;}
</style>
"""
if not SHOW_SIDEBAR_UI:
    st.markdown(HIDE_SIDEBAR_CSS, unsafe_allow_html=True)

# ---------- Bootstrap / header ----------
today = date.today()
default_from_year = today.year - 1

# ---------- Load presets ----------
_presets = safe_load_presets()
preset_names = sorted(list(_presets.keys()))

# =====================================================
# ‚öôÔ∏è SIDEBAR CONTROL PANEL ‚Äî ULTRA MAXED v4.0
# =====================================================
import streamlit as st
import time, platform
from datetime import date

today = date.today()
default_from_year = 2015

# -----------------------------------------------------
# üß© SAFE SESSION INITIALIZATION (once only)
# -----------------------------------------------------
def init_state(key, default):
    if key not in st.session_state:
        st.session_state[key] = default

init_state("show_sidebar", True)
init_state("session_seed", int(time.time()))
init_state("user_cookie", f"user_{int(time.time())}")
init_state("from_year", default_from_year)
init_state("to_year", today.year)
init_state("state_code", "")
init_state("rto_code", "")
init_state("vehicle_classes", "")
init_state("vehicle_makers", "")
init_state("time_period", "Yearly")
init_state("fitness_check", False)
init_state("vehicle_type", "")
init_state("enable_forecast", True)
init_state("enable_anomaly", True)
init_state("enable_cluster", True)
init_state("enable_ai", False)
init_state("base_url", "https://analytics.parivahan.gov.in")
init_state("timeout", 30)
init_state("export_format", "CSV")
init_state("auto_refresh", True)
init_state("enable_beta", False)

# -----------------------------------------------------
# üöÄ SIDEBAR UI (wrapped for safety)
# -----------------------------------------------------
try:
    st.sidebar.markdown("<div class='sidebar-title'>‚öôÔ∏è Control Panel ‚Äî ULTRA MAXED</div>", unsafe_allow_html=True)
    st.sidebar.markdown("---")

    # Top control buttons
    top_col1, top_col2 = st.sidebar.columns([3, 1])
    with top_col1:
        if st.sidebar.button("üîÅ Toggle Sidebar UI"):
            st.session_state["show_sidebar"] = not st.session_state.get("show_sidebar", True)
            st.toast("üîÄ Sidebar toggled")
            st.rerun()

    with top_col2:
        if st.sidebar.button("üßπ Clear Session"):
            keep = {k: st.session_state[k] for k in ["session_seed", "user_cookie"] if k in st.session_state}
            st.session_state.clear()
            st.session_state.update(keep)
            st.toast("üßπ Session cleared & preserved essentials")
            st.rerun()

    # -------------------------------------------------
    # üìä DATA FILTERS
    # -------------------------------------------------
    # ---------- Data Filters ----------
    with st.sidebar.expander("üìä Data Filters", expanded=True):
        st.markdown("### Time range")

        # ‚úÖ Always fall back to integers (never None)
        default_from_year = st.session_state.get("from_year", today.year - 1) or (today.year - 3)
        default_to_year = st.session_state.get("to_year", today.year) or today.year

        from_year = st.sidebar.number_input("From Year", min_value=2012, max_value=today.year, value=default_from_year)
        to_year = st.sidebar.number_input("To Year", min_value=from_year, max_value=today.year, value=today.year)
        state_code = st.sidebar.text_input("State Code (blank=All-India)", value="")
        rto_code = st.sidebar.text_input("RTO Code (0=aggregate)", value="0")
        vehicle_classes = st.sidebar.text_input("Vehicle Classes (e.g., 2W,3W,4W if accepted)", value="")
        vehicle_makers = st.sidebar.text_input("Vehicle Makers (comma-separated or IDs)", value="")
        time_period = st.sidebar.selectbox("Time Period", options=[0,1,2], index=0)
        fitness_check = st.sidebar.selectbox("Fitness Check", options=[0,1], index=0)
        vehicle_type = st.sidebar.text_input("Vehicle Type (optional)", value="")


    # -------------------------------------------------
    # üß† AI / FORECASTING
    # -------------------------------------------------
    with st.sidebar.expander("üß† AI / Forecasting", expanded=False):
        enable_forecast = st.checkbox("üìà Forecasting", st.session_state.enable_forecast, key="input_enable_forecast")
        enable_anomaly = st.checkbox("‚ö†Ô∏è Anomaly Detection", st.session_state.enable_anomaly, key="input_enable_anomaly")
        enable_cluster = st.checkbox("üîç Clustering", st.session_state.enable_cluster, key="input_enable_cluster")
        enable_ai = st.checkbox("ü§ñ AI Narratives", st.session_state.enable_ai, key="input_enable_ai")

        st.session_state.update({
            "enable_forecast": enable_forecast,
            "enable_anomaly": enable_anomaly,
            "enable_cluster": enable_cluster,
            "enable_ai": enable_ai
        })

        if enable_forecast and enable_anomaly:
            st.caption("üìä Forecasts will include anomaly markers.")
        if enable_cluster:
            st.caption("üß© Cluster model adapts dynamically.")
        if enable_ai:
            st.caption("üí¨ AI narratives enabled ‚Äî generating insights...")

        if st.button("üîí Lock AI Settings"):
            st.session_state["_ai_locked"] = True
            st.toast("üß† AI config locked for this session")

    # -------------------------------------------------
    # üß≠ API & Experimental Settings
    # -------------------------------------------------
    with st.sidebar.expander("üß≠ API & Reports", expanded=False):
        st.text_input("üåê API Base URL", st.session_state.base_url, key="input_base_url")
        st.slider("‚è≥ API Timeout (s)", 5, 120, st.session_state.timeout, key="input_timeout")
        st.selectbox("üì§ Export Format", ["CSV", "XLSX", "PDF"],
                     index=["CSV", "XLSX", "PDF"].index(st.session_state.export_format),
                     key="input_export_format")
        st.checkbox("‚ôªÔ∏è Auto-refresh on data change", value=st.session_state.auto_refresh, key="input_auto_refresh")

    with st.sidebar.expander("üß™ Experimental", expanded=False):
        enable_beta = st.checkbox("üß™ Enable Experimental Features", st.session_state.enable_beta, key="input_enable_beta")
        st.session_state.enable_beta = enable_beta
        if enable_beta:
            st.info("Experimental mode ON ‚Äî unstable features enabled")

    # -------------------------------------------------
    # üíæ Preset Management
    # -------------------------------------------------
    with st.sidebar.expander("üíæ Presets", expanded=False):
        presets = safe_load_presets()
        if presets:
            sel = st.selectbox("Load preset", ["-- none --"] + list(sorted(presets.keys())), index=0, key="preset_select")
            if sel and sel != "-- none --":
                if st.button("‚ñ∂Ô∏è Apply Selected Preset"):
                    p = presets[sel]
                    for k, v in p.items():
                        st.session_state[k] = v
                    st.toast(f"Applied preset '{sel}'")
                    time.sleep(0.3)
                    st.rerun()

            del_name = st.text_input("Delete preset name", value="", key="preset_delname_input")
            if st.button("üóëÔ∏è Delete Preset"):
                dn = del_name.strip()
                if dn and dn in presets:
                    del presets[dn]
                    safe_save_presets(presets)
                    st.toast(f"Deleted preset '{dn}'")
                else:
                    st.error("Preset not found")
        else:
            st.caption("No presets saved yet ‚Äî use 'Save Preset' above.")

    # -------------------------------------------------
    # üßæ Footer Info
    # -------------------------------------------------
    st.sidebar.markdown("---")
    st.sidebar.caption(f"üß© User: `{st.session_state.user_cookie}` | üîë Seed: `{st.session_state.session_seed}`")
    st.sidebar.caption(f"üì¶ Python: {platform.python_version()} | Streamlit: {st.__version__}")

except Exception as e:
    st.sidebar.error(f"‚ö†Ô∏è Auto-recovered from sidebar error: {e}")
    import traceback
    traceback.print_exc()
    for k in ["preset_name_input", "preset_select", "preset_delname_input"]:
        if k in st.session_state:
            del st.session_state[k]
    time.sleep(0.2)
    st.rerun()

# =====================================================
# üîÅ LIVE AUTO-REFRESH (Reactive Engine ‚Äî ULTRA MAXED v3.0)
# =====================================================
import streamlit as st
import threading
import time
from datetime import datetime, timedelta

# Ensure proper init
if "auto_refresh" not in st.session_state:
    st.session_state["auto_refresh"] = True
if "auto_refresh_interval" not in st.session_state:
    st.session_state["auto_refresh_interval"] = 120  # seconds
if "auto_refresh_thread" not in st.session_state:
    st.session_state["auto_refresh_thread"] = None
if "auto_refresh_last" not in st.session_state:
    st.session_state["auto_refresh_last"] = datetime.now()
if "auto_refresh_stop" not in st.session_state:
    st.session_state["auto_refresh_stop"] = False


# ---------- REFRESH FUNCTION ----------
def _auto_refresh_loop(interval=120):
    """Runs background refresh loop safely."""
    while not st.session_state.get("auto_refresh_stop", False):
        time.sleep(interval)
        st.session_state["_auto_refresh_trigger"] = datetime.now().isoformat()
        st.session_state["auto_refresh_last"] = datetime.now()
        # Toast works only when visible, so guard with try
        try:
            st.toast("üîÅ Auto-refresh triggered!", icon="üïí")
        except Exception:
            pass
        st.rerun()


# ---------- CONTROL PANEL (VISIBLE AT TOP OR FOOTER) ----------
with st.sidebar.expander("üïí Auto-Refresh Control", expanded=False):
    st.markdown("### üîÑ Live Data Auto-Refresh")
    st.caption("Keep your dashboard continuously synced with the latest data.")

    c1, c2 = st.columns([3, 1])
    with c1:
        interval = st.slider("‚è≥ Refresh Interval (sec)", 30, 600,
                             st.session_state.get("auto_refresh_interval", 120),
                             step=30, key="auto_refresh_interval")
    with c2:
        st.caption(" ")

    # Start/Stop buttons
    c3, c4 = st.columns(2)
    with c3:
        if st.button("‚ñ∂Ô∏è Start Auto-Refresh"):
            st.session_state["auto_refresh_stop"] = False
            if st.session_state.get("auto_refresh_thread") is None or not st.session_state["auto_refresh_thread"]:
                thread = threading.Thread(
                    target=_auto_refresh_loop,
                    args=(st.session_state["auto_refresh_interval"],),
                    daemon=True
                )
                thread.start()
                st.session_state["auto_refresh_thread"] = True
                st.toast(f"üîÅ Auto-refresh started ({interval}s interval).", icon="üü¢")
    with c4:
        if st.button("‚èπÔ∏è Stop Auto-Refresh"):
            st.session_state["auto_refresh_stop"] = True
            st.session_state["auto_refresh_thread"] = None
            st.toast("‚èπÔ∏è Auto-refresh stopped.", icon="üî¥")

    # Show status and time since last refresh
    if not st.session_state.get("auto_refresh_stop", False):
        elapsed = (datetime.now() - st.session_state.get("auto_refresh_last", datetime.now())).seconds
        next_refresh_in = st.session_state.get("auto_refresh_interval", 120) - elapsed
        if next_refresh_in < 0:
            next_refresh_in = 0
        st.markdown(f"**üïí Next refresh in:** `{next_refresh_in}s`")
        st.progress(max(0, min(1, 1 - elapsed / st.session_state.get("auto_refresh_interval", 120))))
    else:
        st.info("‚è∏Ô∏è Auto-refresh paused.")


# ---------- AUTO START (if enabled) ----------
if st.session_state.get("auto_refresh", True) and not st.session_state.get("auto_refresh_stop", False):
    if "auto_refresh_thread" not in st.session_state or not st.session_state["auto_refresh_thread"]:
        thread = threading.Thread(
            target=_auto_refresh_loop,
            args=(st.session_state["auto_refresh_interval"],),
            daemon=True
        )
        thread.start()
        st.session_state["auto_refresh_thread"] = True
        st.toast(f"üîÅ Auto-refresh active every {st.session_state['auto_refresh_interval']}s (ULTRA-MAXED).", icon="üïí")

# =====================================================
# üöÄ VAHAN ANALYTICS API ENGINE ‚Äî ALL-MAXED PURE v7.0
# =====================================================

# =====================================================
# üß© STREAMLIT SAFE IMPORT ‚Äî ALL-MAXED
# =====================================================
import os, re, json, time, random, logging, pickle, requests
from datetime import datetime
from urllib.parse import urlencode
from zoneinfo import ZoneInfo
from typing import Optional, Dict, Any
from contextlib import nullcontext

# =====================================================
# üß© STREAMLIT SAFE IMPORT
# =====================================================
try:
    import streamlit as st
except ImportError:
    class st:
        @staticmethod
        def expander(*_, **__): return nullcontext()
        @staticmethod
        def json(_): pass
        @staticmethod
        def spinner(_): return nullcontext()
        @staticmethod
        def warning(msg): print("[WARNING]", msg)
        @staticmethod
        def error(msg): print("[ERROR]", msg)



# =====================================================
# ‚öôÔ∏è CONSTANTS
# =====================================================
BASE_URL = "https://analytics.parivahan.gov.in/analytics/publicdashboard"
MAX_RETRIES = 4
DEFAULT_TIMEOUT = 60

# =====================================================
# üïí UTILITIES ‚Äî LOGGING + TIME
# =====================================================
def ist_now() -> str:
    return datetime.now(ZoneInfo("Asia/Kolkata")).strftime("%Y-%m-%d %I:%M:%S %p")

def log(msg: str, level: str = "INFO"):
    prefix = f"[IST {ist_now()}] [{level}]"
    line = f"{prefix} {msg}"
    color = {
        "INFO": "\033[94m",
        "WARNING": "\033[93m",
        "ERROR": "\033[91m",
        "SUCCESS": "\033[92m",
    }.get(level, "\033[0m")
    print(color + line + "\033[0m")
    try:
        if level == "ERROR":
            st.error(line)
        elif level == "WARNING":
            st.warning(line)
    except Exception:
        pass

# =====================================================
# üß∞ CLEAN STRING
# =====================================================
def clean_str(v: str) -> str:
    if not v: return ""
    return re.sub(r"[^A-Za-z0-9_\- ,./]", "", str(v).strip())

# =====================================================
# üß© PARAMETER BUILDER (NO PREFILL)
# =====================================================
def build_params(
    from_year: int,
    to_year: int,
    *,
    state_code: str,
    rto_code: str,
    vehicle_classes: str,
    vehicle_makers: str,
    time_period: str,
    fitness_check: bool,
    vehicle_type: str,
    extra_params: Optional[dict] = None,
) -> dict:
    """Strict builder ‚Äî no prefilled defaults."""
    current_year = datetime.now().year
    errors = []

    # strict checks
    if from_year is None or to_year is None:
        errors.append("Both from_year and to_year must be provided.")
    if not isinstance(from_year, int) or not isinstance(to_year, int):
        errors.append("Year values must be integers.")
    if from_year > to_year:
        errors.append(f"From year ({from_year}) cannot exceed To year ({to_year}).")
    if from_year < 2000 or to_year > current_year:
        errors.append(f"Year range must be between 2000 and {current_year}.")
    if not isinstance(fitness_check, bool):
        errors.append("Fitness flag must be boolean (True/False).")
    if not all([state_code, rto_code, vehicle_classes, vehicle_makers, time_period, vehicle_type]):
        errors.append("All core parameters must be explicitly provided ‚Äî no blanks allowed.")

    if errors:
        for e in errors:
            log(f"‚ùå Parameter Error: {e}", "ERROR")
        raise ValueError(" | ".join(errors))

    time_period = time_period.title().strip()
    if time_period not in ["Yearly", "Quarterly", "Monthly"]:
        log(f"‚ö†Ô∏è Invalid time_period '{time_period}', defaulting to 'Yearly'", "WARNING")
        time_period = "Yearly"

    params = {
        "from_year": from_year,
        "to_year": to_year,
        "state_cd": clean_str(state_code),
        "rto_cd": clean_str(rto_code),
        "vclass": clean_str(vehicle_classes),
        "maker": clean_str(vehicle_makers),
        "time_period": time_period,
        "include_fitness": "Y" if fitness_check else "N",
        "veh_type": clean_str(vehicle_type),
        "_session_seed": datetime.now().strftime("%Y%m%d%H%M%S"),
    }

    if extra_params:
        for k, v in extra_params.items():
            if v not in [None, "", [], {}]:
                params[k] = v

    params["_meta"] = {
        "created": ist_now(),
        "validated": True,
        "safe_hash": abs(hash(json.dumps(params, sort_keys=True))) % 1_000_000,
    }

    log(f"üß© Params built successfully ‚Üí hash {params['_meta']['safe_hash']}", "SUCCESS")

    try:
        with st.expander("üîç VAHAN Parameter Summary", expanded=False):
            st.json(params)
    except Exception:
        pass

    return params


import streamlit as st
from datetime import datetime
from zoneinfo import ZoneInfo

# =====================================================
# üé® GLOBAL STYLE + HEADER
# =====================================================
st.markdown("""
<style>
html, body, [class*="css"] {
    font-family: 'Inter', sans-serif;
}
h1, h2, h3 {
    font-weight: 700 !important;
}
.metric-card {
    background: linear-gradient(135deg, #004e92, #000428);
    color: white;
    padding: 1.2rem;
    border-radius: 1rem;
    text-align: center;
    box-shadow: 0 6px 18px rgba(0,0,0,0.25);
    transition: transform 0.2s ease-in-out;
}
.metric-card:hover {
    transform: translateY(-3px);
    box-shadow: 0 10px 25px rgba(0,0,0,0.35);
}
.refresh-indicator {
    animation: blink 1.5s infinite alternate;
    color: #00eaff;
}
@keyframes blink {
    from { opacity: 0.4; }
    to { opacity: 1; }
}
hr.glow {
    border: none;
    height: 2px;
    background: linear-gradient(90deg, #00c6ff, #0072ff);
    border-radius: 2px;
    margin: 2rem 0;
}
footer {
    text-align: center;
    opacity: 0.8;
    font-size: 13px;
    padding: 1.5rem 0;
}
footer small {
    font-size: 11px;
    color: #aaa;
}
</style>
""", unsafe_allow_html=True)

# =====================================================
# üöó HEADER
# =====================================================
st.markdown("""
<div style='text-align:center;margin-bottom:2rem;'>
  <h1>üöó <b>Vahan Intelligence Dashboard</b></h1>
  <p>AI-Driven Analytics ‚Ä¢ Forecasts ‚Ä¢ Trends ‚Ä¢ All-India Transport Intelligence</p>
</div>
""", unsafe_allow_html=True)

# =====================================================
# üîÅ REFRESH INDICATOR (Fixed - no conflict with log())
# =====================================================
ist_refresh_time = datetime.now(ZoneInfo("Asia/Kolkata")).strftime("%I:%M:%S %p")
st.markdown(f"""
<div style='text-align:center;margin-bottom:1rem;font-size:14px;'>
  <span class='refresh-indicator'>üîÑ Auto-Refreshed at {ist_refresh_time} IST</span>
</div>
""", unsafe_allow_html=True)

# =====================================================
# üßæ FOOTER
# =====================================================
st.markdown("""
<footer>
  <hr class="glow">
  <div>
    ‚ú® <b>Vahan Intelligence Engine</b> ‚Äî AI-Augmented Dashboard<br>
    <small>¬© 2025 Transport Data Division ‚Ä¢ Auto-refresh Enabled</small>
  </div>
</footer>
""", unsafe_allow_html=True)

# =====================================================
# üéâ ONE-TIME LAUNCH TOAST
# =====================================================
if "dashboard_loaded" not in st.session_state:
    st.session_state["dashboard_loaded"] = True
    st.toast("üöÄ Real VAHAN Dashboard Loaded (All-Maxed Edition)", icon="üåà")
    st.balloons()

# =====================================================
# ü§ñ DEEPINFRA & UNIVERSAL AI ‚Äî ALL-MAXED CONNECTOR v2.0
# =====================================================
import os
import json
import time
import random
import requests
import traceback
from datetime import datetime
import streamlit as st
from dotenv import load_dotenv
from typing import Optional, Dict, Any

# -------------------------
# üîê Load secrets/env safely
# -------------------------
load_dotenv()  # load .env if present

def get_secret(key: str, default: Optional[str] = "") -> str:
    # priority: st.secrets -> env -> default
    try:
        v = st.secrets.get(key)
        if v:
            return v
    except Exception:
        pass
    return os.getenv(key, default) or default

# Primary keys / config
DEEPINFRA_API_KEY = get_secret("DEEPINFRA_API_KEY", "")
DEEPINFRA_MODEL = get_secret("DEEPINFRA_MODEL", "mistralai/Mixtral-8x7B-Instruct-v0.1")
DEEPINFRA_TIMEOUT = int(get_secret("DEEPINFRA_TIMEOUT", "8"))

AI_PROVIDER = get_secret("AI_PROVIDER", "deepinfra").lower()  # 'deepinfra' by default
AI_MODEL = get_secret("AI_MODEL", DEEPINFRA_MODEL if AI_PROVIDER == "deepinfra" else get_secret("OPENAI_MODEL","gpt-4o-mini"))
AI_API_KEY = get_secret("AI_API_KEY", "")  # generic key fallback
AI_TIMEOUT = int(get_secret("AI_TIMEOUT", "20"))

# endpoint mapping (basic)
ENDPOINTS = {
    "openai": "https://api.openai.com/v1/chat/completions",
    "deepinfra": "https://api.deepinfra.com/v1/openai/chat/completions",
    "anthropic": "https://api.anthropic.com/v1/messages",
    "groq": "https://api.groq.com/openai/v1/chat/completions",
    # 'gemini' usually uses Google client libs ‚Äî leaving generic root for reachability checks
    "gemini": "https://generativelanguage.googleapis.com/v1beta1/models",
}

AI_URL = ENDPOINTS.get(AI_PROVIDER, ENDPOINTS["openai"])

# =====================================================
# üß† SAFE SIDEBAR TOGGLE (ALL-MAXED v3 ‚Äî ULTRA EDITION)
# =====================================================
import streamlit as st
from datetime import datetime

# -----------------------------------------------------
# üé® Sidebar header
# -----------------------------------------------------
st.sidebar.markdown(
    """
    <div style='padding:8px 10px;
                border-radius:8px;
                background:linear-gradient(90deg,#0b3d91,#1a73e8);
                color:white;
                font-weight:700;
                text-shadow:0 0 6px rgba(255,255,255,0.3);'>
        ü§ñ DeepInfra / Universal AI Connector
    </div>
    """,
    unsafe_allow_html=True
)

# -----------------------------------------------------
# üß© Initialize State (Safe, Idempotent)
# -----------------------------------------------------
def init_state(key, default=None):
    if key not in st.session_state:
        st.session_state[key] = default

init_state("enable_ai", True)
init_state("last_ai_check", None)
init_state("ai_status", "idle")

# -----------------------------------------------------
# üß† Unified Toggle Component (Duplicate-Proof)
# -----------------------------------------------------
def safe_checkbox(label, session_key, default=True, help_text=None):
    """Safely create a checkbox synced with session_state (duplicate-safe)."""
    widget_key = f"{session_key}_widget"  # Unique key per widget
    val = st.sidebar.checkbox(label, key=widget_key, value=st.session_state.get(session_key, default), help=help_text)
    if val != st.session_state[session_key]:
        st.session_state[session_key] = val
    return val

# -----------------------------------------------------
# üöÄ AI Enable Toggle
# -----------------------------------------------------
enable_ai = safe_checkbox(
    "Enable AI",
    "enable_ai",
    default=True,
    help_text="Toggle DeepInfra / Universal AI connector",
)


# -----------------------------------------------------
# üìä Dynamic Status Display (Instant Feedback)
# -----------------------------------------------------
if enable_ai:
    st.session_state.ai_status = "active"
    status_color = "#16c784"
    emoji = "‚úÖ"
    status_text = "AI Enabled ‚Äî DeepInfra connector active."
else:
    st.session_state.ai_status = "disabled"
    status_color = "#f39c12"
    emoji = "‚ö†Ô∏è"
    status_text = "AI Disabled ‚Äî manual mode only."

# -----------------------------------------------------
# üïí Last AI Check Timestamp (optional tracking)
# -----------------------------------------------------
if enable_ai:
    st.session_state.last_ai_check = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# -----------------------------------------------------
# üí¨ Sidebar Visual Status Badge
# -----------------------------------------------------
st.sidebar.markdown(
    f"""
    <div style='margin-top:8px;
                padding:10px 12px;
                border-radius:10px;
                background:{status_color}20;
                border:1px solid {status_color};
                color:{status_color};
                font-weight:600;
                text-align:center;'>
        {emoji} {status_text}<br>
        <small style='color:#888;'>Last check: {st.session_state.last_ai_check or "‚Äî"}</small>
    </div>
    """,
    unsafe_allow_html=True
)
# -----------------------------------------------------
# üîß Future Expansion Example
# -----------------------------------------------------
# (add other connectors dynamically)
# st.sidebar.toggle("Enable OpenAI", key="enable_openai")
# st.sidebar.toggle("Enable HuggingFace", key="enable_hf")


# -------------------------
# üîå Ping / connectivity checks
# -------------------------
def _now_ts():
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

def ping_provider(provider: str = AI_PROVIDER, api_key: str = None, timeout: int = AI_TIMEOUT):
    """Ping provider root (non intrusive). Returns (status_code_or_str, latency_ms_or_None)."""
    url = AI_URL.split("/v1")[0] if "/" in AI_URL else AI_URL
    headers = {}
    if api_key:
        headers["Authorization"] = f"Bearer {api_key}"
    try:
        start = time.time()
        resp = requests.get(url, headers=headers, timeout=timeout)
        latency = round((time.time() - start) * 1000, 1)
        return resp.status_code, latency
    except requests.exceptions.Timeout:
        return "timeout", None
    except Exception as e:
        return f"error: {str(e)[:120]}", None

# show connectivity card
def render_provider_card():
    if not enable_ai:
        st.sidebar.info("AI Mode: Disabled")
        return

    key_to_show = AI_API_KEY or (DEEPINFRA_API_KEY if AI_PROVIDER == "deepinfra" else "")
    has_key = bool(key_to_show)
    if not has_key:
        st.sidebar.error("‚ùå No API key found. Add to st.secrets or environment (AI_API_KEY / DEEPINFRA_API_KEY).")
        return

    status, latency = ping_provider(api_key=key_to_show)
    st.session_state.last_ai_check = _now_ts()
    if status == 200:
        st.sidebar.success(f"‚úÖ {AI_PROVIDER.title()} reachable ‚Äî {latency} ms")
        st.sidebar.caption(f"Model: **{AI_MODEL}** | Last check: {st.session_state.last_ai_check}")
    elif status == "timeout":
        st.sidebar.error("‚è±Ô∏è Provider timed out")
    elif isinstance(status, int) and status == 401:
        st.sidebar.error("üö´ Unauthorized ‚Äî invalid API key")
    else:
        st.sidebar.warning(f"‚ö†Ô∏è Status: {status} | Last check: {st.session_state.last_ai_check}")

render_provider_card()
st.sidebar.markdown("---")

# -------------------------
# üß† Session chat memory
# -------------------------
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# -------------------------
# ‚öôÔ∏è Universal Chat (safe, streaming-friendly)
# -------------------------
def universal_chat(
    system_prompt: str,
    user_prompt: str,
    *,
    model: Optional[str] = None,
    provider: Optional[str] = None,
    api_key: Optional[str] = None,
    stream: bool = True,
    temperature: float = 0.2,
    max_tokens: int = 512,
    retries: int = 3,
    timeout: int = AI_TIMEOUT,
) -> Dict[str, Any]:
    """
    Returns dict with keys: {ok:bool, text:str, error:Optional[str]}
    Streamed incremental updates are shown in-place (st.empty()).
    """
    provider = (provider or AI_PROVIDER).lower()
    model = model or AI_MODEL
    api_key = api_key or (AI_API_KEY if AI_API_KEY else (DEEPINFRA_API_KEY if provider=="deepinfra" else None))
    url = ENDPOINTS.get(provider, ENDPOINTS["openai"])
    headers = {"Content-Type": "application/json"}
    if api_key:
        headers["Authorization"] = f"Bearer {api_key}"

    # Build payload for OpenAI-style endpoints (DeepInfra compatible)
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        "temperature": float(temperature),
        "max_tokens": int(max_tokens),
        "stream": bool(stream),
    }

    # Fallback adjustments for Anthropic style or provider differences could be added here
    attempt = 0
    placeholder = st.empty()
    last_text = ""

    while attempt < retries:
        attempt += 1
        try:
            with requests.post(url, headers=headers, json=payload, stream=stream, timeout=timeout) as r:
                if r.status_code != 200:
                    # Show a helpful short message
                    snippet = r.text[:400].strip().replace("\n"," ")
                    placeholder.warning(f"‚ö†Ô∏è Provider responded {r.status_code}: {snippet}")
                    # For 4xx/5xx, may retry depending on code
                    if 400 <= r.status_code < 500:
                        return {"ok": False, "error": f"HTTP {r.status_code}", "text": ""}
                    # else try again after backoff
                    raise RuntimeError(f"HTTP {r.status_code} - {snippet}")

                if stream:
                    # stream lines (OpenAI/DeepInfra style)
                    for raw in r.iter_lines(decode_unicode=True):
                        if not raw:
                            continue
                        # typical stream: 'data: {...}'
                        line = raw.strip()
                        if line.startswith("data:"):
                            payload_line = line[len("data:"):].strip()
                        else:
                            payload_line = line
                        if payload_line == "[DONE]":
                            break
                        try:
                            obj = json.loads(payload_line)
                            # try common locations for delta/content
                            choices = obj.get("choices") or []
                            if choices:
                                delta = choices[0].get("delta", {})
                                chunk = delta.get("content") or delta.get("text") or ""
                                if chunk:
                                    last_text += chunk
                                    placeholder.markdown(f"**ü§ñ AI:** {last_text}")
                        except Exception:
                            # not JSON or unknown chunk ‚Äî append raw
                            last_text += payload_line
                            placeholder.markdown(f"**ü§ñ AI:** {last_text}")

                    # finish
                    st.session_state.chat_history.append({"user": user_prompt, "ai": last_text, "ts": _now_ts()})
                    st.toast("‚úÖ AI response complete", icon="ü§ñ")
                    return {"ok": True, "text": last_text, "error": None}

                else:
                    # Non-streaming path
                    data = r.json()
                    # try to extract the main text in common schema
                    text = ""
                    if "choices" in data:
                        try:
                            text = data["choices"][0]["message"]["content"]
                        except Exception:
                            text = json.dumps(data)[:1000]
                    else:
                        text = json.dumps(data)[:1000]
                    placeholder.markdown(f"**ü§ñ AI:** {text}")
                    st.session_state.chat_history.append({"user": user_prompt, "ai": text, "ts": _now_ts()})
                    return {"ok": True, "text": text, "error": None}

        except Exception as e:
            backoff = (2 ** attempt) + random.random()
            placeholder.error(f"‚ö†Ô∏è Attempt {attempt} failed: {str(e)[:120]}. Retrying in {backoff:.1f}s...")
            time.sleep(backoff)
            continue

    # final failure
    placeholder.error("‚õî AI request failed after retries.")
    return {"ok": False, "text": "", "error": "failed after retries"}

# -------------------------
# üß© Utility: insight generator (non-blocking UX)
# -------------------------
def universal_insight(df, topic="dataset"):
    """
    Generate a short insight summary about the DataFrame.
    This uses a small sample and runs the chat (may be slow depending on provider).
    """
    if df is None or getattr(df, "empty", True):
        st.warning("No data available for AI insight.")
        return
    sample_md = df.head(8).to_markdown(index=False)
    prompt = f"You're a senior data analyst. Given this sample of {topic} (markdown table):\n\n{sample_md}\n\nProvide a concise summary (3 bullets): key trends, notable anomalies, and 2 short recommendations."
    return universal_chat("You are a data analytics expert.", prompt, stream=True, max_tokens=300)

# -------------------------
# üß™ Test / UI helpers
# -------------------------
def universal_test_ui():
    st.subheader("üß™ AI Connector Test")
    if enable_ai:
        masked = (AI_API_KEY or DEEPINFRA_API_KEY) or ""
        masked_display = f"{masked[:4]}...{masked[-4:]}" if masked else "‚Äî"
        st.info(f"Provider: **{AI_PROVIDER}** | Model: **{AI_MODEL}** | Key: `{masked_display}`")

        if st.button("üîó Ping Provider"):
            status, latency = ping_provider()
            if latency:
                st.success(f"Reachable ‚Äî {status} | {latency} ms")
            else:
                st.warning(f"Status: {status}")
    else:
        st.info("AI Mode is currently disabled in sidebar.")

    with st.expander("üí¨ Quick test prompt"):
        prompt = st.text_area("Prompt", "Summarize the state of registrations given a short table sample.")
        if st.button("üöÄ Run test prompt"):
            if prompt.strip():
                universal_chat("You are a helpful assistant.", prompt, stream=True, max_tokens=240)
            else:
                st.warning("Enter a prompt first.")

# Safe helper
def set_query_param(key, value):
    try:
        st.query_params[key] = value  # Streamlit ‚â•1.40
    except TypeError:
        st.experimental_set_query_params(**{key: value})  # Older versions

def get_query_param(key, default=None):
    try:
        return st.query_params.get(key, default)  # Streamlit ‚â•1.40
    except TypeError:
        return st.experimental_get_query_params().get(key, [default])[0]

# Use safely
if st.sidebar.button("Open AI Test Panel"):
    set_query_param("_open_ai_test", "1")

if get_query_param("_open_ai_test"):
    universal_test_ui()

# -------------------------
# ‚úÖ End of ALL-MAXED AI connector
# -------------------------

# import plotly.express as px
# import altair as alt
# import matplotlib.pyplot as plt

# # ---------- Fetch & display category + makers ----------------------------------
# st.subheader("üìä Vehicle Categories & Top Makers")

# try:
#     with st.spinner("Fetching Category distribution..."):
#         cat_json, cat_url = get_json("vahandashboard/categoriesdonutchart", params)
#         df_cat = to_df(cat_json)

#     col1, col2 = st.columns([2, 3])

#     # ---- Category distribution ----
#     with col1:
#         st.markdown("### üöó Category Distribution")

#         if not df_cat.empty:
#             try:
#                 fig = px.bar(
#                     df_cat,
#                     x="label",
#                     y="value",
#                     color="label",
#                     title="Vehicle Category Distribution",
#                     text_auto=True,
#                 )
#                 fig.update_layout(
#                     xaxis_title="Category",
#                     yaxis_title="Count",
#                     showlegend=False,
#                     template="plotly_white",
#                 )
#                 st.plotly_chart(fig, use_container_width=True)

#             except Exception as e:
#                 st.warning(f"‚ö†Ô∏è Plotly failed ({e}). Falling back to Altair.")
#                 alt_chart = (
#                     alt.Chart(df_cat)
#                     .mark_bar()
#                     .encode(
#                         x="label:N",
#                         y="value:Q",
#                         color="label:N",
#                         tooltip=["label", "value"],
#                     )
#                     .properties(title="Vehicle Category Distribution")
#                 )
#                 st.altair_chart(alt_chart, use_container_width=True)
#         else:
#             st.info("‚ÑπÔ∏è No category data found for the given filters.")

# =========================================================
# üî• ALL-MAXED ‚Äî Category Analytics (multi-frequency, multi-year) ‚Äî MAXED BLOCK
# Drop-in Streamlit module. Replace or import into your app.
# Created: ALL-MAXED v2 ‚Äî resilient, instrumented, cached, mock-safe
# =========================================================
import time
import math
import json
import random
import logging
import requests
from typing import Any, Dict, List, Optional, Tuple
from datetime import datetime
from dateutil.relativedelta import relativedelta

import numpy as np
import pandas as pd
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go

print("‚úÖ Imports loaded successfully ‚Äî ALL-MAXED Category Analytics block initialized.")

logger = logging.getLogger("all_maxed_category")
if not logger.handlers:
    h = logging.StreamHandler()
    h.setFormatter(logging.Formatter("%(asctime)s | %(levelname)s | %(message)s"))
    logger.addHandler(h)
logger.setLevel(logging.DEBUG)

print("üß† Logger 'all_maxed_category' configured (level=DEBUG). Ready to log events.")

# =====================================================
# üöÄ ALL-MAXED ANALYTICS CORE v12.0
# -----------------------------------------------------
# Fully loaded mock data, chart builders, and analytics UI helpers
# =====================================================

import streamlit as st
import pandas as pd
import numpy as np
import random, uuid
import plotly.express as px
from datetime import datetime
from typing import Dict, Any
from plotly.colors import qualitative

print("‚úÖ Imports loaded successfully ‚Äî ALL-MAXED ANALYTICS CORE v12.0 initialized.")

# -----------------------------------------------------
# üéØ Master Category Reference
# -----------------------------------------------------
CATEGORIES_MASTER = [
    "Motorcycle", "Car", "Truck", "Bus", "Tractor",
    "E-Rickshaw", "Trailer", "Pickup", "Ambulance", "Taxi"
]

print(f"üì¶ Loaded {len(CATEGORIES_MASTER)} master categories:", CATEGORIES_MASTER)

# -----------------------------------------------------
# üíæ Deterministic Mock Data Generator (Multi-Frequency)
# -----------------------------------------------------
def deterministic_mock_categories(year: int, freq: str = "Monthly", seed_base: str = "categories") -> Dict[str, Any]:
    """Generate reproducible, realistic mock data for categories (daily, monthly, yearly)."""
    print(f"üß© Generating deterministic mock data for year={year}, freq={freq}, seed_base={seed_base}")
    rnd = random.Random(hash((year, seed_base)) & 0xFFFFFFFF)
    data = []

    if freq == "Yearly":
        print("üìÖ Mode: Yearly")
        for c in CATEGORIES_MASTER:
            val = rnd.randint(50_000, 2_500_000)
            data.append({"label": c, "value": val, "year": year})

    elif freq == "Monthly":
        print("üìÖ Mode: Monthly")
        for month in range(1, 13):
            for c in CATEGORIES_MASTER:
                base = rnd.randint(10_000, 200_000)
                seasonal_boost = 1.2 if month in [3, 9, 12] else 1.0
                value = int(base * seasonal_boost * (0.8 + rnd.random() * 0.5))
                data.append({
                    "label": c,
                    "value": value,
                    "year": year,
                    "month": month,
                    "month_name": datetime(year, month, 1).strftime("%b")
                })

    elif freq == "Daily":
        print("üìÖ Mode: Daily")
        for month in range(1, 13):
            for day in range(1, 29):
                for c in CATEGORIES_MASTER:
                    base = rnd.randint(200, 15000)
                    val = int(base * (0.8 + rnd.random() * 0.6))
                    data.append({
                        "label": c, "value": val, "year": year, "month": month, "day": day
                    })
    else:
        print(f"‚ö†Ô∏è Unknown frequency '{freq}' ‚Äî no data generated.")

    print(f"‚úÖ Generated {len(data)} records for {year} ({freq})")

    return {
        "data": data,
        "meta": {
            "generatedAt": datetime.utcnow().isoformat(),
            "note": f"deterministic mock for {year} ({freq})",
            "freq": freq
        }
    }

# -----------------------------------------------------
# ‚öôÔ∏è Formatting helpers
# -----------------------------------------------------
def format_number(n):
    """Return number formatted as K, M, or Cr."""
    print(f"üî¢ Formatting number: {n}")
    if n >= 10_000_000:
        formatted = f"{n/10_000_000:.2f} Cr"
    elif n >= 100_000:
        formatted = f"{n/100_000:.2f} L"
    elif n >= 1_000:
        formatted = f"{n/1_000:.2f} K"
    else:
        formatted = f"{n:,}"
    print(f"‚úÖ Formatted result: {formatted}")
    return formatted

# -----------------------------------------------------
# üé® Global Chart Style Settings
# -----------------------------------------------------
print("üé® Initializing Global Chart Style Settings...")

COLOR_PALETTE = qualitative.Plotly + qualitative.D3 + qualitative.Vivid
DEFAULT_TEMPLATE = "plotly_white"
TITLE_STYLE = dict(size=20, color="#111", family="Segoe UI Semibold")

print(f"‚úÖ COLOR_PALETTE loaded with {len(COLOR_PALETTE)} colors.")
print(f"‚úÖ DEFAULT_TEMPLATE: {DEFAULT_TEMPLATE}")
print(f"‚úÖ TITLE_STYLE: {TITLE_STYLE}")


# -----------------------------------------------------
# üß© MAXED CHART HELPERS (Legend, Hover, UI polish)
# -----------------------------------------------------
def _unique_key(prefix="chart"):
    key = f"{prefix}_{uuid.uuid4().hex[:6]}"
    print(f"üß© Generated unique key: {key}")
    return key


def bar_from_df(df: pd.DataFrame, title="Bar Chart", x="label", y="value",
                color=None, barmode="group", height=500, section_id="bar"):
    """Enhanced bar chart with full UX polish."""
    print(f"üìä [bar_from_df] Rendering bar chart: {title}")
    if df is None or df.empty:
        print("‚ö†Ô∏è [bar_from_df] Empty or missing DataFrame ‚Äî skipping chart.")
        st.warning("‚ö†Ô∏è No data to plot.")
        return

    print(f"‚úÖ [bar_from_df] Data shape: {df.shape}, Columns: {list(df.columns)}")

    fig = px.bar(
        df, x=x, y=y, color=color or x, text_auto=".2s",
        title=title, color_discrete_sequence=COLOR_PALETTE,
        barmode=barmode
    )
    fig.update_traces(
        hovertemplate="<b>%{x}</b><br>%{y:,.0f} registrations",
        textfont_size=12, textangle=0, cliponaxis=False
    )
    fig.update_layout(
        template=DEFAULT_TEMPLATE,
        title_font=TITLE_STYLE,
        xaxis_title=x.title(),
        yaxis_title=y.title(),
        legend=dict(
            orientation="h", yanchor="bottom", y=1.02,
            xanchor="right", x=1, title=None, bgcolor="rgba(0,0,0,0)"
        ),
        height=height, bargap=0.2, margin=dict(t=60, b=40, l=40, r=20)
    )
    print(f"‚úÖ [bar_from_df] Chart ready ‚Äî pushing to Streamlit section '{section_id}'.")
    st.plotly_chart(fig, use_container_width=True, key=_unique_key(section_id))


def pie_from_df(df: pd.DataFrame, title="Pie Chart", donut=True, section_id="pie", height=450):
    """Enhanced donut/pie chart with interactivity + auto legends."""
    print(f"ü•ß [pie_from_df] Rendering pie chart: {title}")
    if df is None or df.empty:
        print("‚ö†Ô∏è [pie_from_df] Empty or missing DataFrame ‚Äî skipping chart.")
        st.warning("‚ö†Ô∏è No data to plot.")
        return

    print(f"‚úÖ [pie_from_df] Data shape: {df.shape}, Labels: {df['label'].nunique()}")

    fig = px.pie(
        df, names="label", values="value", hole=0.45 if donut else 0.0,
        title=title, color_discrete_sequence=COLOR_PALETTE
    )
    fig.update_traces(
        textinfo="label+percent",
        hovertemplate="<b>%{label}</b><br>%{value:,.0f} registrations<br>%{percent}",
        pull=[0.03] * len(df),
    )
    fig.update_layout(
        template=DEFAULT_TEMPLATE,
        title_font=TITLE_STYLE,
        legend=dict(orientation="v", yanchor="top", y=0.95, xanchor="left", x=0),
        height=height, margin=dict(t=60, b=40, l=40, r=40)
    )
    print(f"‚úÖ [pie_from_df] Chart ready ‚Äî pushing to Streamlit section '{section_id}'.")
    st.plotly_chart(fig, use_container_width=True, key=_unique_key(section_id))


def trend_from_df(df: pd.DataFrame, title="Trend Over Time", section_id="trend", height=500):
    """Advanced line chart supporting animation + multiple years."""
    print(f"üìà [trend_from_df] Rendering trend chart: {title}")
    if df is None or df.empty:
        print("‚ö†Ô∏è [trend_from_df] Empty or missing DataFrame ‚Äî skipping chart.")
        st.warning("‚ö†Ô∏è No trend data available.")
        return

    print(f"‚úÖ [trend_from_df] Data shape: {df.shape}, Columns: {list(df.columns)}")

    if "month_name" in df.columns:
        print("üìÜ [trend_from_df] Applying month order for chronological axis.")
        df["month_order"] = pd.Categorical(
            df["month_name"], 
            categories=["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],
            ordered=True
        )

    fig = px.line(
        df, x="month_order" if "month_order" in df.columns else "year",
        y="value", color="label", markers=True,
        title=title, color_discrete_sequence=COLOR_PALETTE,
        line_shape="spline"
    )
    fig.update_traces(hovertemplate="<b>%{x}</b><br>%{y:,.0f} registrations")
    fig.update_layout(
        template=DEFAULT_TEMPLATE,
        title_font=TITLE_STYLE,
        legend=dict(
            orientation="h", yanchor="bottom", y=1.02,
            xanchor="right", x=1, bgcolor="rgba(0,0,0,0)"
        ),
        height=height, margin=dict(t=60, b=40, l=40, r=40)
    )
    print(f"‚úÖ [trend_from_df] Chart ready ‚Äî pushing to Streamlit section '{section_id}'.")
    st.plotly_chart(fig, use_container_width=True, key=_unique_key(section_id))

# -----------------------------------------------------
# üß† Auto Dashboard Section ‚Äî Single Function
# -----------------------------------------------------
def render_category_dashboard(year: int, freq="Monthly"):
    """Render full UI: fetch mock data, show KPI, bar, pie, trend ‚Äî ALL-MAXED."""
    print(f"\nüöÄ [render_category_dashboard] Starting dashboard render for {year} ({freq})")

    st.subheader(f"üìä Category Distribution ‚Äî {year} ({freq})")

    # Generate deterministic data
    print(f"üßÆ Generating deterministic mock data for year={year}, freq={freq}")
    mock_json = deterministic_mock_categories(year, freq=freq)
    df = pd.DataFrame(mock_json["data"])
    print(f"‚úÖ Mock data generated ‚Äî {len(df):,} rows, columns: {list(df.columns)}")

    # Compute key metrics
    total = df["value"].sum()
    top = df.sort_values("value", ascending=False).iloc[0]
    print(f"üèÜ Top Category: {top['label']} ‚Äî {format_number(top['value'])}")
    print(f"üìä Total Registrations: {format_number(total)}")

    st.success(f"üèÜ **Top Category:** {top['label']} ‚Äî {format_number(top['value'])} registrations")
    st.caption(f"Total: {format_number(total)} | Generated: {mock_json['meta']['generatedAt']}")

    # Layout 2-col + trend
    print("üß± Rendering charts layout (bar + pie + trend)...")
    c1, c2 = st.columns([2, 1])
    with c1:
        print("üìä Rendering BAR chart block...")
        bar_from_df(df, title=f"{year} {freq} Breakdown (Bar)", color="label", section_id=f"bar_{year}")
    with c2:
        print("ü•ß Rendering PIE chart block...")
        pie_from_df(df, title=f"{year} Share (Donut)", section_id=f"pie_{year}")

    # Optional trend
    if "month_name" in df.columns:
        print("üìà Detected monthly data ‚Äî rendering animated trend...")
        trend_from_df(df, title=f"{year} Monthly Trend (Animated)", section_id=f"trend_{year}")
    else:
        print("üìâ No monthly data ‚Äî rendering simple category trend...")
        trend_from_df(df, title=f"{year} Category Trend", section_id=f"trend_{year}")

    print(f"‚úÖ [render_category_dashboard] Completed dashboard for {year} ({freq})\n")
    return df


# ============================================================
# ‚öôÔ∏è Synthetic Timeseries Expansion ‚Äî MAXED ULTRA VERSION
# ------------------------------------------------------------
# Expands per-category annual totals into realistic timeseries
# with seasonality, trend variation, and reproducible randomness
# ============================================================

import numpy as np
import pandas as pd
from datetime import datetime

def year_to_timeseries(
    df_year: pd.DataFrame,
    year: int,
    freq: str = "Monthly",
    trend_strength: float = 0.15,
    noise_strength: float = 0.10,
    seasonal_boost: bool = True,
    seed_base: str = "timeseries"
) -> pd.DataFrame:
    """
    Expand per-category totals into an enhanced, realistic synthetic timeseries.
    """
    print(f"\n‚öôÔ∏è [year_to_timeseries] Start ‚Äî year={year}, freq={freq}, trend={trend_strength}, noise={noise_strength}")
    
    if df_year is None or df_year.empty:
        print("‚ö†Ô∏è Empty input dataframe ‚Äî returning empty timeseries.")
        return pd.DataFrame(columns=["ds","label","value","year","month","quarter","month_name"])

    # --- deterministic seed
    seed = abs(hash((year, freq, seed_base))) % (2**32)
    rng = np.random.default_rng(seed)
    print(f"üî¢ Seed initialized: {seed}")

    # --- index generation
    start = pd.Timestamp(f"{year}-01-01")
    end = pd.Timestamp(f"{year}-12-31")
    freq = freq.capitalize()

    if freq == "Daily":
        idx = pd.date_range(start=start, end=end, freq="D")
    elif freq == "Monthly":
        idx = pd.date_range(start=start, end=end, freq="M")
    elif freq == "Quarterly":
        idx = pd.date_range(start=start, end=end, freq="Q")
    else:
        idx = pd.date_range(start=start, end=end, freq="Y")

    n = len(idx)
    print(f"üóìÔ∏è Generated {n} periods from {start.date()} to {end.date()} ({freq})")

    rows = []

    def seasonal_factor(i):
        if not seasonal_boost:
            return 1.0
        return 1.0 + 0.25 * np.sin((i / n) * 2 * np.pi * 4)  # 4 seasonal peaks

    # --- iterate categories
    for _, r in df_year.iterrows():
        cat = r.get("label", "Unknown")
        total = float(r.get("value", 0.0))
        if total <= 0:
            continue

        print(f"üìà Expanding category '{cat}' ‚Äî total={total:,.0f}")
        base_per = total / max(1, n)

        # random trend (upward or downward)
        trend = np.linspace(1 - trend_strength, 1 + trend_strength, n)
        if rng.random() > 0.5:
            trend = trend[::-1]

        noise = rng.normal(0, noise_strength, n)
        vals = np.maximum(base_per * trend * (1 + noise), 0)

        for i, ts in enumerate(idx):
            factor = seasonal_factor(i)
            v = vals[i] * factor
            rows.append({
                "ds": ts,
                "label": cat,
                "value": float(v),
                "year": int(year),
                "month": ts.month,
                "quarter": ts.quarter,
                "month_name": ts.strftime("%b")
            })

    df_out = pd.DataFrame(rows)
    df_out["value"] = df_out["value"].round(2)
    print(f"‚úÖ Generated {len(df_out):,} records before normalization.")

    # --- normalize totals
    grouped = df_out.groupby("label")["value"].sum().to_dict()
    for cat in grouped:
        original_total = float(df_year.loc[df_year["label"] == cat, "value"].iloc[0])
        if grouped[cat] > 0:
            scale = original_total / grouped[cat]
            df_out.loc[df_out["label"] == cat, "value"] *= scale
            print(f"üîß Normalized '{cat}': scale={scale:.4f}")

    df_out.reset_index(drop=True, inplace=True)
    print(f"üèÅ [year_to_timeseries] Done ‚Äî final rows: {len(df_out):,}\n")
    return df_out

# ============================================================
# üöÄ ALL-MAXED ULTRA ‚Äî CATEGORY ANALYTICS (MULTI-YEAR)
# ============================================================

import streamlit as st
import pandas as pd
from colorama import Fore
import time

# ============================================================
# üöò CATEGORY FETCHER ‚Äî ALL-MAXED ULTRA + PRINT DEBUG
# ============================================================

import streamlit as st
import pandas as pd
import plotly.express as px
import numpy as np
import random
from datetime import datetime
from colorama import Fore

def fetch_year_category(year: int, params: dict, show_debug: bool = True) -> pd.DataFrame:
    """Fetch category donut for a given year and render local charts, insights, and summaries.
    ‚úÖ Always returns a non-empty DataFrame with ['label','value','year'].
    ‚úÖ Includes deterministic mock fallback and rich Plotly visualizations.
    ‚úÖ Now includes print-based debugging for CLI/log visibility.
    """

    # --- Prepare request ---
    st.markdown(f"## üìä Vehicle Categories ‚Äî {year}")
    print(f"\n[INFO] Fetching category data for year: {year}")
    print(f"[PARAMS] {params}")

    # --- Fetch safely ---
    try:
        cat_json, cat_url = get_json("vahandashboard/categoriesdonutchart", params)
        print(f"[SUCCESS] Data fetched from {cat_url}")
    except Exception as e:
        print(Fore.RED + f"[ERROR] get_json failed for {year}: {e}")
        cat_json, cat_url = deterministic_mock_categories(year), f"mock://categoriesdonutchart/{year}"
        print(f"[FALLBACK] Using deterministic mock for year {year}")

    # --- Debug panel ---
    if show_debug:
        with st.expander(f"üß© Debug JSON ‚Äî Categories {year}", expanded=False):
            st.write("**URL:**", cat_url)
            st.json(cat_json if isinstance(cat_json, (dict, list)) else str(cat_json))
    print(f"[DEBUG] Source URL: {cat_url}")

    # --- Normalize JSON to DataFrame ---
    try:
        df = to_df(cat_json)
        print(f"[SUCCESS] Converted JSON to DataFrame with {len(df)} rows.")
    except Exception as e:
        print(Fore.YELLOW + f"[WARNING] to_df failed for {year}: {e}")
        df = to_df(deterministic_mock_categories(year))

    if df is None or df.empty:
        print(Fore.YELLOW + f"[WARNING] Empty df for {year}, regenerating deterministic mock")
        df = to_df(deterministic_mock_categories(year))

    df = df.copy()
    df["year"] = int(year)

    # --- Data quality & totals ---
    df["value"] = pd.to_numeric(df["value"], errors="coerce").fillna(0)
    df = df.sort_values("value", ascending=False)
    total_reg = int(df["value"].sum())

    print(f"[INFO] Total registrations ({year}): {total_reg:,}")
    print(f"[INFO] Categories: {', '.join(df['label'].astype(str).tolist())}")

    st.caption(f"üîó **Source:** {cat_url}")
    st.markdown(f"**Total Registrations ({year}):** {total_reg:,}")

    
    # -----------------------------
    # Calculate stats for tooltips/annotations
    # -----------------------------
    total = df["value"].sum()
    mean_val = df["value"].mean()
    median_val = df["value"].median()
    
    # --- Charts layout ---
    c1, c2 = st.columns([1.8, 1.2])
    
    with c1:
        try:
            # -------------------------
            # Maxed Bar Chart
            # -------------------------
            fig_bar = px.bar(
                df,
                x="label",
                y="value",
                color="label",
                text_auto=".2s",
                title=f"üöó Category Distribution ‚Äî {year}",
                color_discrete_sequence=px.colors.qualitative.Safe,
            )
    
            # Add mean & median lines
            fig_bar.add_hline(y=mean_val, line_dash="dash", line_color="green",
                              annotation_text=f"Mean: {mean_val:.0f}", annotation_position="top left")
            fig_bar.add_hline(y=median_val, line_dash="dot", line_color="blue",
                              annotation_text=f"Median: {median_val:.0f}", annotation_position="bottom right")
    
            fig_bar.update_layout(
                template="plotly_white",
                showlegend=True,
                legend_title_text="Categories",
                margin=dict(t=80, b=50, l=50, r=50),
                title_font=dict(size=24, family="Segoe UI", color="#222"),
                height=500,
                xaxis_title="Category",
                yaxis_title="Registrations",
                bargap=0.25,
                hovermode="x unified",
            )
    
            # Custom hovertemplate
            fig_bar.update_traces(
                hovertemplate="<b>%{x}</b><br>Registrations: %{y:,}<br>Share: %{customdata[0]:.1%}",
                customdata=np.array(df["value"] / total).reshape(-1, 1),
                marker_line_width=1.5,
            )
    
            st.plotly_chart(fig_bar, use_container_width=True, key=f"bar_{year}")
    
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Bar chart failed: {e}")
            st.dataframe(df)
    
    with c2:
        try:
            # -------------------------
            # Maxed Pie Chart
            # -------------------------
            fig_pie = px.pie(
                df,
                names="label",
                values="value",
                hole=0.4,
                color_discrete_sequence=px.colors.qualitative.Vivid,
                title=f"Category Share ‚Äî {year}",
            )
    
            fig_pie.update_traces(
                textinfo="percent+label",
                hovertemplate="<b>%{label}</b><br>%{value:,} registrations<br>%{percent}",
                pull=[0.05]*len(df),
                marker=dict(line=dict(color='#ffffff', width=2))
            )
    
            # Add annotations for total, mean, median
            fig_pie.add_annotation(
                text=f"Total: {total:,}<br>Mean: {mean_val:.0f}<br>Median: {median_val:.0f}",
                x=0.5, y=-0.1, showarrow=False, font=dict(size=14, color="#555"), align="center"
            )
    
            fig_pie.update_layout(
                template="plotly_white",
                margin=dict(t=60, b=80, l=20, r=20),
                height=450,
                showlegend=True,
                legend_title_text="Categories",
                title_font=dict(size=22, family="Segoe UI", color="#222"),
            )
    
            st.plotly_chart(fig_pie, use_container_width=True, key=f"pie_{year}")
    
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Pie chart failed: {e}")
            st.dataframe(df)

    
    # -----------------------------
    # Calculate stats
    # -----------------------------
    total_reg = df["value"].sum()
    mean_val = df["value"].mean()
    median_val = df["value"].median()
    
    # --- Top category insight (maxed) ---
    try:
        top = df.iloc[0]
        pct = (top["value"] / total_reg) * 100 if total_reg else 0
        st.markdown(
            f"üèÜ <span style='font-size:20px; font-weight:bold; color:#2E86AB'>Top Category:</span> "
            f"<span style='font-size:22px; font-weight:bold;'>{top['label']}</span> ‚Äî "
            f"<span style='color:#27AE60;'>{int(top['value']):,}</span> registrations "
            f"(<span style='color:#D35400'>{pct:.1f}%</span>)",
            unsafe_allow_html=True,
        )
        st.info(f"‚ÑπÔ∏è Mean registrations: {int(mean_val):,}, Median registrations: {int(median_val):,}")
        print(f"[INFO] Top Category: {top['label']} ({pct:.1f}% share, {int(top['value']):,} units)")
    except Exception as e:
        print(f"[WARNING] Could not determine top category: {e}")
        st.warning("‚ö†Ô∏è Could not determine top category")
    
    # --- Extra insights table (maxed) ---
    df["share_%"] = (df["value"] / total_reg * 100).round(2)
    df_sorted = df.sort_values("share_%", ascending=False)
    st.dataframe(
        df_sorted.style.format({"value": "{:,.0f}", "share_%": "{:.2f}%"}).bar(
            subset=["share_%"], color="#4CAF50"
        ).highlight_max(subset=["value"], color="#F39C12").highlight_min(subset=["value"], color="#E74C3C"),
        use_container_width=True,
        height=350,
    )
    print("[INFO] Added data table with share percentages, max/min highlights.")
    
    # --- Minor animations / expansion: synthetic trend (maxed) ---
    with st.expander("üìà Synthetic Trend Simulation", expanded=True):
        df_ts = year_to_timeseries(df, year, freq="Monthly")  # Make sure this returns ['ds','label','value']
    
        # Maxed line chart
        fig_line = px.line(
            df_ts,
            x="ds",
            y="value",
            color="label",
            line_group="label",
            title=f"üìä Synthetic Monthly Trend ‚Äî {year}",
            markers=True,
            color_discrete_sequence=px.colors.qualitative.Set2,
        )
    
        # Add mean and median lines
        fig_line.add_hline(y=mean_val, line_dash="dash", line_color="green",
                           annotation_text=f"Mean: {mean_val:.0f}", annotation_position="top left")
        fig_line.add_hline(y=median_val, line_dash="dot", line_color="blue",
                           annotation_text=f"Median: {median_val:.0f}", annotation_position="bottom right")
    
        # Add shaded area for min/max per month
        min_max = df_ts.groupby("ds")["value"].agg(["min", "max"]).reset_index()
        fig_line.add_traces([
            go.Scatter(
                x=min_max["ds"].tolist() + min_max["ds"].tolist()[::-1],
                y=min_max["max"].tolist() + min_max["min"].tolist()[::-1],
                fill='toself',
                fillcolor='rgba(0,176,246,0.1)',
                line=dict(color='rgba(255,255,255,0)'),
                hoverinfo="skip",
                showlegend=False,
            )
        ])
    
        # Layout maxed
        fig_line.update_layout(
            template="plotly_white",
            legend=dict(orientation="h", yanchor="bottom", y=-0.3, xanchor="center", x=0.5),
            margin=dict(t=80, b=100, l=60, r=40),
            height=450,
            title_font=dict(size=22, family="Segoe UI", color="#222"),
            hovermode="x unified",
            xaxis_title="Month",
            yaxis_title="Registrations",
        )
    
        # Custom hover template
        fig_line.update_traces(
            hovertemplate="<b>%{fullData.name}</b><br>Date: %{x|%b %Y}<br>Registrations: %{y:,}"
        )
    
        st.plotly_chart(fig_line, use_container_width=True, key=f"trend_{year}")
        print(f"[INFO] Rendered synthetic monthly trend for {year}")

    print(f"[DONE] Completed fetch_year_category for {year}\n{'-'*60}")
    return df
    
# =====================================================
# -------------------------
# Main Streamlit UI ‚Äî All-Maxed Block
# -------------------------
# =====================================================
from typing import Optional

from typing import Optional

def all_maxed_category_block(params: Optional[dict] = None):
    """
    Render the ALL-MAXED category analytics block inside Streamlit.

    Parameters
    ----------
    params : dict, optional
        Dictionary of configuration parameters (e.g., filters or overrides).
    """
    import numpy as np
    import pandas as pd
    import plotly.express as px
    import plotly.graph_objects as go
    import time, math, json
    from dateutil.relativedelta import relativedelta
    import streamlit as st
    from datetime import datetime

    start_overall = time.time()
    params = params or {}

    print("\n" + "=" * 80)
    print("[ALL-MAXED] üöó Starting CATEGORY analytics control block setup")
    print(f"[TIME] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    # -------------------------
    # ALL-MAXED UI STARTING BANNER
    # -------------------------
    st.markdown(
        f"""
        <div style="
            text-align:center;
            padding:20px;
            border-radius:14px;
            background:linear-gradient(90deg,#11998e,#38ef7d);
            color:white;
            font-size:20px;
            font-weight:bold;
            box-shadow: 0 0 16px rgba(0,0,0,0.4);
        ">
            üöó ALL-MAXED CATEGORY Analytics Block Started ‚Äî {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
        </div>
        """,
        unsafe_allow_html=True
    )
    
    st.divider()  # optional visual separation

    # -------------------------
    # Controls ‚Äî All Maxed (with unique keys)
    # -------------------------
    freq = st.radio(
        "Aggregation Frequency",
        ["Daily", "Monthly", "Quarterly", "Yearly"],
        index=1,
        horizontal=True,
        key="allmaxed_freq_radio"
    )
    print(f"[CONTROL] Frequency selected ‚Üí {freq}")
    
    mode = st.radio(
        "View Mode",
        ["Separate (Small Multiples)", "Combined (Overlay / Stacked)"],
        index=1,
        horizontal=True,
        key="allmaxed_mode_radio"
    )
    print(f"[CONTROL] Mode selected ‚Üí {mode}")
    
    current_year = datetime.now().year
    print(f"[INFO] Current year detected: {current_year}")
    
    start_year = st.number_input(
        "From Year",
        min_value=2010,
        max_value=current_year,
        value=current_year-1,
        key="allmaxed_start_year"
    )
    print(f"[CONTROL] Start year selected ‚Üí {start_year}")
    
    end_year = st.number_input(
        "To Year",
        min_value=start_year,
        max_value=current_year,
        value=current_year,
        key="allmaxed_end_year"
    )
    print(f"[CONTROL] End year selected ‚Üí {end_year}")
    
    years = list(range(int(start_year), int(end_year)+1))
    print(f"[INFO] Years in scope ‚Üí {years}")
    
    show_heatmap = st.checkbox(
        "Show Heatmap (year √ó category)",
        value=True,
        key="allmaxed_show_heatmap"
    )
    print(f"[OPTION] Heatmap enabled ‚Üí {show_heatmap}")
    
    show_radar = st.checkbox(
        "Show Radar (per year)",
        value=True,
        key="allmaxed_show_radar"
    )
    print(f"[OPTION] Radar enabled ‚Üí {show_radar}")
    
    do_forecast = st.checkbox(
        "Enable Forecasting",
        value=True,
        key="allmaxed_do_forecast"
    )
    print(f"[OPTION] Forecasting enabled ‚Üí {do_forecast}")
    
    do_anomaly = st.checkbox(
        "Enable Anomaly Detection",
        value=False,
        key="allmaxed_do_anomaly"
    )
    print(f"[OPTION] Anomaly detection enabled ‚Üí {do_anomaly}")
    
    do_clustering = st.checkbox(
        "Enable Clustering (KMeans)",
        value=False,
        key="allmaxed_do_clustering"
    )
    print(f"[OPTION] Clustering enabled ‚Üí {do_clustering}")
    
    enable_ai = st.checkbox(
        "Enable AI Narrative (requires provider)",
        value=False,
        key="allmaxed_enable_ai"
    )
    print(f"[OPTION] AI narrative enabled ‚Üí {enable_ai}")

    st.info(f"üöÄ Starting ALL-MAXED category pipeline (debug ON) ‚Äî years: {years} | freq: {freq} | mode: {mode}")
    
    print(f"[PIPELINE] üöÄ ALL-MAXED pipeline initialized successfully")
    print(f"[SUMMARY] freq={freq} | mode={mode} | years={years}")
    print("[STATUS] Waiting for further analytics execution...")
    print("="*80 + "\n")

    # -------------------------
    # Fetch multi-year category data
    # -------------------------
    print("\n" + "-"*80)
    print(f"[FETCH] Starting multi-year category fetch ‚Üí {years}")
    print(f"[FETCH] Parameters: {params}")
    print("-"*80)

    all_year_dfs = []
    with st.spinner("Fetching category data for selected years..."):
        for y in years:
            print(f"[FETCH] üîÑ Fetching category data for year {y} ...")
            try:
                df_y = fetch_year_category(y, params, show_debug=False)
                if df_y is None or df_y.empty:
                    print(f"[WARN] ‚ö†Ô∏è No category data found for year {y}")
                    st.warning(f"No category data for {y}")
                    continue
                print(f"[OK] ‚úÖ Year {y}: {len(df_y)} rows loaded.")
                all_year_dfs.append(df_y)
            except Exception as e:
                print(f"[ERROR] ‚ùå Failed to fetch data for {y}: {e}")
                logger.exception(f"Error fetching {y}: {e}")
                st.error(f"Error fetching {y}: {e}")

    if not all_year_dfs:
        print("[INFO] ‚ÑπÔ∏è No category data loaded for selected range. Falling back to deterministic mocks.")
        st.info("No category data loaded for selected range. Displaying deterministic mocks for demonstration.")
        # Generate mocks
        for y in years:
            print(f"[MOCK] Generating mock data for {y} ...")
            all_year_dfs.append(to_df(deterministic_mock_categories(y)).assign(year=y))
        print(f"[MOCK] ‚úÖ Generated {len(all_year_dfs)} mock DataFrames.")

    df_cat_all = pd.concat(all_year_dfs, ignore_index=True)
    print(f"[FINAL] ‚úÖ Combined dataset shape: {df_cat_all.shape}")
    print("-"*80 + "\n")

    # -------------------------
    # Frequency expansion -> time series (synthetic if needed)
    # -------------------------
    print("\n" + "-"*80)
    print("[TIMESERIES] üîÅ Starting frequency expansion to time series...")
    print(f"[TIMESERIES] Unique years in dataset: {sorted(df_cat_all['year'].unique().tolist())}")
    print(f"[TIMESERIES] Target frequency: {freq}")
    print("-"*80)

    ts_list = []
    for y in sorted(df_cat_all["year"].unique()):
        print(f"[TIMESERIES] ‚è≥ Expanding year {y} ...")
        df_y = df_cat_all[df_cat_all["year"] == y].reset_index(drop=True)
        print(f"[TIMESERIES]    - Rows: {len(df_y)} | Categories: {df_y['label'].nunique()}")
        ts = year_to_timeseries(df_y, int(y), freq=freq)
        print(f"[TIMESERIES]    ‚úÖ Generated {len(ts)} time points for {y}")
        ts_list.append(ts)

    if ts_list:
        df_ts = pd.concat(ts_list, ignore_index=True)
        print(f"[TIMESERIES] ‚úÖ Combined time series shape: {df_ts.shape}")
    else:
        print("[TIMESERIES] ‚ö†Ô∏è No time series generated, creating empty DataFrame.")
        df_ts = pd.DataFrame(columns=["ds", "label", "value", "year"])

    df_ts["ds"] = pd.to_datetime(df_ts["ds"], errors="coerce")

    # -------------------------
    # Resample to requested frequency (group-by label)
    # -------------------------
    print(f"[RESAMPLE] üîÑ Resampling time series ‚Üí {freq}")
    if freq == "Daily":
        resampled = df_ts.groupby(["label", pd.Grouper(key="ds", freq="D")])["value"].sum().reset_index()
    elif freq == "Monthly":
        resampled = df_ts.groupby(["label", pd.Grouper(key="ds", freq="M")])["value"].sum().reset_index()
    elif freq == "Quarterly":
        resampled = df_ts.groupby(["label", pd.Grouper(key="ds", freq="Q")])["value"].sum().reset_index()
    else:
        resampled = df_ts.groupby(["label", pd.Grouper(key="ds", freq="Y")])["value"].sum().reset_index()

    print(f"[RESAMPLE] ‚úÖ Resampled shape: {resampled.shape}")

    resampled["year"] = resampled["ds"].dt.year
    pivot = resampled.pivot_table(index="ds", columns="label", values="value", aggfunc="sum").fillna(0)
    pivot_year = resampled.pivot_table(index="year", columns="label", values="value", aggfunc="sum").fillna(0)

    print(f"[PIVOT] üìä Pivot (date-indexed) shape: {pivot.shape}")
    print(f"[PIVOT] üìÖ Pivot (year-indexed) shape: {pivot_year.shape}")
    print("-"*80 + "\n")

    # -------------------------
    # Visualization Section ‚Äî All-Maxed
    # -------------------------
    print("\n" + "="*80)
    print("[VISUALIZATION] üé® Starting visualization section for All-Maxed block")
    print(f"[VISUALIZATION] Checking data availability before rendering...")
    print("-"*80)

    st.subheader("üìä Visualizations ‚Äî Multi-year & Multi-frequency (All-Maxed)")

    # --- Safety Checks ---
    if "resampled" not in locals() or resampled is None or resampled.empty:
        print("[VISUALIZATION] ‚ùå 'resampled' data missing or empty.")
        st.warning("‚ö†Ô∏è No valid 'resampled' data available for visualization.")
        st.stop()
    else:
        print(f"[VISUALIZATION] ‚úÖ 'resampled' ready | Shape: {resampled.shape}")

    if "pivot" not in locals() or pivot is None or pivot.empty:
        print("[VISUALIZATION] ‚ùå 'pivot' data missing or empty.")
        st.warning("‚ö†Ô∏è No valid 'pivot' data available for visualization.")
        st.stop()
    else:
        print(f"[VISUALIZATION] ‚úÖ 'pivot' ready | Shape: {pivot.shape}")

    if "mode" not in locals():
        print("[VISUALIZATION] ‚ÑπÔ∏è 'mode' not found ‚Äî defaulting to Combined (Overlay / Stacked).")
        mode = "Combined (Overlay / Stacked)"
    else:
        print(f"[VISUALIZATION] üéõ Mode: {mode}")

    print("="*80 + "\n")
    
    
    # -------------------------
    # Combined vs Separate Mode
    # -------------------------
    if mode.startswith("Combined"):
        # -------------------------
        # Combined View ‚Äî MAXED
        # -------------------------
        print("\n" + "="*80)
        print("[COMBINED VIEW] üåà Rendering Combined (Stacked + Overlay) Maxed View")
        print(f"[COMBINED VIEW] DataFrame shape: {resampled.shape}")
        print(f"[COMBINED VIEW] Columns: {list(resampled.columns)}")
        print("-"*80)
    
        st.markdown("### üåà Stacked & Overlay Trends ‚Äî Maxed Combined View")
    
        total_reg = resampled["value"].sum()
        mean_val = resampled["value"].mean()
        median_val = resampled["value"].median()
    
        # --- Stacked Area Chart ---
        try:
            fig_area = px.area(
                resampled, x="ds", y="value", color="label",
                title="üìä Stacked Registrations by Category Over Time",
                color_discrete_sequence=px.colors.qualitative.Set3
            )
            fig_area.add_hline(y=mean_val, line_dash="dash", line_color="green",
                               annotation_text=f"Mean: {mean_val:.0f}", annotation_position="top left")
            fig_area.add_hline(y=median_val, line_dash="dot", line_color="blue",
                               annotation_text=f"Median: {median_val:.0f}", annotation_position="bottom right")
            fig_area.update_layout(
                template="plotly_white",
                legend_title_text="Category",
                legend=dict(orientation="h", yanchor="bottom", y=-0.25, xanchor="center", x=0.5),
                xaxis_title="Date", yaxis_title="Registrations",
                margin=dict(t=80, b=100, l=60, r=40),
                hovermode="x unified",
                title_font=dict(size=22, family="Segoe UI", color="#222"),
                height=500
            )
            fig_area.update_traces(
                hovertemplate="<b>%{fullData.name}</b><br>Date: %{x|%b %Y}<br>Registrations: %{y:,}"
            )
            st.plotly_chart(fig_area, use_container_width=True)
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Stacked area failed: {e}")
    
        # --- Overlay Line Chart ---
        try:
            fig_line = px.line(
                resampled, x="ds", y="value", color="label",
                title="üìà Category Trends Overlay", markers=True,
                color_discrete_sequence=px.colors.qualitative.Bold
            )
            fig_line.update_traces(line=dict(width=3))
            fig_line.add_hline(y=mean_val, line_dash="dash", line_color="green",
                               annotation_text=f"Mean: {mean_val:.0f}", annotation_position="top left")
            fig_line.add_hline(y=median_val, line_dash="dot", line_color="blue",
                               annotation_text=f"Median: {median_val:.0f}", annotation_position="bottom right")
            fig_line.update_layout(
                template="plotly_white",
                legend_title_text="Category",
                legend=dict(orientation="h", yanchor="bottom", y=-0.25, xanchor="center", x=0.5),
                xaxis_title="Date", yaxis_title="Registrations",
                margin=dict(t=80, b=100, l=60, r=40),
                hovermode="x unified",
                title_font=dict(size=22, family="Segoe UI", color="#222"),
                height=500
            )
            fig_line.update_traces(
                hovertemplate="<b>%{fullData.name}</b><br>Date: %{x|%b %Y}<br>Registrations: %{y:,}"
            )
            st.plotly_chart(fig_line, use_container_width=True)
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Overlay lines failed: {e}")
    
    else:
        # -------------------------
        # Separate Mode (Small Multiples) ‚Äî MAXED
        # -------------------------
        print("\n" + "="*80)
        print("[SEPARATE VIEW] üß© Rendering Maxed Small Multiples (Yearly Category Distribution)")
        print(f"[SEPARATE VIEW] Incoming DataFrame shape: {resampled.shape}")
        print(f"[SEPARATE VIEW] Columns: {list(resampled.columns)}")
        print("-"*80)
    
        st.markdown("### üß© Maxed Small Multiples ‚Äî Yearly Category Distribution")
    
        try:
            years_sorted = sorted(resampled["year"].unique())
        except Exception:
            years_sorted = []
    
        sel_small = st.multiselect(
            "Select specific years for small multiples (limit 6)",
            years_sorted,
            default=years_sorted[-min(3, len(years_sorted)):] if years_sorted else []
        )
    
        if sel_small:
            for y in sel_small[:6]:
                d = resampled[resampled["year"] == y]
                if d.empty:
                    st.caption(f"‚ö†Ô∏è No data for {y}")
                    continue
    
                total_reg = d["value"].sum()
                mean_val = d["value"].mean()
                median_val = d["value"].median()
    
                fig_bar = px.bar(
                    d, x="label", y="value", color="label",
                    text_auto=".2s",
                    title=f"üìä Category Distribution ‚Äî {y}",
                    color_discrete_sequence=px.colors.qualitative.Pastel1
                )
                fig_bar.add_hline(y=mean_val, line_dash="dash", line_color="green",
                                  annotation_text=f"Mean: {mean_val:.0f}", annotation_position="top left")
                fig_bar.add_hline(y=median_val, line_dash="dot", line_color="blue",
                                  annotation_text=f"Median: {median_val:.0f}", annotation_position="bottom right")
                fig_bar.update_layout(
                    template="plotly_white",
                    margin=dict(t=60, b=40, l=40, r=40),
                    height=450,
                    xaxis_title="Category",
                    yaxis_title="Registrations",
                    hovermode="x unified"
                )
                fig_bar.update_traces(
                    hovertemplate="<b>%{x}</b><br>Registrations: %{y:,}<br>Share: %{customdata[0]:.1%}",
                    customdata=np.array(d["value"] / total_reg).reshape(-1, 1)
                )
                st.plotly_chart(fig_bar, use_container_width=True)
        else:
            st.info("Select at least one year to show small multiples.")
    # -------------------------
    # Optional Advanced Visuals ‚Äî MAXED
    # -------------------------
    print("\n" + "="*80)
    print("[ADVANCED VISUALS] ‚öôÔ∏è Starting optional visualization layer...")
    print(f"[ADVANCED VISUALS] show_heatmap={show_heatmap}, show_radar={show_radar}")
    print(f"[ADVANCED VISUALS] pivot_year shape: {pivot_year.shape if 'pivot_year' in locals() else 'N/A'}")
    print("-"*80)
    
    # --- Heatmap ---
    if show_heatmap:
        print("[HEATMAP] üî• Rendering category heatmap (Year √ó Category)...")
        st.markdown("### üî• Category Heatmap (Year √ó Category)")
        try:
            pivot_heat = pivot_year.copy()
            fig_heat = px.imshow(
                pivot_heat.T,
                labels=dict(x="Year", y="Category", color="Registrations"),
                text_auto=True,
                aspect="auto",
                color_continuous_scale="YlOrRd",
                title="Heatmap of Registrations per Category per Year",
            )
            fig_heat.update_layout(
                template="plotly_white",
                title_font=dict(size=20, family="Segoe UI", color="#222"),
                xaxis=dict(tickangle=-45),
                yaxis=dict(tickfont=dict(size=12)),
                margin=dict(t=80, b=80, l=60, r=40),
                height=500
            )
            st.plotly_chart(fig_heat, use_container_width=True)
            print("[HEATMAP] ‚úÖ Heatmap rendered successfully.")
        except Exception as e:
            print(f"[HEATMAP] ‚ùå Heatmap failed: {e}")
            st.warning(f"‚ö†Ô∏è Heatmap failed: {e}")
    
    # --- Radar Chart ---
    if show_radar:
        print("[RADAR] üï∏Ô∏è Rendering radar chart (Category profiles per year)...")
        st.markdown("### üï∏Ô∏è Radar Chart ‚Äî Category Profiles per Year")
        try:
            import plotly.graph_objects as go
            cats = list(pivot_year.columns)
            years_for_radar = sorted(pivot_year.index)[-min(4, len(pivot_year.index)):]
            fig_radar = go.Figure()
            colors = px.colors.qualitative.Set2
    
            for i, y in enumerate(years_for_radar):
                vals = pivot_year.loc[y].values
                fig_radar.add_trace(go.Scatterpolar(
                    r=vals,
                    theta=cats,
                    fill='toself',
                    name=str(y),
                    opacity=0.6,
                    line=dict(color=colors[i % len(colors)], width=2),
                    hovertemplate="<b>%{theta}</b><br>Registrations: %{r:,}<extra>%{fullData.name}</extra>"
                ))
    
            # Optional mean line overlay
            mean_vals = pivot_year.loc[years_for_radar].mean()
            fig_radar.add_trace(go.Scatterpolar(
                r=mean_vals.values,
                theta=cats,
                fill=None,
                line=dict(color="black", width=3, dash="dash"),
                name="Mean",
                hovertemplate="<b>%{theta}</b><br>Mean Registrations: %{r:.0f}<extra>%{fullData.name}</extra>"
            ))
    
            fig_radar.update_layout(
                polar=dict(radialaxis=dict(visible=True, tickformat=",", nticks=5)),
                showlegend=True,
                template="plotly_white",
                title="Radar Comparison of Category Patterns",
                title_font=dict(size=20, family="Segoe UI", color="#222"),
                height=500,
                margin=dict(t=80, b=60, l=60, r=60)
            )
            st.plotly_chart(fig_radar, use_container_width=True)
            print("[RADAR] ‚úÖ Radar chart rendered successfully.")
        except Exception as e:
            print(f"[RADAR] ‚ùå Radar chart failed: {e}")
            st.warning(f"‚ö†Ô∏è Radar chart failed: {e}")
    
    print("="*80 + "\n")

    # -------------------------
    # üç© Donut & Sunburst (All-Maxed)
    # -------------------------
    print("\n" + "="*80)
    print("[DONUT+SUNBURST] üç© Starting latest-period visualization block...")
    print(f"[DONUT+SUNBURST] resampled rows={len(resampled) if 'resampled' in locals() else 'N/A'}")
    print("-"*80)
    
    st.markdown("### üç© Donut & Sunburst ‚Äî Latest Available Period (All-Maxed)")
    
    if resampled.empty:
        st.warning("‚ö†Ô∏è No resampled data available for donut/sunburst charts.")
    else:
        # Latest period
        latest_period = resampled.loc[resampled["value"] > 0, "ds"].max()
        print(f"[DONUT+SUNBURST] Latest period detected: {latest_period}")
    
        if latest_period is None or pd.isna(latest_period):
            st.info("No valid non-zero data found for latest period visualization.")
        else:
            d_latest = (
                resampled[resampled["ds"] == latest_period]
                .groupby("label", as_index=False)["value"]
                .sum()
                .sort_values("value", ascending=False)
            )
            total_latest = d_latest["value"].sum()
            d_latest["Share_%"] = (d_latest["value"] / total_latest * 100).round(2)
    
            if not d_latest.empty and total_latest > 0:
                # --- Donut Chart ---
                try:
                    fig_donut = px.pie(
                        d_latest,
                        names="label",
                        values="value",
                        hole=0.55,
                        color_discrete_sequence=px.colors.qualitative.Vivid,
                        title=f"Category Split ‚Äî {latest_period.strftime('%Y-%m')} (Total: {int(total_latest):,})",
                        hover_data={"Share_%": True, "value": True},
                    )
                    fig_donut.update_traces(
                        textposition="inside",
                        textinfo="percent+label",
                        pull=[0.05] * len(d_latest),
                    )
                    fig_donut.update_layout(
                        template="plotly_white",
                        showlegend=True,
                        legend_title_text="Category",
                        title_font=dict(size=20, family="Segoe UI", color="#222"),
                        height=450,
                    )
                    st.plotly_chart(fig_donut, use_container_width=True)
                    print("[DONUT] ‚úÖ Donut chart rendered successfully.")
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Donut chart failed: {e}")
    
                # --- Sunburst Chart ---
                try:
                    sb = (
                        df_cat_all.groupby(["year", "label"], as_index=False)["value"]
                        .sum()
                        .sort_values(["year", "value"], ascending=[True, False])
                    )
                    fig_sb = px.sunburst(
                        sb,
                        path=["year", "label"],
                        values="value",
                        color="value",
                        color_continuous_scale="Sunset",
                        title="üåû Sunburst ‚Äî Year ‚Üí Category ‚Üí Value",
                        hover_data={"value": True},
                    )
                    fig_sb.update_layout(
                        template="plotly_white",
                        title_font=dict(size=20, family="Segoe UI", color="#222"),
                        height=500,
                    )
                    st.plotly_chart(fig_sb, use_container_width=True)
                    print("[SUNBURST] ‚úÖ Sunburst chart rendered successfully.")
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Sunburst chart failed: {e}")
    
                # --- Data Summary Table ---
                with st.expander("üìã Latest Period Data Summary"):
                    st.dataframe(
                        d_latest.style.format({"value": "{:,}", "Share_%": "{:.2f}"}),
                        use_container_width=True,
                        height=300,
                    )
                    print("[SUMMARY] ‚úÖ Data summary rendered.")
    
            else:
                st.info("‚ö†Ô∏è Latest period has zero or empty category values.")
    
    print("[DONUT+SUNBURST] ‚úÖ Block completed successfully.")
    print("="*80 + "\n")

    # -------------------------
    # üî• HEATMAP ‚Äî Year √ó Category (All-Maxed)
    # -------------------------
    if show_heatmap:
        st.markdown("### üî• Heatmap ‚Äî Year √ó Category (All-Maxed)")
    
        print("\n[DEBUG] üî• show_heatmap is True")
        print(f"[DEBUG] pivot_year shape: {pivot_year.shape if 'pivot_year' in locals() else 'pivot_year not defined'}")
    
        if pivot_year.empty:
            st.info("‚ö†Ô∏è No category data available for heatmap.")
            print("[DEBUG] pivot_year is empty ‚Äî skipping heatmap.")
        else:
            try:
                # Copy and prepare data
                heat = pivot_year.copy()
                print(f"[DEBUG] heat (pivot_year copy) ‚Äî columns: {list(heat.columns)}, index (years): {list(heat.index)}")
    
                # Toggle normalization
                normalize_opt = st.toggle("Normalize heatmap (relative per year)", value=True)
                heat_used = heat.div(heat.max(axis=1), axis=0).fillna(0) if normalize_opt else heat
                print(f"[DEBUG] Using {'normalized' if normalize_opt else 'absolute'} data for heatmap.")
    
                # Build heatmap
                fig_h = go.Figure(
                    data=go.Heatmap(
                        z=heat_used.values,
                        x=heat_used.columns.astype(str),
                        y=heat_used.index.astype(str),
                        colorscale="Viridis",
                        hovertemplate="%{y} | %{x}: %{z:,.2f}" if normalize_opt else "%{y} | %{x}: %{z:,}",
                        text=heat_used.round(2) if normalize_opt else None,
                        texttemplate="%{text:.2f}" if normalize_opt else None,
                        hoverongaps=False,
                    )
                )
    
                fig_h.update_layout(
                    title=(
                        "Normalized Registrations by Category per Year"
                        if normalize_opt
                        else "Absolute Registrations by Category per Year"
                    ),
                    xaxis_title="Category",
                    yaxis_title="Year",
                    template="plotly_white",
                    height=500,
                    margin=dict(t=80, b=60, l=60, r=40),
                    coloraxis_colorbar=dict(
                        title="Share (0‚Äì1)" if normalize_opt else "Registrations"
                    ),
                )
    
                st.plotly_chart(fig_h, use_container_width=True)
                print("[DEBUG] Heatmap rendered successfully in Streamlit.")
    
                # Optional: Data table with gradient
                with st.expander("üìã View Heatmap Data Table"):
                    st.dataframe(
                        heat_used.round(2)
                        .style.format("{:.2f}" if normalize_opt else "{:,.0f}")
                        .background_gradient(cmap="viridis"),
                        use_container_width=True,
                    )
                    print("[DEBUG] Heatmap data table shown in expander.")
    
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Heatmap rendering failed: {e}")
                print(f"[ERROR] Heatmap rendering failed: {e}")

    # -------------------------
    # üåà RADAR ‚Äî Snapshot per Year (All-Maxed)
    # -------------------------
    if show_radar:
        st.markdown("### üåà Radar ‚Äî Category Profile Snapshot (All-Maxed)")
    
        print("\n[DEBUG] üåà show_radar is True")
        print(f"[DEBUG] pivot_year shape: {pivot_year.shape if 'pivot_year' in locals() else 'pivot_year not defined'}")
    
        if pivot_year.empty:
            st.info("‚ö†Ô∏è Not enough data for radar visualization.")
            print("[DEBUG] pivot_year is empty ‚Äî skipping radar.")
        else:
            try:
                # Select last 4 years (or fewer)
                yrs_for_radar = sorted(pivot_year.index)[-min(4, len(pivot_year.index)):]
                cats = pivot_year.columns.tolist()
                radar_df = pivot_year.copy()
    
                # Optional normalization
                normalize_radar = st.toggle("Normalize radar per category (0‚Äì1)", value=True)
                df_radar_used = radar_df.div(radar_df.max(axis=0), axis=1).fillna(0) if normalize_radar else radar_df
                print(f"[DEBUG] Using {'normalized' if normalize_radar else 'absolute'} data for radar plot.")
    
                # Build radar figure
                fig_r = go.Figure()
                for y in yrs_for_radar:
                    vals = df_radar_used.loc[y].values.tolist()
                    fig_r.add_trace(
                        go.Scatterpolar(
                            r=vals,
                            theta=cats,
                            fill="toself",
                            name=str(y),
                            hovertemplate="<b>%{theta}</b><br>Value: %{r:.2f}<extra></extra>",
                        )
                    )
    
                fig_r.update_layout(
                    polar=dict(
                        radialaxis=dict(
                            visible=True,
                            range=[0, 1] if normalize_radar else [0, df_radar_used.values.max()],
                            showline=True,
                            linewidth=1,
                            gridcolor="lightgray",
                        )
                    ),
                    showlegend=True,
                    title="Category Distribution Radar (Last Years)",
                    template="plotly_white",
                    height=600,
                )
    
                st.plotly_chart(fig_r, use_container_width=True)
                print("[DEBUG] Radar chart rendered successfully.")
    
                # Optional data table
                with st.expander("üìã Radar Data Used"):
                    st.dataframe(
                        df_radar_used.round(2)
                        .style.format("{:,.0f}" if not normalize_radar else "{:.2f}")
                        .background_gradient(cmap="cool"),
                        use_container_width=True,
                    )
                    print("[DEBUG] Radar data table displayed in expander.")
    
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Radar chart rendering failed: {e}")
                print(f"[ERROR] Radar chart rendering failed: {e}")

    # -------------------------
    # üîÆ FORECASTING ‚Äî All-Maxed (Linear + Prophet + Auto Insights)
    # -------------------------
    if do_forecast:
        st.markdown("## üîÆ Forecasting (All-Maxed)")
        print("\n[DEBUG] üîÆ Forecasting section entered")
        print(f"[DEBUG] pivot_year shape: {pivot_year.shape if 'pivot_year' in locals() else 'pivot_year not defined'}")

        # ---------------------
        # 1Ô∏è‚É£ Select category & horizon
        # ---------------------
        categories = (
            pivot_year.columns.tolist()
            if not pivot_year.empty
            else df_cat_all["label"].unique().tolist()
        )
        print(f"[DEBUG] Categories available: {categories}")

        if not categories:
            st.info("‚ö†Ô∏è No categories available for forecasting.")
            print("[DEBUG] No categories ‚Äî exiting forecasting.")
        else:
            cat_to_forecast = st.selectbox("üìä Choose category to forecast", categories)
            horizon_years = st.slider("Forecast horizon (years)", 1, 10, 3)
            st.caption("Select a category and choose how many future years to forecast.")
            print(f"[DEBUG] Selected category: {cat_to_forecast}, horizon: {horizon_years} years")

            # ---------------------
            # 2Ô∏è‚É£ Prepare time series
            # ---------------------
            if cat_to_forecast in pivot_year.columns:
                series = pivot_year[[cat_to_forecast]].reset_index().rename(
                    columns={cat_to_forecast: "y", "index": "year"}
                )
                print("[DEBUG] Series extracted from pivot_year:")
                print(series.head())
            else:
                series = pd.DataFrame(columns=["year", "y"])
                print("[DEBUG] Category not found in pivot_year, empty DataFrame created.")

            if series.empty or series["y"].isna().all():
                st.info("‚ö†Ô∏è Insufficient data for forecasting this category.")
                print("[DEBUG] Series is empty or all NaN ‚Äî cannot forecast.")
            else:
                series["ds"] = pd.to_datetime(series["year"].astype(str) + "-01-01")
                series = series[["ds", "y"]].dropna()
                print("[DEBUG] Final series for forecasting:")
                print(series)

                # ---------------------
                # 3Ô∏è‚É£ Linear Regression Forecast
                # ---------------------
                st.markdown("### üìà Linear Regression Forecast")
                try:
                    from sklearn.linear_model import LinearRegression
                    X = np.arange(len(series)).reshape(-1, 1)
                    y = series["y"].values
                    print(f"[DEBUG] LinearRegression X shape: {X.shape}, y shape: {y.shape}")

                    model = LinearRegression().fit(X, y)
                    print("[DEBUG] Linear model coefficients:", model.coef_, "Intercept:", model.intercept_)

                    fut_idx = np.arange(len(series) + horizon_years).reshape(-1, 1)
                    preds = model.predict(fut_idx)
                    print(f"[DEBUG] Predictions shape: {preds.shape}")

                    fut_dates = pd.date_range(
                        start=series["ds"].iloc[0],
                        periods=len(series) + horizon_years,
                        freq="YS",
                    )
                    df_fore = pd.DataFrame({"ds": fut_dates, "Linear": preds})
                    df_fore["Type"] = ["Historical"] * len(series) + ["Forecast"] * horizon_years
                    print("[DEBUG] Forecast DataFrame (Linear):")
                    print(df_fore.tail())

                    fig_l = px.line(df_fore, x="ds", y="Linear", color="Type",
                                    title=f"Linear Trend Forecast ‚Äî {cat_to_forecast}")
                    fig_l.add_scatter(x=series["ds"], y=series["y"], mode="markers+lines",
                                      name="Observed", line=dict(color="blue"))
                    fig_l.update_layout(template="plotly_white", height=500)
                    st.plotly_chart(fig_l, use_container_width=True)

                    # KPI summary
                    last_val = series["y"].iloc[-1]
                    next_val = preds[len(series)]
                    growth = ((next_val - last_val) / last_val) * 100 if last_val else np.nan
                    st.metric("Next Year Projection", f"{next_val:,.0f}", f"{growth:+.1f}% vs last year")
                    print(f"[DEBUG] Linear growth projection: next={next_val}, growth={growth:.2f}%")
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Linear regression forecast failed: {e}")
                    print(f"[ERROR] Linear regression forecast failed: {e}")

                # ---------------------
                # 4Ô∏è‚É£ Prophet Forecast (if available)
                # ---------------------
                st.markdown("### üßô Prophet Forecast (Advanced, if available)")
                try:
                    from prophet import Prophet
                    print("[DEBUG] Prophet module imported successfully.")

                    m = Prophet(
                        yearly_seasonality=True,
                        seasonality_mode="multiplicative",
                        changepoint_prior_scale=0.05,
                    )
                    m.fit(series)
                    print("[DEBUG] Prophet model fitted successfully.")
                    future = m.make_future_dataframe(periods=horizon_years, freq="Y")
                    forecast = m.predict(future)
                    print(f"[DEBUG] Prophet forecast generated: shape={forecast.shape}")
                    print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail())

                    figp = go.Figure()
                    figp.add_trace(go.Scatter(
                        x=series["ds"], y=series["y"],
                        mode="markers+lines", name="Observed", line=dict(color="blue")))
                    figp.add_trace(go.Scatter(
                        x=forecast["ds"], y=forecast["yhat"],
                        mode="lines", name="Forecast (yhat)", line=dict(color="orange", width=3)))
                    figp.add_trace(go.Scatter(
                        x=forecast["ds"], y=forecast["yhat_upper"],
                        mode="lines", name="Upper Bound", line=dict(color="lightgray", dash="dot")))
                    figp.add_trace(go.Scatter(
                        x=forecast["ds"], y=forecast["yhat_lower"],
                        mode="lines", name="Lower Bound", line=dict(color="lightgray", dash="dot")))

                    figp.update_layout(
                        title=f"Prophet Forecast ‚Äî {cat_to_forecast}",
                        template="plotly_white",
                        height=550,
                        legend=dict(orientation="h", y=-0.2),
                        xaxis_title="Year",
                        yaxis_title="Registrations",
                    )
                    st.plotly_chart(figp, use_container_width=True)
                    print("[DEBUG] Prophet forecast plot rendered successfully.")

                    last_year = series["ds"].dt.year.max()
                    fut_y = forecast.tail(horizon_years)["yhat"].mean()
                    st.success(f"üìä Prophet projects an **average of {fut_y:,.0f}** registrations/year for the next {horizon_years} years.")
                    print(f"[DEBUG] Prophet mean projection next {horizon_years} yrs: {fut_y:,.0f}")
                except ImportError:
                    st.info("üß† Prophet not installed ‚Äî only linear forecast shown.")
                    print("[DEBUG] Prophet not installed.")
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Prophet forecast failed: {e}")
                    print(f"[ERROR] Prophet forecast failed: {e}")

                # ---------------------
                # 5Ô∏è‚É£ Display Forecast Data
                # ---------------------
                with st.expander("üìã View Forecast Data Table"):
                    try:
                        comb = df_fore.copy()
                        if "forecast" in locals():
                            comb = comb.merge(
                                forecast[["ds", "yhat", "yhat_lower", "yhat_upper"]],
                                on="ds", how="outer"
                            )
                            print(f"[DEBUG] Merged Linear + Prophet forecast table, shape={comb.shape}")
                        st.dataframe(
                            comb.round(2).style.background_gradient(cmap="PuBuGn"),
                            use_container_width=True,
                        )
                    except Exception as e:
                        st.dataframe(df_fore, use_container_width=True)
                        print(f"[ERROR] Forecast data merge/display failed: {e}")

    # -------------------------
    # ‚ö†Ô∏è ANOMALY DETECTION ‚Äî All-Maxed (with params + debug)
    # -------------------------
    if do_anomaly:
        st.markdown("## ‚ö†Ô∏è Anomaly Detection (All-Maxed)")
        st.caption("Detects outliers and abnormal spikes/drops per category time series using IsolationForest + z-score fallback.")
    
        try:
            from sklearn.ensemble import IsolationForest
            import numpy as np
            import pandas as pd
            import plotly.graph_objects as go
    
            # --- User Parameters ---
            with st.expander("‚öôÔ∏è Detection Parameters"):
                contamination_default = st.slider("Contamination Base Rate", 0.005, 0.1, 0.02, 0.005)
                z_threshold = st.slider("Z-Score Threshold (fallback)", 1.5, 4.0, 2.8, 0.1)
                n_estimators = st.slider("Isolation Forest Trees", 50, 500, 200, 50)
                rolling_window = st.slider("Rolling Window (z-score fallback)", 3, 12, 6, 1)
    
            anomaly_records = []
    
            if resampled.empty:
                st.warning("No resampled data available for anomaly detection.")
            else:
                categories = sorted(resampled["label"].unique())
                prog_bar = st.progress(0.0)
                total_detected = 0
    
                for i, cat in enumerate(categories):
                    prog_bar.progress((i + 1) / len(categories))
                    ser = (
                        resampled[resampled["label"] == cat]
                        .set_index("ds")["value"]
                        .fillna(0)
                        .sort_index()
                    )
    
                    if len(ser) < 8 or ser.std() == 0:
                        print(f"[DEBUG] Skipping {cat} ‚Äî insufficient length or zero variance")
                        continue
    
                    # --- Adaptive contamination ---
                    cont_rate = min(0.05, max(0.005, contamination_default * (ser.std() / (ser.mean() + 1e-9))))
                    print(f"[DEBUG] Category={cat}, Adaptive Contamination={cont_rate:.4f}")
    
                    # --- IsolationForest Detection ---
                    try:
                        iso = IsolationForest(
                            contamination=cont_rate,
                            random_state=42,
                            n_estimators=n_estimators,
                            bootstrap=True,
                        )
                        X = ser.values.reshape(-1, 1)
                        preds = iso.fit_predict(X)
                        an_idxs = ser.index[preds == -1]
                        ser_an = ser.loc[an_idxs]
    
                        for dt, val in ser_an.items():
                            anomaly_records.append({"Category": cat, "Date": dt, "Value": val})
                            total_detected += 1
    
                    except Exception as e_if:
                        print(f"[WARN] IsolationForest failed for {cat}: {e_if}")
                        # --- Fallback: Rolling Z-Score ---
                        rolling_mean = ser.rolling(rolling_window, min_periods=2).mean()
                        rolling_std = ser.rolling(rolling_window, min_periods=2).std()
                        zscores = (ser - rolling_mean) / rolling_std
                        ser_an = ser[np.abs(zscores) > z_threshold]
    
                        for dt, val in ser_an.items():
                            anomaly_records.append({"Category": cat, "Date": dt, "Value": val})
                            total_detected += 1
    
                prog_bar.empty()
    
                # -------------------------------
                # üßæ Summary + Visualization
                # -------------------------------
                if not anomaly_records:
                    st.success("‚úÖ No significant anomalies detected across categories.")
                else:
                    df_an = pd.DataFrame(anomaly_records)
                    df_an["Date"] = pd.to_datetime(df_an["Date"])
    
                    st.markdown("### üìã Detected Anomalies Summary")
                    st.dataframe(
                        df_an.sort_values("Date", ascending=False).style.format({"Value": "{:,.0f}"}),
                        use_container_width=True,
                        height=300,
                    )
    
                    st.info(f"üìä {total_detected} anomalies detected across {len(categories)} categories.")
    
                    # --- Category selector for visualization ---
                    sel_cat = st.selectbox(
                        "üîç View anomalies for a specific category", sorted(df_an["Category"].unique())
                    )
                    ser = (
                        resampled[resampled["label"] == sel_cat]
                        .set_index("ds")["value"]
                        .fillna(0)
                        .sort_index()
                    )
                    an_dates = df_an[df_an["Category"] == sel_cat]["Date"]
    
                    fig_a = go.Figure()
                    fig_a.add_trace(
                        go.Scatter(
                            x=ser.index,
                            y=ser.values,
                            mode="lines+markers",
                            name="Value",
                            line=dict(color="steelblue"),
                        )
                    )
                    if not an_dates.empty:
                        fig_a.add_trace(
                            go.Scatter(
                                x=an_dates,
                                y=ser.loc[an_dates],
                                mode="markers",
                                name="Anomaly",
                                marker=dict(color="red", size=10, symbol="x"),
                            )
                        )
                    fig_a.update_layout(
                        title=f"Anomalies in {sel_cat} ‚Äî Time Series Overlay",
                        template="plotly_white",
                        xaxis_title="Date",
                        yaxis_title="Registrations",
                        height=500,
                    )
                    st.plotly_chart(fig_a, use_container_width=True)
    
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Anomaly detection failed: {e}")
            print("[ERROR] Anomaly detection exception:", e)

    # -------------------------
    # üîç CLUSTERING (KMeans) ‚Äî All-Maxed (with params + debug)
    # -------------------------
    if do_clustering:
        st.markdown("## üîç Clustering (KMeans) ‚Äî All-Maxed Mode")
        st.caption("Groups years by their category registration mix using KMeans with normalization, PCA view & silhouette score.")

        try:
            from sklearn.cluster import KMeans
            from sklearn.preprocessing import StandardScaler, MinMaxScaler
            from sklearn.decomposition import PCA
            from sklearn.metrics import silhouette_score
            import numpy as np

            if pivot_year.empty:
                st.warning("‚ö†Ô∏è No pivot_year data available for clustering.")
            else:
                # ------------------------------
                # ‚öôÔ∏è PARAMS PANEL
                # ------------------------------
                with st.expander("‚öôÔ∏è Clustering Parameters"):
                    normalize_opt = st.toggle("Normalize before clustering", value=True)
                    scale_method = st.selectbox("Scaling method", ["StandardScaler", "MinMaxScaler"])
                    max_k = min(8, max(3, len(pivot_year) - 1))
                    k = st.slider("Number of clusters (K)", 2, max_k, min(4, max_k))
                    n_init = st.slider("KMeans n_init", 5, 50, 10, 5)
                    pca_dim = st.slider("PCA components (for visualization)", 2, 3, 2, 1)
                    random_seed = st.number_input("Random Seed", value=42, step=1)

                # ------------------------------
                # üßÆ DATA PREP
                # ------------------------------
                X = pivot_year.fillna(0).values
                scaler = StandardScaler() if scale_method == "StandardScaler" else MinMaxScaler()
                X_scaled = scaler.fit_transform(X) if normalize_opt else X

                print(f"[DEBUG] pivot_year shape={X.shape}, normalize={normalize_opt}, scale={scale_method}")

                # ------------------------------
                # üß† KMEANS FITTING
                # ------------------------------
                km = KMeans(
                    n_clusters=k,
                    n_init=n_init,
                    random_state=random_seed,
                )
                labels = km.fit_predict(X_scaled)
                inertia = km.inertia_
                sil = silhouette_score(X_scaled, labels) if len(set(labels)) > 1 else np.nan

                print(f"[DEBUG] KMeans done: K={k}, Inertia={inertia:.3f}, Silhouette={sil:.3f}")

                # ------------------------------
                # üìã CLUSTER SUMMARY
                # ------------------------------
                df_cluster = pd.DataFrame({
                    "Year": pivot_year.index.astype(str),
                    "Cluster": labels
                })

                st.markdown("### üßæ Cluster Assignment Summary")
                st.dataframe(df_cluster, use_container_width=True, height=300)

                c1, c2, c3 = st.columns(3)
                c1.metric("Clusters", str(k))
                c2.metric("Inertia (‚Üì better)", f"{inertia:.2f}")
                c3.metric("Silhouette (‚Üë better)", f"{sil:.3f}" if not np.isnan(sil) else "n/a")

                # ------------------------------
                # üß© CLUSTER CENTERS
                # ------------------------------
                centers_scaled = km.cluster_centers_
                centers = scaler.inverse_transform(centers_scaled) if normalize_opt else centers_scaled
                centers_df = pd.DataFrame(centers, columns=pivot_year.columns)

                st.markdown("### üß† Cluster Centers ‚Äî Approximate Category Mix")
                st.dataframe(centers_df.style.format("{:,.0f}"), use_container_width=True)

                # ------------------------------
                # üé® PCA VISUALIZATION
                # ------------------------------
                try:
                    pca = PCA(n_components=pca_dim, random_state=random_seed)
                    X_pca = pca.fit_transform(X_scaled)
                    df_pca = pd.DataFrame(X_pca, columns=[f"PC{i+1}" for i in range(pca_dim)])
                    df_pca["Year"] = pivot_year.index.astype(str)
                    df_pca["Cluster"] = labels.astype(str)

                    if pca_dim == 2:
                        fig_pca = px.scatter(
                            df_pca,
                            x="PC1",
                            y="PC2",
                            color="Cluster",
                            symbol="Cluster",
                            hover_data=["Year"],
                            title="üìä PCA Projection ‚Äî Years clustered by category mix",
                        )
                    else:
                        fig_pca = px.scatter_3d(
                            df_pca,
                            x="PC1",
                            y="PC2",
                            z="PC3",
                            color="Cluster",
                            hover_data=["Year"],
                            title="üìä PCA 3D Projection ‚Äî Years clustered by category mix",
                        )

                    fig_pca.update_traces(marker=dict(size=12, line=dict(width=1, color="black")))
                    fig_pca.update_layout(template="plotly_white", height=500)
                    st.plotly_chart(fig_pca, use_container_width=True)
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è PCA visualization failed: {e}")
                    print("[ERROR] PCA failure:", e)

                # ------------------------------
                # üåê RADAR VISUALIZATION
                # ------------------------------
                st.markdown("### üåê Radar ‚Äî Cluster-wise Average Category Mix")
                try:
                    fig_r = go.Figure()
                    for c in sorted(df_cluster["Cluster"].unique()):
                        cluster_mean = pivot_year.iloc[df_cluster["Cluster"] == c].mean()
                        fig_r.add_trace(go.Scatterpolar(
                            r=cluster_mean.values.tolist(),
                            theta=pivot_year.columns.tolist(),
                            fill="toself",
                            name=f"Cluster {c}",
                        ))
                    fig_r.update_layout(
                        polar=dict(radialaxis=dict(visible=True)),
                        title="Cluster-wise Average Category Mix (Radar View)",
                        template="plotly_white",
                    )
                    st.plotly_chart(fig_r, use_container_width=True)
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Radar view failed: {e}")
                    print("[ERROR] Radar failure:", e)

                st.success(f"‚úÖ Clustering completed ‚Äî {k} clusters, silhouette={sil:.3f}.")

        except Exception as e:
            st.warning(f"‚ö†Ô∏è Clustering failed: {e}")
            print("[ERROR] Clustering exception:", e)

    # =====================================================
    # ü§ñ AI Narrative (All-Maxed, guarded, debug-ready)
    # =====================================================
    if enable_ai and do_forecast:
        st.markdown("## ü§ñ AI Narrative (Summary & Recommendations) ‚Äî All-Maxed")

        try:
            # Safety checks
            if pivot_year is None or pivot_year.empty or df_cat_all is None or df_cat_all.empty:
                st.warning("‚ö†Ô∏è No valid data available for AI narrative.")
                st.stop()

            # --- UI controls for AI block ---
            st.caption("Generate a concise analyst-style summary plus 3 actionable recommendations. "
                       "Requires consent for AI provider (e.g. `universal_chat`).")

            with st.expander("‚öôÔ∏è AI Narrative Parameters"):
                allow_send = st.checkbox("Allow AI call (send anonymized data)", value=False)
                preview_prompt = st.checkbox("Preview the generated AI prompt", value=False)
                show_raw = st.checkbox("Show raw AI output (debug)", value=False)
                max_chars = st.slider("Prompt truncation length", 1000, 8000, 3000, step=500)
                temperature = st.slider("AI temperature (creativity)", 0.0, 1.0, 0.0, 0.1)

            # -----------------------------------
            # üßÆ Data preparation
            # -----------------------------------
            agg = pivot_year.reset_index().copy()
            agg.rename(columns={"index": "year"}, inplace=True)
            agg["year"] = agg["year"].astype(str)

            for col in agg.columns:
                if col != "year":
                    agg[col] = agg[col].fillna(0).astype(int)

            small_records = agg.to_dict(orient="records")
            small_preview = json.dumps(small_records)[:max_chars]

            total_by_year = agg.set_index("year").sum(axis=1)
            years = list(total_by_year.index)
            total_first = float(total_by_year.iloc[0]) if len(total_by_year) > 0 else 0.0
            total_last = float(total_by_year.iloc[-1]) if len(total_by_year) > 0 else 0.0
            years_count = max(1, len(total_by_year) - 1)
            cagr_val = ((total_last / total_first) ** (1 / years_count) - 1) * 100 if total_first > 0 else 0.0

            # Top categories & growth
            top_overall = df_cat_all.groupby("label")["value"].sum().sort_values(ascending=False)
            top3 = top_overall.head(3)
            latest_year = pivot_year.index.max() if not pivot_year.empty else None

            growth_per_cat = {}
            if latest_year is not None and (latest_year - 1) in pivot_year.index:
                prev = pivot_year.loc[latest_year - 1]
                curr = pivot_year.loc[latest_year]
                for cat in pivot_year.columns:
                    prev_v = float(prev.get(cat, 0))
                    curr_v = float(curr.get(cat, 0))
                    growth_per_cat[cat] = ((curr_v - prev_v) / prev_v * 100) if prev_v > 0 else (100.0 if curr_v > 0 else 0.0)

            print(f"[DEBUG] AI narrative: {len(agg)} years, top3={list(top3.index)}, CAGR={cagr_val:.2f}%")

            # -----------------------------------
            # üí¨ Prompt Construction
            # -----------------------------------
            system_prompt = (
                "You are a senior transport data analyst. Provide a concise, factual summary (3‚Äì6 bullet points) "
                "of vehicle registration trends based on the provided aggregated metrics. "
                "Then provide 3 short, prioritized, actionable recommendations for policymakers or planners. "
                "Avoid speculation; use approximate percentages and category references if relevant."
            )

            user_context = (
                f"Aggregated yearly totals (truncated): {small_preview}\n\n"
                f"Top categories overall: {', '.join(top3.index.tolist())}.\n"
                f"Total CAGR: {cagr_val:.2f}% over {len(years)} years."
            )

            if preview_prompt:
                with st.expander("Prompt Preview", expanded=False):
                    st.write({"system": system_prompt, "user": user_context[:max_chars]})

            ai_text = None
            ai_raw = None

            # -----------------------------------
            # üåê AI Call (if consented)
            # -----------------------------------
            if allow_send:
                try:
                    if "universal_chat" in globals() or "universal_chat" in locals():
                        ai_resp = universal_chat(
                            system_prompt,
                            user_context,
                            stream=False,
                            temperature=temperature,
                            max_tokens=500,
                            retries=2,
                        )
                        if isinstance(ai_resp, dict):
                            ai_raw = ai_resp
                            ai_text = ai_resp.get("text") or ai_resp.get("response") or ai_resp.get("output")
                        elif isinstance(ai_resp, str):
                            ai_text = ai_resp
                            ai_raw = {"text": ai_resp}
                    else:
                        st.info("üß† No configured AI provider found (e.g., `universal_chat`).")
                except Exception as e:
                    st.warning(f"AI provider call failed: {e}")
                    print("[ERROR] AI call failed:", e)
                    ai_text = None

            # -----------------------------------
            # üß† Fallback deterministic summary
            # -----------------------------------
            if not ai_text:
                bullets = [
                    f"Top categories overall: {', '.join(top3.index.tolist())}.",
                    f"Total registrations {'increased' if cagr_val>0 else 'decreased'} at ~{abs(cagr_val):.2f}% CAGR ({years[0]}‚Äì{years[-1]}).",
                ]

                notable = [
                    f"{cat}: {'up' if g>0 else 'down'} {abs(g):.1f}% YoY (latest)"
                    for cat, g in growth_per_cat.items() if cat in top3.index
                ]
                if notable:
                    bullets.append("Recent changes ‚Äî " + "; ".join(notable))
                bullets.append("Data note: Aggregated at yearly level; monthly cadence can reveal finer patterns.")

                recs = [
                    "Support high-growth vehicle categories with focused policy incentives.",
                    "Prioritize infrastructure for dominant vehicle types to optimize road load.",
                    "Adopt higher data update frequency for improved forecasting accuracy."
                ]

                st.markdown("### üß† Quick Narrative (Deterministic Fallback)")
                for b in bullets:
                    st.markdown(f"- {b}")
                st.markdown("**Recommendations:**")
                for i, r in enumerate(recs, 1):
                    st.markdown(f"{i}. {r}")

                print("[DEBUG] Fallback narrative generated (no AI call).")

            else:
                st.markdown("### üß† AI Summary")
                st.markdown(ai_text)
                if show_raw and ai_raw:
                    with st.expander("Raw AI Response", expanded=False):
                        st.json(ai_raw)
                print("[DEBUG] AI text received and displayed.")

            # Cache last result in session
            st.session_state["_last_ai_narrative"] = ai_text or "\n".join(bullets)

        except Exception as e:
            st.error(f"üí• AI Narrative generation failed: {e}")
            print("[ERROR] AI Narrative Exception:", e)

    # =====================================================
    # üß© ALL-MAXED FINAL SUMMARY + EXPORTS + DEBUG INSIGHTS
    # =====================================================
    st.markdown("## üß† Final Summary, Exports & Debug Insights ‚Äî ALL-MAXED")
    
    try:
        import time
        import io
        import xlsxwriter
    
        summary_start = time.time()
    
        # ----------------------------------------------------
        # 1Ô∏è‚É£ SAFE INPUT + FALLBACKS
        # ----------------------------------------------------
        df_src = df_cat_all.copy() if "df_cat_all" in locals() else pd.DataFrame()
        freq = freq if "freq" in locals() else "Monthly"
        years = years if "years" in locals() and years else [2024, 2025]
        current_year = datetime.now().year
    
        if df_src.empty:
            st.warning("‚ö†Ô∏è No valid ALL-MAXED data found to summarize.")
            st.stop()
    
        # ----------------------------------------------------
        # 2Ô∏è‚É£ BASIC CLEANUP + ADD TIME FIELDS
        # ----------------------------------------------------
        df_src = df_src.copy()
        if "ds" not in df_src.columns:
            if "date" in df_src.columns:
                df_src["ds"] = pd.to_datetime(df_src["date"])
            elif "year" in df_src.columns:
                df_src["ds"] = pd.to_datetime(df_src["year"].astype(str) + "-01-01")
            else:
                df_src["ds"] = pd.date_range(end=datetime.today(), periods=len(df_src))
    
        df_src["year"] = df_src["ds"].dt.year
        df_src["month"] = df_src["ds"].dt.month
        df_src["label"] = df_src["label"].astype(str) if "label" in df_src.columns else "Unknown"
    
        # ----------------------------------------------------
        # 3Ô∏è‚É£ RESAMPLING + PIVOT
        # ----------------------------------------------------
        resampled = df_src.groupby(["label", "year"])["value"].sum().reset_index() if freq == "Yearly" else df_src.copy()
    
        pivot_year = (
            resampled.pivot_table(index="year", columns="label", values="value", aggfunc="sum").fillna(0)
            if "year" in resampled.columns else pd.DataFrame()
        )
    
        # ----------------------------------------------------
        # 4Ô∏è‚É£ KPIs & METRICS
        # ----------------------------------------------------
        year_totals = pivot_year.sum(axis=1).rename("TotalRegistrations").to_frame()
        year_totals["YoY_%"] = year_totals["TotalRegistrations"].pct_change() * 100
        year_totals["TotalRegistrations"] = year_totals["TotalRegistrations"].fillna(0).astype(int)
        year_totals["YoY_%"] = year_totals["YoY_%"].replace([np.inf, -np.inf], np.nan).fillna(0)
    
        # CAGR
        if len(year_totals) >= 2:
            first = float(year_totals["TotalRegistrations"].iloc[0])
            last = float(year_totals["TotalRegistrations"].iloc[-1])
            years_count = max(1, len(year_totals) - 1)
            cagr = ((last / first) ** (1 / years_count) - 1) * 100 if first > 0 else 0.0
        else:
            cagr = 0.0
    
        # Latest MoM if monthly
        latest_mom = "n/a"
        if freq == "Monthly":
            resampled["month_period"] = resampled["year"].astype(str) + "-" + resampled["month"].astype(str)
            month_totals = resampled.groupby("month_period")["value"].sum().reset_index()
            month_totals["MoM_%"] = month_totals["value"].pct_change() * 100
            latest_mom = f"{month_totals['MoM_%'].iloc[-1]:.2f}%" if len(month_totals) > 1 else "n/a"
    
        # Category share
        latest_year = int(year_totals.index.max())
        cat_share = (pivot_year.loc[latest_year] / pivot_year.loc[latest_year].sum() * 100).sort_values(ascending=False).round(1)
    
        # Top Category
        top_cat_row = df_src.groupby("label")["value"].sum().reset_index().sort_values("value", ascending=False).iloc[0]
        top_cat_share = (top_cat_row["value"] / df_src["value"].sum()) * 100 if df_src["value"].sum() > 0 else 0
        # Top Year
        top_year_row = df_src.groupby("year")["value"].sum().reset_index().sort_values("value", ascending=False).iloc[0]
    
        # ----------------------------------------------------
        # 5Ô∏è‚É£ DISPLAY METRICS
        # ----------------------------------------------------
        st.metric("üìÖ Years Loaded", f"{years[0]} ‚Üí {years[-1]}", f"{len(years)} yrs")
        st.metric("üèÜ Absolute Top Category", top_cat_row["label"], f"{top_cat_share:.2f}% share")
        st.metric("üìÖ Peak Year", f"{int(top_year_row['year'])}", f"{top_year_row['value']:,.0f} registrations")
    
        st.markdown("#### üìò Category Share (Latest Year)")
        st.dataframe(pd.DataFrame({
            "Category": cat_share.index,
            "Share_%": cat_share.values,
            "Volume": pivot_year.loc[latest_year].astype(int).values
        }).sort_values("Share_%", ascending=False), use_container_width=True)
    
        import io
        import pandas as pd
        import xlsxwriter
        
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
            workbook = writer.book
        
            # ------------------ Sheet 1: Summary KPIs ------------------
            summary_df = pd.DataFrame({
                "Metric": ["Years Loaded", "Total Categories", "Total Registrations", "Top Category", "Top Category Share (%)", "Peak Year", "Peak Year Registrations", "CAGR (%)", "Latest MoM (%)"],
                "Value": [f"{years[0]} ‚Üí {years[-1]}", df_src["label"].nunique(), df_src["value"].sum(),
                          top_cat_row["label"], round(top_cat_share, 2),
                          top_year_row["year"], top_year_row["value"],
                          round(cagr, 2), latest_mom]
            })
            summary_df.to_excel(writer, sheet_name="Dashboard", index=False)
            ws = writer.sheets["Dashboard"]
        
            # Format headers
            header_format = workbook.add_format({'bold': True, 'bg_color': '#051937', 'font_color': 'white', 'align': 'center', 'border':1})
            for col_num, value in enumerate(summary_df.columns.values):
                ws.write(0, col_num, value, header_format)
        
            # Column widths
            ws.set_column(0, 0, 30)
            ws.set_column(1, 1, 25)
        
            # Highlight top metrics
            highlight_format = workbook.add_format({'bg_color': '#00bf72', 'font_color': 'white', 'bold': True})
            ws.write(3, 1, top_cat_row["label"], highlight_format)
            ws.write(5, 1, top_year_row["year"], highlight_format)
        
            # ------------------ Sheet 2: Yearly Pivot with Sparkline ------------------
            if not pivot_year.empty:
                pivot_year.to_excel(writer, sheet_name="Yearly_Pivot")
                ws2 = writer.sheets["Yearly_Pivot"]
        
                # Column widths
                for i, col in enumerate(pivot_year.columns):
                    ws2.set_column(i+1, i+1, max(len(str(col)), 15))  # +1 for index
        
                # Conditional formatting for highest value
                ws2.conditional_format(1,1,len(pivot_year),len(pivot_year.columns), {'type': '3_color_scale'})
        
                # Add sparkline for total registrations per year
                ws2.write(len(pivot_year)+2,0, "Trend (Sparkline)")
                ws2.add_sparkline(len(pivot_year)+2,1, {'range': f'B2:{chr(65+len(pivot_year.columns))}{1+len(pivot_year)}', 'type':'line', 'max':True, 'min':True})
        
                # Column chart for total registrations
                chart = workbook.add_chart({'type': 'column'})
                chart.add_series({
                    'name': 'Total Registrations',
                    'categories': ['Yearly_Pivot', 1, 0, len(pivot_year), 0],
                    'values': ['Yearly_Pivot', 1, 1, len(pivot_year), 1],
                    'fill': {'color': '#008793'}
                })
                chart.set_title({'name': 'Total Registrations per Year'})
                chart.set_x_axis({'name': 'Year'})
                chart.set_y_axis({'name': 'Registrations'})
                ws2.insert_chart('H2', chart, {'x_scale': 1.5, 'y_scale': 1.5})
        
            # ------------------ Sheet 3: Top Categories ------------------
            top_cat_df = df_src.groupby("label")["value"].sum().reset_index().sort_values("value", ascending=False)
            top_cat_df.to_excel(writer, sheet_name="Top_Categories", index=False)
            ws3 = writer.sheets["Top_Categories"]
        
            for i, col in enumerate(top_cat_df.columns):
                ws3.set_column(i, i, max(len(str(col)), 20))
        
            # Top 10 bar chart
            n_top = min(10, len(top_cat_df))
            chart2 = workbook.add_chart({'type': 'bar'})
            chart2.add_series({
                'name': 'Registrations',
                'categories': ['Top_Categories', 1, 0, n_top, 0],
                'values': ['Top_Categories', 1, 1, n_top, 1],
                'fill': {'color': '#004d7a'}
            })
            chart2.set_title({'name': 'Top 10 Categories'})
            chart2.set_x_axis({'name': 'Registrations'})
            chart2.set_y_axis({'name': 'Category'})
            ws3.insert_chart('D2', chart2, {'x_scale': 2, 'y_scale': 1.5})
        
        # XLSX written automatically
        processed_data = output.getvalue()
        st.download_button("üíæ Download Categories Dashboard", processed_data, "ALL-MAXED_Categories_Dashboard.xlsx")

    
        # ----------------------------------------------------
        # 7Ô∏è‚É£ DEBUG METRICS
        # ----------------------------------------------------
        summary_time = time.time() - summary_start
        st.markdown("### ‚öôÔ∏è Debug Performance Metrics")
        st.code(f"""
    Rows processed: {len(df_src):,}
    Categories: {df_src['label'].nunique()}
    Total registrations: {df_src['value'].sum():,.0f}
    Top category: {top_cat_row['label']} ‚Üí {top_cat_row['value']:,.0f} ({top_cat_share:.2f}%)
    Peak year: {top_year_row['year']} ‚Üí {top_year_row['value']:,.0f}
    CAGR: {cagr:.2f}%
    Latest MoM: {latest_mom}
    Runtime: {summary_time:.2f}s
    """, language="yaml")
    
    except Exception as e:
        st.error(f"‚õî ALL-MAXED summary failed: {e}")
        import traceback as _tb
        st.text(_tb.format_exc())

    
# -----------------------------------------------------
# üß© Safe Entry Point ‚Äî Streamlit-only Execution Guard
# (with simple prints for local logs / CI)
# -----------------------------------------------------
if __name__ == "__main__":
    import streamlit as st
    import sys
    import platform
    import traceback
    from datetime import datetime

    # Simple console prints for easier debugging in logs
    print("=== ALL-MAXED CATEGORY ANALYTICS ‚Äî START ===")
    print(f"Timestamp: {datetime.utcnow().isoformat()} UTC")
    print(f"Python: {sys.version.splitlines()[0]}")
    print(f"Platform: {platform.platform()}")

    
    try:
        print("[ALL-MAXED] Calling all_maxed_category_block()")
        all_maxed_category_block()
        print("[ALL-MAXED] all_maxed_category_block() completed successfully")
    except Exception as e:
        # print traceback to console/logs for easier debugging
        tb = traceback.format_exc()
        print("[ALL-MAXED] ERROR during rendering:", e)
        print(tb)
        st.error(f"üí• Error while rendering All-Maxed block: {e}")
        st.code(tb, language="python")

    print("=== ALL-MAXED CATEGORY ANALYTICS ‚Äî END ===")



#     # ---- Top Makers ----
#     with col2:
#         st.markdown("### üè≠ Top Vehicle Makers")

#         try:
#             mk_json, mk_url = get_json("vahandashboard/top5Makerchart", params)
#             df_mk = to_df(mk_json, label_keys=("makerName", "manufacturer", "name", "label"))

#             if not df_mk.empty:
#                 fig2 = px.bar(
#                     df_mk,
#                     x="label",
#                     y="value",
#                     color="label",
#                     text_auto=True,
#                     title="Top 5 Makers",
#                 )
#                 fig2.update_layout(
#                     xaxis_title="Maker",
#                     yaxis_title="Count",
#                     showlegend=False,
#                     template="plotly_white",
#                 )
#                 st.plotly_chart(fig2, use_container_width=True)
#             else:
#                 st.info("‚ÑπÔ∏è No maker data available.")

#         except Exception as e:
#             st.error(f"‚ùå Error fetching or displaying makers: {e}")

# except Exception as e:
#     st.error(f"üö® Data fetch error: {e}")

# ===============================================================
# üè≠ MAKERS ‚Äî ALL-MAXED ANALYTICS SUITE (Streamlit)
# ===============================================================
# Mirrors the ALL-MAXED Category Analytics style but focused on Vehicle Makers.
# Assumptions: `get_json`, `to_df`, `params`, `enable_ai`, `deepinfra_chat`,
# helper viz functions (bar_from_df, pie_from_df, stack_from_df) exist in your app environment.

# -----------------------------
# MAKERS ANALYTICS ‚Äî ALL MAXED (DROP-IN)
# -----------------------------
# import math
# import json
# import numpy as np
# import pandas as pd
# import streamlit as st
# import plotly.express as px
# import plotly.graph_objects as go
# from datetime import datetime
# from dateutil.relativedelta import relativedelta
# import random
# import logging

# st.markdown("## üè≠ ALL-MAXED ‚Äî Makers Analytics (multi-frequency, multi-year)")

# # =====================================================
# # CONTROLS ‚Äî ALL ON MAIN PAGE (no sidebar)
# # =====================================================
# section_id = rto_opt.lower() if "rto_opt" in locals() else "main"

# # Frequency & Mode
# freq = st.radio(
#     "Aggregation Frequency",
#     ["Daily", "Monthly", "Quarterly", "Yearly"],
#     index=3,
#     horizontal=True,
#     key=f"freq_{section_id}"
# )

# mode = st.radio(
#     "View Mode",
#     ["Separate (Small Multiples)", "Combined (Overlay / Stacked)"],
#     index=1,
#     horizontal=True,
#     key=f"mode_{section_id}"
# )

# # Year range
# today = datetime.now()
# current_year = today.year
# default_from_year = current_year - 1


# from_year = st.sidebar.number_input(
#     "From Year",
#     min_value=2012,
#     max_value=today.year,
#     value=default_from_year,
#     key=f"from_year_{section_id}"
# )

# to_year = st.sidebar.number_input(
#     "To Year",
#     min_value=from_year,
#     max_value=today.year,
#     value=today.year,
#     key=f"to_year_{section_id}"
# )

# state_code = st.sidebar.text_input(
#     "State Code (blank=All-India)",
#     value="",
#     key=f"state_{section_id}"
# )

# rto_code = st.sidebar.text_input(
#     "RTO Code (0=aggregate)",
#     value="0",
#     key=f"rto_{section_id}"
# )

# vehicle_classes = st.sidebar.text_input(
#     "Vehicle Classes (e.g., 2W,3W,4W if accepted)",
#     value="",
#     key=f"classes_{section_id}"
# )

# vehicle_makers = st.sidebar.text_input(
#     "Vehicle Makers (comma-separated or IDs)",
#     value="",
#     key=f"makers_{section_id}"
# )

# time_period = st.sidebar.selectbox(
#     "Time Period",
#     options=[0, 1, 2],
#     index=0,
#     key=f"period_{section_id}"
# )

# fitness_check = st.sidebar.selectbox(
#     "Fitness Check",
#     options=[True, False],
#     index=0,
#     format_func=lambda x: "Enabled" if x else "Disabled",
#     key=f"fitness_{section_id}"
# )

# vehicle_type = st.sidebar.text_input(
#     "Vehicle Type (optional)",
#     value="",
#     key=f"type_{section_id}"
# )

# # Extra feature toggles
# st.divider()
# col3, col4, col5 = st.columns(3)
# with col3:
#     show_heatmap = st.checkbox("Show Heatmap (year √ó maker)", True, key=f"heatmap_{section_id}")
#     show_radar = st.checkbox("Show Radar (per year)", True, key=f"radar_{section_id}")
# with col4:
#     do_forecast = st.checkbox("Enable Forecasting", True, key=f"forecast_{section_id}")
#     do_anomaly = st.checkbox("Enable Anomaly Detection", False, key=f"anomaly_{section_id}")
# with col5:
#     do_clustering = st.checkbox("Enable Clustering (KMeans)", False, key=f"cluster_{section_id}")

# params_common = build_params(
#     from_year=from_year,
#     to_year=to_year,
#     state_code=state_code or "ALL",
#     rto_code=rto_code or "0",
#     vehicle_classes=vehicle_classes or "ALL",
#     vehicle_makers=vehicle_makers or "ALL",
#     time_period=freq,
#     fitness_check=fitness_check,
#     vehicle_type=vehicle_type or "ALL"
# )

# years = list(range(int(from_year), int(to_year) + 1))

# st.info(f"üîó Using parameters: {params_common}")

# # =====================================================
# # üöó VAHAN MAKER ANALYTICS ‚Äî ALL-MAXED VISUAL ENGINE
# # =====================================================
# import streamlit as st
# import pandas as pd
# import numpy as np
# import plotly.express as px
# import plotly.graph_objects as go
# import uuid
# import math
# from datetime import datetime
# import time

# # Start timer for KPI/summary section
# summary_start = time.time()

# # -----------------------------------------------------
# # ‚ö° GLOBAL VISUAL THEME
# # -----------------------------------------------------
# MAXED_COLORS = px.colors.qualitative.Safe + px.colors.qualitative.Plotly
# TITLE_FONT = dict(size=20, color="#111", family="Segoe UI Semibold")
# LABEL_FONT = dict(size=13, color="#333", family="Segoe UI")
# HOVER_TMPL = "<b>%{label}</b><br>%{y:,.0f} registrations<br>Year: %{x}"

# # -----------------------------------------------------
# # üîπ UNIVERSAL SAFE WRAPPER
# # -----------------------------------------------------
# def _safe_df(df: pd.DataFrame, required: list[str]) -> pd.DataFrame:
#     if df is None or df.empty:
#         st.warning("‚ö†Ô∏è Empty dataframe provided to chart renderer.")
#         return pd.DataFrame(columns=required)
#     missing = [c for c in required if c not in df.columns]
#     if missing:
#         st.warning(f"‚ö†Ô∏è Missing columns for chart: {missing}")
#         for c in missing:
#             df[c] = np.nan
#     return df


# # -----------------------------------------------------
# # üü¶ MAXED BAR CHART (COMBINED / STACKED / NORMALIZED)
# # -----------------------------------------------------
# def _bar_from_df(
#     df: pd.DataFrame,
#     title: str,
#     combined: bool = False,
#     stacked: bool = False,
#     normalized: bool = False,
#     section_id: str = "",
#     color_field: str = "year",
# ):
#     """Render a polished bar chart with advanced legend, hover, and adaptive layout."""
#     try:
#         df = _safe_df(df, ["label", "value"])
#         unique_key = f"barmaxed_{section_id}_{uuid.uuid4().hex[:6]}"

#         if df.empty:
#             st.info("‚ÑπÔ∏è No data to render.")
#             return

#         if combined and color_field in df.columns:
#             mode = "stack" if stacked else "group"
#             fig = px.bar(
#                 df,
#                 x="label",
#                 y="value",
#                 color=color_field,
#                 barmode=mode,
#                 text_auto=True,
#                 title=title,
#                 color_discrete_sequence=MAXED_COLORS,
#             )
#         else:
#             fig = px.bar(
#                 df,
#                 x="label",
#                 y="value",
#                 color="label",
#                 text_auto=True,
#                 title=title,
#                 color_discrete_sequence=MAXED_COLORS,
#             )

#         # --- Normalization (percentage)
#         if normalized:
#             fig.for_each_trace(
#                 lambda t: t.update(y=t.y / np.sum(t.y) * 100 if np.sum(t.y) > 0 else t.y)
#             )
#             fig.update_yaxes(title_text="Share (%)")

#         # --- Layout polish
#         fig.update_layout(
#             template="plotly_white",
#             title_font=TITLE_FONT,
#             legend_title_text="",
#             legend=dict(
#                 orientation="h",
#                 yanchor="bottom",
#                 y=-0.25,
#                 xanchor="center",
#                 x=0.5,
#                 bgcolor="rgba(250,250,250,0.9)",
#                 bordercolor="rgba(0,0,0,0.1)",
#                 borderwidth=1,
#             ),
#             xaxis_title="Category / Maker",
#             yaxis_title="Registrations",
#             margin=dict(t=60, b=80, l=40, r=30),
#             bargap=0.15,
#             height=480,
#         )

#         # --- Enhanced hover
#         fig.update_traces(
#             hovertemplate="<b>%{x}</b><br>Registrations: %{y:,.0f}<extra></extra>",
#             textfont=LABEL_FONT,
#         )

#         st.plotly_chart(fig, use_container_width=True, key=unique_key)

#     except Exception as e:
#         st.warning(f"‚ö†Ô∏è MAXED Bar chart failed: {e}")


# # -----------------------------------------------------
# # üü£ MAXED PIE / DONUT CHART (3D FEEL + CENTER LABEL)
# # -----------------------------------------------------
# def _pie_from_df(
#     df: pd.DataFrame,
#     title: str,
#     section_id: str = "",
#     donut: bool = True,
#     legend: bool = True,
# ):
#     """Render a fully-styled donut/pie chart with hover & label polish."""
#     try:
#         df = _safe_df(df, ["label", "value"])
#         unique_key = f"piemaxed_{section_id}_{uuid.uuid4().hex[:6]}"

#         if df.empty or df["value"].sum() <= 0:
#             st.info("‚ÑπÔ∏è No valid data for pie chart.")
#             return

#         fig = px.pie(
#             df,
#             names="label",
#             values="value",
#             hole=0.45 if donut else 0,
#             title=title,
#             color_discrete_sequence=MAXED_COLORS,
#         )

#         fig.update_traces(
#             textinfo="percent+label",
#             hovertemplate="<b>%{label}</b><br>%{value:,.0f} registrations<br>%{percent}",
#             pull=[0.05] * len(df),
#         )

#         # Center label for donut
#         if donut:
#             total = df["value"].sum()
#             fig.add_annotation(
#                 text=f"<b>{total:,.0f}</b><br><span style='font-size:12px;color:#666'>Total</span>",
#                 showarrow=False,
#                 font=dict(size=14),
#                 x=0.5,
#                 y=0.5,
#             )

#         fig.update_layout(
#             template="plotly_white",
#             title_font=TITLE_FONT,
#             margin=dict(t=60, b=40, l=40, r=40),
#             showlegend=legend,
#             height=420,
#             legend_title_text="",
#         )

#         st.plotly_chart(fig, use_container_width=True, key=unique_key)

#     except Exception as e:
#         st.warning(f"‚ö†Ô∏è MAXED Pie chart failed: {e}")


# # -----------------------------------------------------
# # üü® MAXED LINE / TREND CHART
# # -----------------------------------------------------
# def _line_from_df(
#     df: pd.DataFrame,
#     x_field: str = "year",
#     y_field: str = "value",
#     color_field: str = "label",
#     title: str = "",
#     section_id: str = "",
#     smooth: bool = False,
# ):
#     """Render a smooth trend line chart with multi-series overlay."""
#     try:
#         df = _safe_df(df, [x_field, y_field, color_field])
#         unique_key = f"linemaxed_{section_id}_{uuid.uuid4().hex[:6]}"

#         if df.empty:
#             st.info("‚ÑπÔ∏è No data to render trend.")
#             return

#         fig = px.line(
#             df,
#             x=x_field,
#             y=y_field,
#             color=color_field,
#             markers=True,
#             title=title,
#             color_discrete_sequence=MAXED_COLORS,
#         )

#         if smooth:
#             # Basic moving average smoothing for visual
#             df_sorted = df.sort_values(x_field)
#             fig.data = []
#             for lbl in df[color_field].unique():
#                 sub = df_sorted[df_sorted[color_field] == lbl]
#                 sub["smoothed"] = sub[y_field].rolling(3, min_periods=1).mean()
#                 fig.add_trace(go.Scatter(
#                     x=sub[x_field], y=sub["smoothed"],
#                     name=lbl, mode="lines+markers"
#                 ))

#         fig.update_layout(
#             template="plotly_white",
#             title_font=TITLE_FONT,
#             xaxis_title=x_field.capitalize(),
#             yaxis_title=y_field.capitalize(),
#             legend=dict(orientation="h", y=-0.3, x=0.5, xanchor="center"),
#             margin=dict(t=60, b=60, l=40, r=30),
#             height=460,
#         )

#         st.plotly_chart(fig, use_container_width=True, key=unique_key)

#     except Exception as e:
#         st.warning(f"‚ö†Ô∏è MAXED Line chart failed: {e}")


# # -----------------------------------------------------
# # FETCH FUNCTION (ROBUST + MOCK FALLBACK)
# # -----------------------------------------------------
# # =====================================================
# # üöÄ ALL-MAXED MAKER FETCH & VISUAL MODULE
# # =====================================================

# # -----------------------------------------------------
# # üîß FETCH FUNCTION ‚Äî SAFE + SMART + MOCK-RESILIENT
# # -----------------------------------------------------
# def fetch_maker_year(year: int, params_common: dict):
#     """Fetch top vehicle makers for a given year ‚Äî fully maxed with safe params + mock fallback."""
#     logger.info(Fore.CYAN + f"üöÄ Fetching top makers for {year}...")

#     # --- Safe param cleanup ---
#     safe_params = params_common.copy()
#     safe_params["fromYear"] = year
#     safe_params["toYear"] = year

#     for k in ["fitnessCheck", "stateCode", "rtoCode", "vehicleType"]:
#         if k in safe_params and (
#             safe_params[k] in ["ALL", "0", "", None, False]
#         ):
#             safe_params.pop(k, None)

#     mk_json, mk_url = None, None
#     try:
#         mk_json, mk_url = get_json("vahandashboard/top5Makerchart", safe_params)
#     except Exception as e:
#         logger.error(Fore.RED + f"‚ùå API fetch failed for {year}: {e}")
#         mk_json, mk_url = None, "MOCK://top5Makerchart"

#     # --- Status caption ---
#     color = "orange" if mk_url and "MOCK" in mk_url else "green"
#     st.markdown(
#         f"üîó **API ({year}):** <span style='color:{color}'>{mk_url or 'N/A'}</span>",
#         unsafe_allow_html=True,
#     )

#     with st.expander(f"üß© JSON Debug ‚Äî {year}", expanded=False):
#         st.json(mk_json)

#     # --- Validation: check for expected fields ---
#     is_valid = False
#     df = pd.DataFrame()

#     if isinstance(mk_json, dict):
#         # ‚úÖ Case 1: Chart.js-style JSON
#         if "datasets" in mk_json and "labels" in mk_json:
#             data_values = mk_json["datasets"][0].get("data", [])
#             labels = mk_json.get("labels", [])
#             if data_values and labels:
#                 df = pd.DataFrame({"label": labels, "value": data_values})
#                 is_valid = True

#         # ‚úÖ Case 2: API returned dict with "data" or "result"
#         elif "data" in mk_json:
#             df = pd.DataFrame(mk_json["data"])
#             is_valid = not df.empty
#         elif "result" in mk_json:
#             df = pd.DataFrame(mk_json["result"])
#             is_valid = not df.empty

#     elif isinstance(mk_json, list) and mk_json:
#         # ‚úÖ Case 3: Direct list of records
#         df = pd.DataFrame(mk_json)
#         is_valid = not df.empty

#     # --- Handle missing or invalid data ---
#     if not is_valid or df.empty:
#         logger.warning(Fore.YELLOW + f"‚ö†Ô∏è Using mock data for {year}")
#         st.warning(f"‚ö†Ô∏è No valid API data for {year}, generating mock values.")
#         random.seed(year)

#         makers = [
#             "Maruti Suzuki", "Tata Motors", "Hyundai", "Mahindra", "Hero MotoCorp",
#             "Bajaj Auto", "TVS Motor", "Honda", "Kia", "Toyota", "Renault",
#             "Ashok Leyland", "MG Motor", "Eicher", "Piaggio", "BYD", "Olectra", "Force Motors"
#         ]
#         random.shuffle(makers)
#         top = makers[:10]
#         base = random.randint(200_000, 1_000_000)
#         growth = 1 + (year - 2020) * 0.06
#         df = pd.DataFrame({
#             "label": top,
#             "value": [int(base * random.uniform(0.5, 1.5) * growth) for _ in top]
#         })
#     else:
#         st.success(f"‚úÖ Valid API data loaded for {year}")

#     # --- Normalize columns ---
#     df.columns = [c.lower() for c in df.columns]
#     df["year"] = year
#     df = df.sort_values("value", ascending=False)

#     # --- Visual output ---
#     if not df.empty:
#         st.info(f"üèÜ **{year}** ‚Üí **{df.iloc[0]['label']}** ‚Äî {df.iloc[0]['value']:,} registrations")
#         _bar_from_df(df, f"Top Makers ({year})", combined=False)
#         _pie_from_df(df, f"Maker Share ({year})")

#     return df
# # -----------------------------------------------------
# # üîÅ MAIN LOOP ‚Äî MULTI-YEAR FETCH
# # -----------------------------------------------------
# all_years = []
# with st.spinner("‚è≥ Fetching maker data for all selected years..."):
#     for y in years:
#         try:
#             dfy = fetch_maker_year(y, params_common)   # ‚úÖ FIXED: pass params_common
#             all_years.append(dfy)
#         except Exception as e:
#             st.error(f"‚ùå {y} fetch error: {e}")
#             logger.error(Fore.RED + f"Fetch error {y}: {e}")

# # ===============================================================
# # üöò MAKER ANALYTICS ‚Äî FULLY MAXED + SAFE + DEBUG READY
# # ===============================================================
# import pandas as pd, numpy as np, math, time, random
# import streamlit as st
# import plotly.express as px
# import plotly.graph_objects as go
# from colorama import Fore
# from sklearn.linear_model import LinearRegression
# from dateutil.relativedelta import relativedelta

# # ----------------------------------
# # üß© Helpers
# # ----------------------------------
# def normalize_freq_rule(freq):
#     return {"Daily": "D", "Monthly": "M", "Quarterly": "Q"}.get(freq, "Y")

# def year_to_timeseries_maker(df_year, year, freq):
#     """Convert maker-year totals into evenly distributed synthetic timeseries."""
#     rule = normalize_freq_rule(freq)
#     idx = pd.date_range(
#         start=f"{year}-01-01",
#         end=f"{year}-12-31",
#         freq=("D" if freq == "Daily" else "M"),
#     )
#     rows = []
#     for _, r in df_year.iterrows():
#         maker = r.get("label", f"Maker_{_}")
#         total = float(r.get("value", 0))
#         per = total / max(1, len(idx))
#         for ts in idx:
#             rows.append({"ds": ts, "label": maker, "value": per, "year": year})
#     return pd.DataFrame(rows)

# # ===============================================================
# # üß≠ FETCH MAKER DATA (PER YEAR)
# # ===============================================================
# def fetch_maker_year(year: int, params_common: dict):
#     """Fetch top vehicle makers for a given year ‚Äî fully maxed with safe params + mock fallback."""
#     logger.info(Fore.CYAN + f"üöÄ Fetching top makers for {year}...")

#     safe_params = params_common.copy()
#     safe_params["fromYear"] = year
#     safe_params["toYear"] = year

#     mk_json, mk_url = None, None
#     try:
#         mk_json, mk_url = get_json("vahandashboard/top5Makerchart", safe_params)
#     except Exception as e:
#         logger.error(Fore.RED + f"‚ùå API fetch failed for {year}: {e}")
#         mk_json, mk_url = None, "MOCK://top5Makerchart"

#     color = "orange" if mk_url and "MOCK" in mk_url else "green"
#     st.markdown(f"üîó **API ({year}):** <span style='color:{color}'>{mk_url or 'N/A'}</span>", unsafe_allow_html=True)

#     with st.expander(f"üß© JSON Debug ‚Äî {year}", expanded=False):
#         st.json(mk_json)

#     if not mk_json or (isinstance(mk_json, dict) and not mk_json.get("datasets")):
#         st.warning(f"‚ö†Ô∏è No valid API data for {year}, generating mock values.")
#         random.seed(year)
#         makers = [
#             "Maruti Suzuki", "Tata Motors", "Hyundai", "Mahindra", "Hero MotoCorp",
#             "Bajaj Auto", "TVS Motor", "Honda", "Kia", "Toyota", "Renault",
#             "Ashok Leyland", "MG Motor", "Eicher", "Piaggio", "BYD", "Olectra", "Force Motors"
#         ]
#         random.shuffle(makers)
#         mk_json = {
#             "datasets": [{"data": [random.randint(200_000, 1_200_000) for _ in range(5)],
#                           "label": "Vehicle Registered"}],
#             "labels": makers[:5]
#         }

#     # --- Normalize API data
#     if isinstance(mk_json, dict) and "datasets" in mk_json:
#         data = mk_json["datasets"][0]["data"]
#         labels = mk_json["labels"]
#         mk_json = [{"label": l, "value": v} for l, v in zip(labels, data)]

#     df = pd.DataFrame(mk_json)
#     df.columns = [c.lower() for c in df.columns]
#     df["year"] = year
#     df = df.sort_values("value", ascending=False)

#     if not df.empty:
#         st.info(f"üèÜ **{year}** ‚Üí **{df.iloc[0]['label']}** ‚Äî {df.iloc[0]['value']:,} registrations")
#         _bar_from_df(df, f"Top Makers ({year})", combined=False)
#         _pie_from_df(df, f"Maker Share ({year})")
#     return df

# # ===============================================================
# # ‚è≥ MULTI-YEAR DATA COLLECTION
# # ===============================================================
# with st.spinner("Fetching maker data for selected years..."):
#     all_year_dfs = []
#     for y in years:
#         try:
#             df_y = fetch_maker_year(y, params_common)
#             if df_y is not None and not df_y.empty:
#                 all_year_dfs.append(df_y)
#             else:
#                 st.warning(f"No data for {y}")
#         except Exception as e:
#             st.error(f"Error fetching {y}: {e}")

# if not all_year_dfs:
#     st.error("üö´ No maker data loaded for selected range.")
#     st.stop()

# df_maker_all = pd.concat(all_year_dfs, ignore_index=True)

# # ===============================================================
# # üìà TIME SERIES & METRICS
# # ===============================================================
# rule = normalize_freq_rule(freq)
# ts_frames = [year_to_timeseries_maker(df_maker_all[df_maker_all["year"] == y], y, freq)
#              for y in sorted(df_maker_all["year"].unique())]
# df_ts = pd.concat(ts_frames, ignore_index=True)
# df_ts["ds"] = pd.to_datetime(df_ts["ds"])

# resampled = df_ts.groupby(["label", pd.Grouper(key="ds", freq=rule)])["value"].sum().reset_index()
# resampled["year"] = resampled["ds"].dt.year
# pivot = resampled.pivot_table(index="ds", columns="label", values="value", aggfunc="sum").fillna(0)
# pivot_year = resampled.pivot_table(index="year", columns="label", values="value", aggfunc="sum").fillna(0)

# # ===============================================================
# # üíé KPI METRICS ‚Äî Makers
# # ===============================================================
# st.subheader("üíé Key Metrics & Growth (Makers)")

# if pivot_year.empty:
#     st.warning("‚ö†Ô∏è No yearly data found for KPI computation.")
#     st.stop()

# # --- Compute totals and YoY
# year_totals = pivot_year.sum(axis=1).rename("TotalRegistrations").to_frame()
# year_totals["YoY_%"] = year_totals["TotalRegistrations"].pct_change() * 100
# year_totals["TotalRegistrations"] = year_totals["TotalRegistrations"].fillna(0).astype(int)
# year_totals["YoY_%"] = year_totals["YoY_%"].replace([np.inf, -np.inf], np.nan).fillna(0)

# # --- CAGR
# if len(year_totals) >= 2:
#     first = float(year_totals["TotalRegistrations"].iloc[0])
#     last = float(year_totals["TotalRegistrations"].iloc[-1])
#     years_count = max(1, len(year_totals) - 1)
#     cagr = ((last / first) ** (1 / years_count) - 1) * 100 if first > 0 else 0.0
# else:
#     cagr = 0.0

# # --- MoM (if monthly)
# if freq == "Monthly":
#     resampled["month_period"] = resampled["year"].astype(str) + "-" + resampled["month"].astype(str)
#     month_totals = resampled.groupby("month_period")["value"].sum().reset_index()
#     month_totals["MoM_%"] = month_totals["value"].pct_change() * 100
#     latest_mom = f"{month_totals['MoM_%'].iloc[-1]:.2f}%" if len(month_totals) > 1 and not np.isnan(month_totals["MoM_%"].iloc[-1]) else "n/a"
# else:
#     latest_mom = "n/a"

# # --- Category (Maker) shares
# latest_year = int(year_totals.index.max())
# latest_total = int(year_totals.loc[latest_year, "TotalRegistrations"])
# cat_share = (pivot_year.loc[latest_year] / pivot_year.loc[latest_year].sum() * 100).sort_values(ascending=False).round(1)

# # ----------------------------------------------------
# # DISPLAY KPIs
# # ----------------------------------------------------
# c1, = st.columns(1)
# c1.metric("üìÖ Years Loaded", f"{years[0]} ‚Üí {years[-1]}", f"{len(years)} yrs")

# st.markdown("#### üìò Maker Share (Latest Year)")
# st.dataframe(
#     pd.DataFrame({
#         "Maker": cat_share.index,
#         "Share_%": cat_share.values,
#         "Volume": pivot_year.loc[latest_year].astype(int).values
#     }).sort_values("Share_%", ascending=False),
#     use_container_width=True
# )

# with st.expander("üîç Yearly Totals & Growth"):
#     st.dataframe(year_totals.style.format({"TotalRegistrations": "{:,}", "YoY_%": "{:.2f}"}))

# # ----------------------------------------------------
# # DEEP INSIGHTS + TRENDS
# # ----------------------------------------------------
# total_all = df_maker_all["value"].sum()
# n_cats = df_maker_all["label"].nunique()
# n_years = df_maker_all["year"].nunique()

# # --- Top Maker
# top_cat_row = df_maker_all.groupby("label")["value"].sum().reset_index().sort_values("value", ascending=False).iloc[0]
# top_cat = {"label": str(top_cat_row["label"]), "value": float(top_cat_row["value"])}
# top_cat_share = (top_cat["value"] / total_all) * 100 if total_all > 0 else 0

# # --- Top Year
# top_year_row = df_maker_all.groupby("year")["value"].sum().reset_index().sort_values("value", ascending=False).iloc[0]
# top_year = {"year": int(top_year_row["year"]), "value": float(top_year_row["value"])}

# st.metric("üèÜ Absolute Top Maker", top_cat["label"], f"{top_cat_share:.2f}% share")
# st.metric("üìÖ Peak Year", f"{top_year['year']}", f"{top_year['value']:,.0f} registrations")

# # --- Plot: Top 10 Makers
# st.write("### üßæ Top 10 Makers ‚Äî Overall")
# top_debug = df_maker_all.groupby("label")["value"].sum().reset_index().sort_values("value", ascending=False)
# fig_top10 = px.bar(top_debug.head(10), x="label", y="value", text_auto=True,
#                    color="value", color_continuous_scale="Blues", title="Top 10 Makers (All Years)")
# fig_top10.update_layout(template="plotly_white", margin=dict(t=50, b=40))
# st.plotly_chart(fig_top10, use_container_width=True)

# # ----------------------------------------------------
# # ADVANCED DEBUG METRICS
# # ----------------------------------------------------
# volatility = df_maker_all.groupby("year")["value"].sum().pct_change().std() * 100 \
#     if len(df_maker_all["year"].unique()) > 2 else 0
# dominance_ratio = (top_cat["value"] / total_all) * n_cats if total_all > 0 else 0
# direction = "increased" if cagr > 0 else "declined"

# summary_time = time.time() - summary_start
# st.markdown("### ‚öôÔ∏è Debug Performance Metrics")
# st.code(f"""
# Years analyzed: {years}
# Makers: {n_cats}
# Rows processed: {len(df_maker_all):,}
# Total registrations: {total_all:,.0f}
# Top maker: {top_cat['label']} ‚Üí {top_cat['value']:,.0f} ({top_cat_share:.2f}%)
# Peak year: {int(top_year['year'])} ‚Üí {top_year['value']:,.0f}
# Dominance ratio: {dominance_ratio:.2f}
# Runtime: {summary_time:.2f}s
# """, language="yaml")

# # ----------------------------------------------------
# # 8Ô∏è‚É£ SMART SUMMARY ‚Äî All-Maxed
# # ----------------------------------------------------
# if isinstance(top_cat, list):
#     top_cat = top_cat[0] if top_cat else {"label": "N/A", "value": 0}

# years_valid = years is not None and len(years) > 0
# top_year_valid = top_year is not None and "year" in top_year and "value" in top_year

# if top_cat and years_valid and top_year_valid:
#     st.success(
#         f"From **{years[0]}** to **{years[-1]}**, total registrations {direction}. "
#         f"**{top_cat.get('label', 'N/A')}** leads with **{top_cat_share:.2f}%** share. "
#         f"Peak year: **{top_year['year']}** with **{top_year['value']:,.0f}** registrations."
#     )
#     logger.info(f"‚úÖ ALL-MAXED summary completed in {summary_time:.2f}s")
# else:
#     st.error("‚õî ALL-MAXED summary failed: Missing or invalid data.")
#     logger.warning("‚ö†Ô∏è ALL-MAXED summary skipped due to incomplete data")

# ----------------------------------------------------
# 9Ô∏è‚É£ MAKERS + STATES SECTIONS (reuse same pattern)
# ----------------------------------------------------
# Repeat the above block for Makers and States by replacing:
# df_src ‚Üí df_maker_all / df_state_all
# pivot_year ‚Üí pivot_maker_year / pivot_state_year
# top_cat ‚Üí top_maker / top_state
# top_cat_share ‚Üí top_mk_share / top_state_share
# Adjust headings & success messages accordingly

    # ----------------------------------------------------
    # 9Ô∏è‚É£ MAKERS + STATES SECTIONS (reuse same pattern)
    # ----------------------------------------------------
    # repeat the above block for Makers/States as needed


# =========================================================
# üî• ALL-MAXED ‚Äî Maker (multi-frequency, multi-year) ‚Äî MAXED BLOCK
# =========================================================
import time
import math
import json
import random
import logging
import requests
from typing import Any, Dict, List, Optional, Tuple
from datetime import datetime
from dateutil.relativedelta import relativedelta

import numpy as np
import pandas as pd
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go

# -------------------------
# Logging Setup
# -------------------------
logger = logging.getLogger("all_maxed_maker")
if not logger.handlers:
    handler = logging.StreamHandler()
    handler.setFormatter(logging.Formatter("%(asctime)s | %(levelname)s | %(message)s"))
    logger.addHandler(handler)
logger.setLevel(logging.DEBUG)

print("\n" + "=" * 80)
print("[ALL-MAXED] üöó Starting MAKER analytics control block setup")
print(f"[TIME] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("=" * 80)

# -------------------------
# ALL-MAXED UI STARTING BANNER
# -------------------------
st.markdown(
    f"""
    <div style="
        text-align:center;
        padding:18px;
        border-radius:12px;
        background:linear-gradient(90deg,#ff6a00,#ee0979);
        color:white;
        font-size:18px;
        font-weight:bold;
        box-shadow: 0 0 12px rgba(0,0,0,0.4);
    ">
        üöÄ ALL-MAXED MAKER Analytics Block Started ‚Äî {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
    </div>
    """,
    unsafe_allow_html=True
)

st.divider()  # optional visual separation

# =====================================================
# CONTROLS ‚Äî ALL ON MAIN PAGE (no sidebar)
# =====================================================

section_id = "main"
print(f"[INFO] Section ID ‚Üí {section_id}")

# -------------------------------
# FREQUENCY & VIEW MODE
# -------------------------------
freq = st.radio(
    "Aggregation Frequency",
    ["Daily", "Monthly", "Quarterly", "Yearly"],
    index=3,
    horizontal=True,
    key=f"freq_{section_id}"
)
print(f"[CONTROL] Frequency selected ‚Üí {freq}")

mode = st.radio(
    "View Mode",
    ["Separate (Small Multiples)", "Combined (Overlay / Stacked)"],
    index=1,
    horizontal=True,
    key=f"mode_{section_id}"
)
print(f"[CONTROL] View Mode selected ‚Üí {mode}")

# -------------------------------
# YEAR RANGE
# -------------------------------
today = datetime.now()
current_year = today.year
default_from_year = current_year - 1
print(f"[INFO] Current year detected ‚Üí {current_year}")

from_year = st.sidebar.number_input(
    "From Year",
    min_value=2012,
    max_value=current_year,
    value=default_from_year,
    key=f"from_year_{section_id}"
)
print(f"[CONTROL] From Year ‚Üí {from_year}")

to_year = st.sidebar.number_input(
    "To Year",
    min_value=from_year,
    max_value=current_year,
    value=current_year,
    key=f"to_year_{section_id}"
)
print(f"[CONTROL] To Year ‚Üí {to_year}")

# -------------------------------
# LOCATION & VEHICLE FILTERS
# -------------------------------
state_code = st.sidebar.text_input(
    "State Code (blank=All-India)",
    value="",
    key=f"state_{section_id}"
)
print(f"[FILTER] State Code ‚Üí '{state_code or 'All-India'}'")

rto_code = st.sidebar.text_input(
    "RTO Code (0=aggregate)",
    value="0",
    key=f"rto_{section_id}"
)
print(f"[FILTER] RTO Code ‚Üí {rto_code}")

vehicle_classes = st.sidebar.text_input(
    "Vehicle Classes (e.g., 2W,3W,4W)",
    value="",
    key=f"classes_{section_id}"
)
print(f"[FILTER] Vehicle Classes ‚Üí '{vehicle_classes or 'All'}'")

vehicle_makers = st.sidebar.text_input(
    "Vehicle Makers (comma-separated or IDs)",
    value="",
    key=f"makers_{section_id}"
)
print(f"[FILTER] Vehicle Makers ‚Üí '{vehicle_makers or 'All'}'")

vehicle_type = st.sidebar.text_input(
    "Vehicle Type (optional)",
    value="",
    key=f"type_{section_id}"
)
print(f"[FILTER] Vehicle Type ‚Üí '{vehicle_type or 'All'}'")

time_period = st.sidebar.selectbox(
    "Time Period",
    options=[0, 1, 2],
    index=0,
    key=f"period_{section_id}"
)
print(f"[FILTER] Time Period selected ‚Üí {time_period}")

fitness_check = st.sidebar.selectbox(
    "Fitness Check",
    options=[True, False],
    index=0,
    format_func=lambda x: "Enabled" if x else "Disabled",
    key=f"fitness_{section_id}"
)
print(f"[FILTER] Fitness Check ‚Üí {'Enabled' if fitness_check else 'Disabled'}")

# -------------------------------
# EXTRA FEATURE TOGGLES
# -------------------------------
st.divider()
col1, col2, col3 = st.columns(3)

with col1:
    show_heatmap = st.checkbox("Show Heatmap (year √ó maker)", True, key=f"heatmap_{section_id}")
    show_radar = st.checkbox("Show Radar (per year)", True, key=f"radar_{section_id}")
print(f"[OPTION] Heatmap ‚Üí {show_heatmap}, Radar ‚Üí {show_radar}")

with col2:
    do_forecast = st.checkbox("Enable Forecasting", True, key=f"forecast_{section_id}")
    do_anomaly = st.checkbox("Enable Anomaly Detection", False, key=f"anomaly_{section_id}")
print(f"[OPTION] Forecasting ‚Üí {do_forecast}, Anomaly Detection ‚Üí {do_anomaly}")

with col3:
    do_clustering = st.checkbox("Enable Clustering (KMeans)", False, key=f"cluster_{section_id}")
print(f"[OPTION] Clustering ‚Üí {do_clustering}")

# -------------------------------
# DERIVE YEARS
# -------------------------------
years = list(range(int(from_year), int(to_year) + 1))
print(f"[INFO] Years derived ‚Üí {years}")
print(f"[SUMMARY] freq={freq} | mode={mode} | range={from_year}-{to_year}")
print("=" * 80 + "\n")


# =====================================================
# üöÄ ALL-MAXED MAKERS ANALYTICS CORE v1.0
# -----------------------------------------------------
# Fully loaded mock data, chart builders, and analytics UI helpers for Makers
# =====================================================

import streamlit as st
import pandas as pd
import numpy as np
import random, uuid
import plotly.express as px
from datetime import datetime
from typing import Dict, Any
from plotly.colors import qualitative

# -----------------------------------------------------
# üéØ Master Maker Reference
# -----------------------------------------------------
MAKERS_MASTER = [
    "Maruti Suzuki", "Tata Motors", "Hyundai", "Mahindra", "Hero MotoCorp",
    "Bajaj Auto", "TVS Motor", "Honda", "Kia", "Toyota", "Renault",
    "Ashok Leyland", "MG Motor", "Eicher", "Piaggio", "BYD", "Olectra", "Force Motors"
]

# ‚úÖ Debug Print
print(f"[ALL-MAXED MAKERS] Loaded {len(MAKERS_MASTER)} makers at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

# -----------------------------------------------------
# üíæ Deterministic Mock Data Generator (Multi-Frequency)
# -----------------------------------------------------
def deterministic_mock_makers(year: int, freq: str = "Monthly", seed_base: str = "makers") -> Dict[str, Any]:
    """Generate reproducible, realistic mock data for makers (daily, monthly, yearly)."""
    rnd = random.Random(hash((year, seed_base)) & 0xFFFFFFFF)
    data = []

    if freq == "Yearly":
        for m in MAKERS_MASTER:
            val = rnd.randint(50_000, 2_500_000)
            data.append({"label": m, "value": val, "year": year})

    elif freq == "Monthly":
        for month in range(1, 13):
            for m in MAKERS_MASTER:
                base = rnd.randint(10_000, 200_000)
                seasonal_boost = 1.2 if month in [3, 9, 12] else 1.0
                value = int(base * seasonal_boost * (0.8 + rnd.random() * 0.5))
                data.append({
                    "label": m,
                    "value": value,
                    "year": year,
                    "month": month,
                    "month_name": datetime(year, month, 1).strftime("%b")
                })

    elif freq == "Daily":
        for month in range(1, 13):
            for day in range(1, 29):
                for m in MAKERS_MASTER:
                    base = rnd.randint(200, 15_000)
                    val = int(base * (0.8 + rnd.random() * 0.6))
                    data.append({
                        "label": m, "value": val, "year": year, "month": month, "day": day
                    })

    print(f"[MOCK MAKERS] ‚úÖ Generated {len(data):,} rows for {year} ({freq}) at {datetime.now().strftime('%H:%M:%S')}")

    return {
        "data": data,
        "meta": {
            "generatedAt": datetime.utcnow().isoformat(),
            "note": f"deterministic mock for {year} ({freq})",
            "freq": freq
        }
    }


def format_number(n):
    """Return number formatted as K, M, or Cr."""
    if n >= 10_000_000:
        out = f"{n/10_000_000:.2f} Cr"
    elif n >= 100_000:
        out = f"{n/100_000:.2f} L"
    elif n >= 1_000:
        out = f"{n/1_000:.2f} K"
    else:
        out = f"{n:,}"

    print(f"[FORMAT] {n:,} ‚Üí {out}")
    return out

# -----------------------------------------------------
# üé® Global Chart Style Settings
# -----------------------------------------------------
COLOR_PALETTE = qualitative.Plotly + qualitative.D3 + qualitative.Vivid
DEFAULT_TEMPLATE = "plotly_white"
TITLE_STYLE = dict(size=20, color="#111", family="Segoe UI Semibold")

print(f"[CHART STYLE] Palette loaded: {len(COLOR_PALETTE)} colors | Template: {DEFAULT_TEMPLATE} | Title style: {TITLE_STYLE}")

# -----------------------------------------------------
# üß© MAXED CHART HELPERS (Legend, Hover, UI polish)
# -----------------------------------------------------
def _unique_key(prefix="chart"):
    key = f"{prefix}_{uuid.uuid4().hex[:6]}"
    print(f"[DEBUG] Generated unique chart key ‚Üí {key}")
    return key

# ===============================================================
# üöÄ ALL-MAXED MAKERS ANALYTICS DASHBOARD
# ===============================================================
from datetime import datetime
import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import numpy as np

# -----------------------------
# Header / Banner
# -----------------------------
st.markdown(
    f"""
    <div style="
        text-align:center;
        padding:20px;
        border-radius:12px;
        background:linear-gradient(90deg,#ff6a00,#ee0979);
        color:white;
        font-size:20px;
        font-weight:bold;
        box-shadow: 0 0 15px rgba(0,0,0,0.5);
    ">
        üöó ALL-MAXED MAKERS Analytics Dashboard ‚Äî {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
    </div>
    """,
    unsafe_allow_html=True
)
st.divider()

# -----------------------------
# Summary Metrics
# -----------------------------
def makers_summary(df: pd.DataFrame):
    if df is None or df.empty:
        st.warning("‚ö†Ô∏è No summary data available.")
        return
    
    total = int(df['value'].sum())
    mean = int(df['value'].mean())
    median = int(df['value'].median())
    mode_val = int(df['value'].mode()[0]) if not df['value'].mode().empty else 0
    max_val = int(df['value'].max())
    min_val = int(df['value'].min())
    
    st.markdown("### üìä Summary Metrics ‚Äî All-Maxed")
    c1, c2, c3, c4, c5, c6 = st.columns(6)
    c1.metric("Total Registrations", f"{total:,}")
    c2.metric("Mean", f"{mean:,}")
    c3.metric("Median", f"{median:,}")
    c4.metric("Mode", f"{mode_val:,}")
    c5.metric("Max", f"{max_val:,}")
    c6.metric("Min", f"{min_val:,}")

# -----------------------------
# BAR CHART ‚Äî MAKERS (ALL-MAXED)
# -----------------------------
def bar_from_makers(df: pd.DataFrame, title="Top Makers", x="label", y="value",
                    color=None, height=550, section_id="bar", combined=False):
    if df is None or df.empty:
        st.warning("‚ö†Ô∏è No data to plot.")
        return

    barmode = "stack" if combined else "group"
    fig = px.bar(
        df, x=x, y=y, color=color or x, text_auto=".2s",
        title=title, color_discrete_sequence=COLOR_PALETTE,
        barmode=barmode
    )
    fig.update_traces(
        hovertemplate="<b>%{x}</b><br><b>%{y:,.0f}</b> registrations",
        textposition="outside",
        textfont_size=13,
        cliponaxis=False
    )
    fig.update_layout(
        template=DEFAULT_TEMPLATE,
        title_font=TITLE_STYLE,
        xaxis_title=x.title(),
        yaxis_title=y.title(),
        legend=dict(
            orientation="h", yanchor="bottom", y=1.02,
            xanchor="right", x=1, title=None, bgcolor="rgba(0,0,0,0)",
            font=dict(size=12)
        ),
        height=height, bargap=0.15,
        margin=dict(t=70, b=50, l=50, r=30)
    )
    st.plotly_chart(fig, use_container_width=True, key=_unique_key(section_id))

# -----------------------------
# PIE / DONUT CHART ‚Äî MAKERS (ALL-MAXED)
# -----------------------------
def pie_from_makers(df: pd.DataFrame, title="Maker Share", donut=True, section_id="pie", height=500):
    if df is None or df.empty:
        st.warning("‚ö†Ô∏è No data to plot.")
        return

    fig = px.pie(
        df, names="label", values="value", hole=0.5 if donut else 0.0,
        title=title, color_discrete_sequence=COLOR_PALETTE
    )
    fig.update_traces(
        textinfo="label+percent",
        hovertemplate="<b>%{label}</b><br><b>%{value:,.0f}</b> registrations<br>%{percent}",
        pull=[0.05] * len(df),
        marker=dict(line=dict(color="#fff", width=2))
    )
    fig.update_layout(
        template=DEFAULT_TEMPLATE,
        title_font=TITLE_STYLE,
        legend=dict(
            orientation="v", yanchor="top", y=0.95,
            xanchor="left", x=0, bgcolor="rgba(0,0,0,0)",
            font=dict(size=12)
        ),
        height=height, margin=dict(t=70, b=50, l=50, r=50)
    )
    st.plotly_chart(fig, use_container_width=True, key=_unique_key(section_id))

# -----------------------------
# TREND / LINE CHART ‚Äî MAKERS (ALL-MAXED)
# -----------------------------
def trend_from_makers(df: pd.DataFrame, title="Trend Over Time", section_id="trend", height=550):
    if df is None or df.empty:
        st.warning("‚ö†Ô∏è No trend data available.")
        return

    x_axis = "year"
    if "month_name" in df.columns:
        df["month_order"] = pd.Categorical(
            df["month_name"],
            categories=["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],
            ordered=True
        )
        x_axis = "month_order"

    fig = px.line(
        df, x=x_axis, y="value", color="label", markers=True,
        title=title, color_discrete_sequence=COLOR_PALETTE,
        line_shape="spline"
    )
    fig.update_traces(
        hovertemplate="<b>%{x}</b><br><b>%{y:,.0f}</b> registrations",
        marker=dict(size=8, line=dict(width=1, color='DarkSlateGrey'))
    )
    fig.update_layout(
        template=DEFAULT_TEMPLATE,
        title_font=TITLE_STYLE,
        legend=dict(
            orientation="h", yanchor="bottom", y=1.02,
            xanchor="right", x=1, bgcolor="rgba(0,0,0,0)",
            font=dict(size=12)
        ),
        xaxis=dict(showgrid=True, gridcolor="#eee"),
        yaxis=dict(showgrid=True, gridcolor="#eee"),
        height=height,
        margin=dict(t=70, b=50, l=50, r=50)
    )
    st.plotly_chart(fig, use_container_width=True, key=_unique_key(section_id))

# -----------------------------------------------------
# üß† Auto Dashboard Section ‚Äî Single Function (MAKERS)
# -----------------------------------------------------
def render_maker_dashboard(year: int, freq="Monthly"):
    """Render full UI: fetch mock makers data, show KPI, bar, pie, trend ‚Äî ALL-MAXED."""
    st.subheader(f"üìä Maker Distribution ‚Äî {year} ({freq})")
    print(f"[DEBUG] Rendering maker dashboard for year={year}, freq={freq}")

    # Generate deterministic data
    mock_json = deterministic_mock_makers(year, freq=freq)
    df = pd.DataFrame(mock_json["data"])
    print(f"[DEBUG] Mock data generated with {len(df)} rows")

    total = df["value"].sum()
    top = df.sort_values("value", ascending=False).iloc[0]
    st.success(f"üèÜ **Top Maker:** {top['label']} ‚Äî {format_number(top['value'])} registrations")
    st.caption(f"Total: {format_number(total)} | Generated: {mock_json['meta']['generatedAt']}")
    print(f"[DEBUG] Top maker: {top['label']} with {top['value']} registrations")

    # Layout 2-col + trend
    c1, c2 = st.columns([2, 1])
    with c1:
        print(f"[DEBUG] Rendering bar chart for year={year}")
        bar_from_makers(df, title=f"{year} {freq} Breakdown (Bar)", color="label", section_id=f"bar_{year}")
    with c2:
        print(f"[DEBUG] Rendering pie chart for year={year}")
        pie_from_makers(df, title=f"{year} Share (Donut)", section_id=f"pie_{year}")

    # Optional trend
    if "month_name" in df.columns:
        print(f"[DEBUG] Rendering monthly trend chart for year={year}")
        trend_from_makers(df, title=f"{year} Monthly Trend (Animated)", section_id=f"trend_{year}")
    else:
        print(f"[DEBUG] Rendering yearly trend chart for year={year}")
        trend_from_makers(df, title=f"{year} Maker Trend", section_id=f"trend_{year}")

    print(f"[DEBUG] Dashboard rendering complete for year={year}")
    return df


# ============================================================
# ‚öôÔ∏è Synthetic Timeseries Expansion ‚Äî MAKER ULTRA MAXED
# ------------------------------------------------------------
# Generates per-category realistic timeseries for Maker dashboards
# Includes trend, seasonality, noise, and deterministic reproducibility
# ============================================================

import numpy as np
import pandas as pd
from datetime import datetime

def maker_year_to_timeseries(
    df_year: pd.DataFrame,
    year: int,
    freq: str = "Monthly",
    trend_strength: float = 0.15,
    noise_strength: float = 0.10,
    seasonal_boost: bool = True,
    seed_base: str = "maker_timeseries"
) -> pd.DataFrame:
    """
    Expand annual category totals into Maker-ready realistic timeseries.
    
    Returns columns: ds, label, value, year, month, quarter, month_name, maker_key
    """
    if df_year is None or df_year.empty:
        return pd.DataFrame(columns=[
            "ds","label","value","year","month","quarter","month_name","maker_key"
        ])

    # --- deterministic seed for reproducibility
    seed = abs(hash((year, freq, seed_base))) % (2**32)
    rng = np.random.default_rng(seed)

    # --- time index generation
    start = pd.Timestamp(f"{year}-01-01")
    end = pd.Timestamp(f"{year}-12-31")
    freq = freq.capitalize()
    if freq == "Daily":
        idx = pd.date_range(start=start, end=end, freq="D")
    elif freq == "Monthly":
        idx = pd.date_range(start=start, end=end, freq="M")
    elif freq == "Quarterly":
        idx = pd.date_range(start=start, end=end, freq="Q")
    else:
        idx = pd.date_range(start=start, end=end, freq="Y")

    n = len(idx)
    rows = []

    # --- seasonal sinusoidal factor helper
    def seasonal_factor(i):
        if not seasonal_boost:
            return 1.0
        return 1.0 + 0.25 * np.sin((i / n) * 2 * np.pi * 4)  # 4 seasonal peaks

    for _, r in df_year.iterrows():
        cat = r.get("label", "Unknown")
        total = float(r.get("value", 0.0))
        if total <= 0:
            continue

        base_per = total / max(1, n)

        # trend (up/down) + noise
        trend = np.linspace(1 - trend_strength, 1 + trend_strength, n)
        if rng.random() > 0.5:
            trend = trend[::-1]
        noise = rng.normal(0, noise_strength, n)

        vals = base_per * trend * (1 + noise)
        vals = np.maximum(vals, 0)

        for i, ts in enumerate(idx):
            factor = seasonal_factor(i)
            v = vals[i] * factor
            rows.append({
                "ds": ts,
                "label": cat,
                "value": float(v),
                "year": int(year),
                "month": ts.month,
                "quarter": ts.quarter,
                "month_name": ts.strftime("%b"),
                "maker_key": f"{cat}_{ts.strftime('%Y%m%d')}"
            })

    df_out = pd.DataFrame(rows)
    df_out["value"] = df_out["value"].round(2)

    # --- normalize to match original annual totals
    grouped = df_out.groupby("label")["value"].sum().to_dict()
    for cat in grouped:
        original_total = float(df_year.loc[df_year["label"] == cat, "value"].iloc[0])
        if grouped[cat] > 0:
            df_out.loc[df_out["label"] == cat, "value"] *= original_total / grouped[cat]

    df_out.reset_index(drop=True, inplace=True)

    # --- Post-normalization ALL-MAXED debug summary
    total_all = df_out['value'].sum()
    top_cat_row = (
        df_out.groupby("label")["value"]
        .sum()
        .reset_index()
        .sort_values("value", ascending=False)
        .iloc[0]
    )
    top_cat_name = top_cat_row["label"]
    top_cat_value = top_cat_row["value"]
    top_cat_share = (top_cat_value / total_all * 100) if total_all > 0 else 0
    
    print("="*80)
    print(f"[ALL-MAXED] Year {year} TIMESERIES SUMMARY")
    print(f"Total registrations generated: {total_all:,.0f}")
    print(f"Top category: {top_cat_name} ‚Üí {top_cat_value:,.0f} ({top_cat_share:.2f}%)")
    print(f"Total rows: {len(df_out)}")
    print("="*80)
    
    return df_out

# ============================================================
# üöÄ MAKER DASHBOARD FETCHER ‚Äî ALL-MAXED ULTRA
# ------------------------------------------------------------
# Fetch Top 5 Makers, render bar + pie charts, insights, 
# and synthetic trends with fallback.
# ============================================================

import streamlit as st
import pandas as pd
import plotly.express as px
import numpy as np
from typing import Dict

# -----------------------------
# Fallback deterministic mock
# -----------------------------

def maker_mock_top5(year: int) -> Dict:
    """Return deterministic mock Top 5 Makers data with ALL-MAXED debug print."""
    data = [
        {"label": f"Maker {i}", "value": np.random.randint(500, 2000)}
        for i in range(1, 6)
    ]
    
    total = sum(d["value"] for d in data)
    top = max(data, key=lambda x: x["value"])
    
    print("="*80)
    print(f"[ALL-MAXED] Year {year} ‚Äî Top 5 Makers Mock Data")
    print(f"Total registrations (Top5 mock): {total}")
    print(f"Top Maker: {top['label']} ‚Üí {top['value']}")
    print("="*80)
    
    return {"data": data}

#-----------------------------
# MAXED Maker fetch + render
#-----------------------------
# -----------------------------------------------------
# üîß FETCH FUNCTION ‚Äî SAFE + SMART + MOCK-RESILIENT
# -----------------------------------------------------

import random
import pandas as pd
import streamlit as st
from colorama import Fore
import logging

logger = logging.getLogger(__name__)

# -----------------------------
# Build parameters
# -----------------------------
params_common = build_params(
    from_year=from_year,
    to_year=to_year,
    state_code=state_code or "ALL",
    rto_code=rto_code or "0",
    vehicle_classes=vehicle_classes or "ALL",
    vehicle_makers=vehicle_makers or "ALL",
    time_period=freq,
    fitness_check=fitness_check,
    vehicle_type=vehicle_type or "ALL"
)

# -----------------------------
# Derive years
# -----------------------------
years = list(range(int(from_year), int(to_year) + 1))

# -----------------------------
# Streamlit info
# -----------------------------
st.info(f"üîó Using parameters: {params_common}")

# -----------------------------
# ALL-MAXED Debug / Print Block
# -----------------------------
print("="*80)
print(f"{Fore.CYAN}[ALL-MAXED] Maker Fetch Debug{Fore.RESET}")
print("Parameters for fetch:")
for key, value in params_common.items():
    print(f"  {key}: {value}")
print(f"Years in scope: {years}")
print("="*80)

# -----------------------------
# Optional: Logger alternative
# -----------------------------
logger.info("[ALL-MAXED] Maker Fetch Debug")
logger.info(f"Parameters: {params_common}")
logger.info(f"Years in scope: {years}")


def fetch_maker_top5(year: int, params_common: dict, show_debug: bool = False):
    """
    Fetch top vehicle makers for a given year ‚Äî fully maxed with safe params + mock fallback.
    
    Parameters:
        year (int): Year to fetch
        params_common (dict): Parameters for the API
        show_debug (bool): If True, show raw JSON in an expander

    Returns:
        pd.DataFrame: DataFrame of top makers with 'label', 'value', 'year'
    """
    import random
    import pandas as pd
    import streamlit as st
    import logging
    from colorama import Fore

    logger = logging.getLogger(__name__)
    logger.info(Fore.CYAN + f"üöÄ Fetching top makers for {year}...")

    # --- Safe param cleanup ---
    safe_params = params_common.copy()
    safe_params["fromYear"] = year
    safe_params["toYear"] = year

    for k in ["fitnessCheck", "stateCode", "rtoCode", "vehicleType"]:
        if k in safe_params and safe_params[k] in ["ALL", "0", "", None, False]:
            safe_params.pop(k, None)

    # -----------------------------
    # Attempt API fetch
    # -----------------------------
    mk_json, mk_url = None, None
    try:
        mk_json, mk_url = get_json("vahandashboard/top5Makerchart", safe_params)
        print(f"[DEBUG] API response URL: {mk_url}")
        print(f"[DEBUG] API response keys: {list(mk_json.keys()) if mk_json else 'None'}")
    except Exception as e:
        logger.error(Fore.RED + f"‚ùå API fetch failed for {year}: {e}")
        mk_json, mk_url = None, "MOCK://top5Makerchart"

    # --- Status caption in Streamlit ---
    color = "orange" if mk_url and "MOCK" in mk_url else "green"
    st.markdown(
        f"üîó **API ({year}):** <span style='color:{color}'>{mk_url or 'N/A'}</span>",
        unsafe_allow_html=True,
    )

    # --- Optional debug ---
    if show_debug:
        with st.expander(f"üß© JSON Debug ‚Äî {year}", expanded=False):
            st.json(mk_json)

    # --- Validate JSON & extract data ---
    is_valid = False
    df = pd.DataFrame()

    if isinstance(mk_json, dict):
        if "datasets" in mk_json and "labels" in mk_json:
            data_values = mk_json["datasets"][0].get("data", [])
            labels = mk_json.get("labels", [])
            if data_values and labels:
                df = pd.DataFrame({"label": labels, "value": data_values})
                is_valid = True
        elif "data" in mk_json:
            df = pd.DataFrame(mk_json["data"])
            is_valid = not df.empty
        elif "result" in mk_json:
            df = pd.DataFrame(mk_json["result"])
            is_valid = not df.empty
    elif isinstance(mk_json, list) and mk_json:
        df = pd.DataFrame(mk_json)
        is_valid = not df.empty

    # --- Handle missing/invalid data with deterministic mock ---
    if not is_valid or df.empty:
        logger.warning(Fore.YELLOW + f"‚ö†Ô∏è Using mock data for {year}")
        st.warning(f"‚ö†Ô∏è No valid API data for {year}, generating mock values.")
        random.seed(year)
        makers = [
            "Maruti Suzuki", "Tata Motors", "Hyundai", "Mahindra", "Hero MotoCorp",
            "Bajaj Auto", "TVS Motor", "Honda", "Kia", "Toyota", "Renault",
            "Ashok Leyland", "MG Motor", "Eicher", "Piaggio", "BYD", "Olectra", "Force Motors"
        ]
        random.shuffle(makers)
        top = makers[:10]
        base = random.randint(200_000, 1_000_000)
        growth = 1 + (year - 2020) * 0.06
        df = pd.DataFrame({
            "label": top,
            "value": [int(base * random.uniform(0.5, 1.5) * growth) for _ in top]
        })
        print(f"[DEBUG] Mock data generated: {df.to_dict(orient='records')}")
    else:
        st.success(f"‚úÖ Valid API data loaded for {year}")
        print(f"[DEBUG] API data loaded successfully: {df.head().to_dict(orient='records')}")

    # --- Normalize columns ---
    df.columns = [c.lower() for c in df.columns]
    df["year"] = year
    df = df.sort_values("value", ascending=False)

    # --- Visual output ---
    if not df.empty:
        st.info(f"üèÜ **{year}** ‚Üí **{df.iloc[0]['label']}** ‚Äî {df.iloc[0]['value']:,} registrations")
        bar_from_makers(df, f"Top Makers ({year})", combined=False)
        pie_from_makers(df, f"Maker Share ({year})")

    # --- Print ALL-MAXED debug summary ---
    print("="*80)
    print(f"[ALL-MAXED] Year: {year}, Top Maker: {df.iloc[0]['label'] if not df.empty else 'N/A'}")
    print(f"Total makers: {len(df)}, Columns: {df.columns.tolist()}")
    print("="*80)

    return df

# -----------------------------------------------------
# üîÅ MAIN LOOP ‚Äî MULTI-YEAR FETCH
# -----------------------------------------------------
all_years = []

st.info(f"üîÑ Starting fetch for {len(years)} years: {years}")

with st.spinner("‚è≥ Fetching maker data for all selected years..."):
    for y in years:
        try:
            st.write(f"‚è≥ Fetching data for {y}...")
            print(f"[ALL-MAXED] Fetching maker top 5 for year {y}...")
            
            # ‚úÖ fetch with debug
            dfy = fetch_maker_top5(y, params_common, show_debug=True)  
            
            if dfy is not None and not dfy.empty:
                all_years.append(dfy)
                print(f"[ALL-MAXED] Year {y} fetched: {len(dfy)} rows, top maker: {dfy.iloc[0]['label']}")
            else:
                print(f"[ALL-MAXED] Year {y} returned empty DataFrame")
            
        except Exception as e:
            st.error(f"‚ùå {y} fetch error: {e}")
            logger.error(Fore.RED + f"Fetch error {y}: {e}")
            print(f"[ALL-MAXED] Exception during fetch for {y}: {e}")

# Combine all years into a single DataFrame if needed
if all_years:
    df_all_years = pd.concat(all_years, ignore_index=True)
    print(f"[ALL-MAXED] Combined all years: {len(df_all_years)} rows")
    st.success(f"‚úÖ Fetched and combined data for {len(all_years)} years")
else:
    df_all_years = pd.DataFrame()
    st.warning("‚ö†Ô∏è No data fetched for the selected years")

# =====================================================
# -------------------------
# Main Streamlit UI ‚Äî All-Maxed Maker Block
# -------------------------
# =====================================================

from typing import Optional
import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime

def all_maxed_maker_block(params_common: dict = None, freq="Monthly", section_id="maker_section"):
    """
    Render the MAXED multi-year Maker analytics block inside Streamlit.
    Handles multi-year fetching, deterministic fallback, and synthetic time series.
    """

    import streamlit as st
    import pandas as pd
    from datetime import datetime
    
    # Ensure params_common is a dict
    params_common = params_common or {}
    
    # Determine year range
    years = list(range(int(from_year), int(to_year)+1))
    print(f"[ALL-MAXED] Years in scope: {years}")
    st.info(f"üîÑ Fetching maker data for years: {years}")
    
    # -------------------------
    # Fetch multi-year maker data
    # -------------------------
    all_year_dfs = []
    with st.spinner("‚è≥ Fetching maker data for selected years..."):
        for y in years:
            try:
                print(f"[ALL-MAXED] Fetching data for year {y}...")
                df_y = fetch_maker_top5(y, params_common, show_debug=True)
                
                if df_y is None or df_y.empty:
                    raise ValueError(f"No maker data for {y}")
                
                print(f"[ALL-MAXED] Fetched {len(df_y)} rows for year {y}, top maker: {df_y.iloc[0]['label']}")
            
            except Exception as e:
                st.warning(f"‚ö†Ô∏è {e}. Using deterministic mock for {y}.")
                print(f"[ALL-MAXED] Exception for {y}: {e}. Generating mock data...")
                logger.exception(f"Error fetching maker data for {y}: {e}")
                
                mock_data = maker_mock_top5(y).get("data", [])
                df_y = pd.DataFrame(mock_data)
                df_y["year"] = y
                if "label" not in df_y.columns:
                    df_y["label"] = df_y.get("name", [f"Maker {i+1}" for i in range(len(df_y))])
                if "value" not in df_y.columns:
                    df_y["value"] = pd.to_numeric(df_y.get("score", 0), errors="coerce").fillna(0)
                
                print(f"[ALL-MAXED] Mock data created for {y}: {len(df_y)} rows")
            
            all_year_dfs.append(df_y)
    
    # Concatenate and sort
    df_maker_all = pd.concat(all_year_dfs, ignore_index=True)
    df_maker_all = df_maker_all.sort_values(["year", "value"], ascending=[True, False]).reset_index(drop=True)
    
    print(f"[ALL-MAXED] Combined all years: {len(df_maker_all)} rows")
  
    # -------------------------
    # Frequency expansion -> synthetic time series
    # -------------------------
    ts_list = []
    unique_years = sorted(df_maker_all["year"].unique())
    print(f"[ALL-MAXED] Expanding timeseries for years: {unique_years}")
    
    for y in unique_years:
        df_y = df_maker_all[df_maker_all["year"] == y].reset_index(drop=True)
        print(f"[ALL-MAXED] Processing year {y}: {len(df_y)} makers")
        
        ts = year_to_timeseries(
            df_y.rename(columns={"label":"label", "value":"value"}),  # ensure expected columns
            int(y),
            freq=freq
        )
        
        print(f"[ALL-MAXED] Year {y} timeseries generated: {len(ts)} rows")
        ts_list.append(ts)
    
    # Concatenate all years into a single DataFrame
    if ts_list:
        df_ts = pd.concat(ts_list, ignore_index=True)
    else:
        df_ts = pd.DataFrame(columns=["ds","label","value","year"])
    
    df_ts["ds"] = pd.to_datetime(df_ts["ds"])
    print(f"[ALL-MAXED] Combined synthetic timeseries: {len(df_ts)} rows")
  
    # -------------------------
    # Resample to requested frequency
    # -------------------------
    freq_map = {"Daily":"D", "Monthly":"M", "Quarterly":"Q", "Yearly":"Y"}
    resample_code = freq_map.get(freq, "M")
    print(f"[ALL-MAXED] Resampling timeseries to {freq} ({resample_code}) frequency")
    
    resampled = (
        df_ts
        .groupby(["label", pd.Grouper(key="ds", freq=resample_code)])["value"]
        .sum()
        .reset_index()
    )
    resampled["year"] = resampled["ds"].dt.year
    
    print(f"[ALL-MAXED] Resampled DataFrame: {len(resampled)} rows, {resampled['label'].nunique()} unique labels")
  
    # -------------------------
    # Pivot tables for heatmap / radar / combined view
    # -------------------------
    pivot = resampled.pivot_table(
        index="ds",
        columns="label",
        values="value",
        aggfunc="sum"
    ).fillna(0)
    
    pivot_year = resampled.pivot_table(
        index="year",
        columns="label",
        values="value",
        aggfunc="sum"
    ).fillna(0)
    
    print(f"[ALL-MAXED] Pivot table by date: {pivot.shape[0]} rows √ó {pivot.shape[1]} columns")
    print(f"[ALL-MAXED] Pivot table by year: {pivot_year.shape[0]} rows √ó {pivot_year.shape[1]} columns")
  
    
    # -------------------------
    # Frequency expansion -> synthetic timeseries
    # -------------------------
    ts_list = []
    for y in sorted(df_maker_all["year"].unique()):
        df_y = df_maker_all[df_maker_all["year"] == y].reset_index(drop=True)
        print(f"[ALL-MAXED] Expanding year {y} with {len(df_y)} makers")
        
        ts = year_to_timeseries(
            df_y.rename(columns={"label": "label", "value": "value"}),
            int(y),
            freq=freq
        )
        print(f"[ALL-MAXED] Year {y} timeseries expanded: {len(ts)} rows")
        ts_list.append(ts)
    
    df_ts = pd.concat(ts_list, ignore_index=True) if ts_list else pd.DataFrame(
        columns=["ds", "label", "value", "year", "month", "quarter", "month_name", "maker_key"]
    )
    df_ts["ds"] = pd.to_datetime(df_ts["ds"])
    
    print(f"[ALL-MAXED] Combined timeseries length: {len(df_ts)} rows")
    
    # -------------------------
    # Resample to requested frequency
    # -------------------------
    freq_map = {"Daily": "D", "Monthly": "M", "Quarterly": "Q", "Yearly": "Y"}
    resample_code = freq_map.get(freq, "M")
    print(f"[ALL-MAXED] Resampling timeseries to freq={freq} ({resample_code})")
    
    resampled = (
        df_ts.groupby(["label", pd.Grouper(key="ds", freq=resample_code)])["value"]
        .sum()
        .reset_index()
    )
    resampled["year"] = resampled["ds"].dt.year
    
    print(f"[ALL-MAXED] Resampled timeseries shape: {resampled.shape}")
    
    # -------------------------
    # Pivot tables for heatmap / radar / combined view
    # -------------------------
    print(f"[ALL-MAXED] Creating pivot tables for heatmap/radar/combined views")
    
    pivot = resampled.pivot_table(
        index="ds", columns="label", values="value", aggfunc="sum"
    ).fillna(0)
    pivot_year = resampled.pivot_table(
        index="year", columns="label", values="value", aggfunc="sum"
    ).fillna(0)
    
    print(f"[ALL-MAXED] Pivot table (time series) shape: {pivot.shape}")
    print(f"[ALL-MAXED] Pivot table (yearly) shape: {pivot_year.shape}")
    
  
    # -----------------------------
    # ALL-MAXED HEATMAP ‚Äî Year √ó Maker
    # -----------------------------
    if show_heatmap and not pivot_year.empty:
        st.subheader("üìä ALL-MAXED Year √ó Maker Heatmap")
        print(f"[ALL-MAXED] Rendering heatmap for {pivot_year.shape[0]} years √ó {pivot_year.shape[1]} makers")
    
        import plotly.express as px
    
        fig = px.imshow(
            pivot_year.T,
            labels=dict(x="Year", y="Maker", color="Registrations"),
            text_auto=True,
            aspect="auto",
            color_continuous_scale="Viridis",
            origin='lower'
        )
    
        # -----------------------------
        # ALL-MAXED Hover & Layout
        # -----------------------------
        fig.update_traces(
            hovertemplate="<b>Maker:</b> %{y}<br><b>Year:</b> %{x}<br><b>Registrations:</b> %{z:,.0f}",
            textfont=dict(size=10, color='black')
        )
        fig.update_layout(
            template=DEFAULT_TEMPLATE,
            title="üå°Ô∏è Year √ó Maker Registration Heatmap (All-Maxed)",
            title_font=TITLE_STYLE,
            xaxis=dict(tickangle=-45, side='bottom', tickfont=dict(size=10)),
            yaxis=dict(tickfont=dict(size=10)),
            coloraxis_colorbar=dict(title="Registrations"),
            height=550,
            margin=dict(t=70, b=60, l=100, r=30)
        )
    
        st.plotly_chart(fig, use_container_width=True)
        st.info(f"‚úÖ Heatmap displayed ‚Äî shape: {pivot_year.shape}")
    
    else:
        st.warning("‚ö†Ô∏è No heatmap data available for the selected years/makers.")

    # -----------------------------
    # ALL-MAXED RADAR / POLAR ‚Äî Makers per Year
    # -----------------------------
    if show_radar and not pivot_year.empty:
        st.subheader("üìà ALL-MAXED Radar / Polar ‚Äî Makers per Year")
        print(f"[ALL-MAXED] Rendering radar for {pivot_year.shape[0]} years √ó {pivot_year.shape[1]} makers")
    
        import plotly.graph_objects as go
        fig = go.Figure()
    
        # Color palette for all years
        import plotly.colors as pc
        palette = pc.qualitative.Dark24
        n_colors = len(pivot_year.index)
        colors = [palette[i % len(palette)] for i in range(n_colors)]
    
        for i, y in enumerate(pivot_year.index):
            fig.add_trace(
                go.Scatterpolar(
                    r=pivot_year.loc[y].values,
                    theta=pivot_year.columns.tolist(),
                    fill='toself',
                    name=str(y),
                    line=dict(color=colors[i], width=2),
                    marker=dict(size=6),
                    hovertemplate="<b>Year:</b> %{name}<br>%{theta}: %{r:,.0f} registrations"
                )
            )
    
        # -----------------------------
        # ALL-MAXED Layout
        # -----------------------------
        fig.update_layout(
            polar=dict(
                radialaxis=dict(visible=True, showline=True, linewidth=1, gridcolor='lightgray'),
                angularaxis=dict(tickfont=dict(size=10))
            ),
            showlegend=True,
            legend=dict(
                orientation="h",
                yanchor="bottom",
                y=-0.2,
                xanchor="center",
                x=0.5,
                title=None
            ),
            title=f"üåê Makers Radar ‚Äî {pivot_year.shape[0]} Years √ó {pivot_year.shape[1]} Makers",
            title_font=dict(size=18, color="darkblue", family="Arial"),
            height=500,
            margin=dict(t=80, b=60, l=60, r=60)
        )
    
        st.plotly_chart(fig, use_container_width=True)
        st.info(f"‚úÖ Radar chart displayed ‚Äî shape: {pivot_year.shape}")
    
    else:
        st.warning("‚ö†Ô∏è No radar data available for the selected years/makers.")

    # =====================================================
    # üìä VISUALIZATIONS ‚Äî ALL-MAXED MAKER
    # =====================================================
    st.subheader("üìä Maker Visualizations ‚Äî Multi-year & Multi-frequency (All-Maxed)")
    
    # --- Safety Checks ---
    print(f"[ALL-MAXED] Checking resampled and pivot data")
    if "resampled" not in locals() or resampled is None or resampled.empty:
        print("[ALL-MAXED] ‚ö†Ô∏è No valid 'resampled' maker data found")
        st.warning("‚ö†Ô∏è No valid 'resampled' maker data available for visualization.")
        st.stop()
    
    if "pivot" not in locals() or pivot is None or pivot.empty:
        print("[ALL-MAXED] ‚ö†Ô∏è No valid 'pivot' maker data found")
        st.warning("‚ö†Ô∏è No valid 'pivot' maker data available for visualization.")
        st.stop()
    
    if "mode" not in locals():
        mode = "Combined (Overlay / Stacked)"
        print(f"[ALL-MAXED] Mode not defined, defaulting to '{mode}'")
    
    st.info(f"[ALL-MAXED] Ready to render visualizations ‚Äî mode: {mode}")
    
    # -------------------------
    # Combined / Small Multiples ‚Äî Maker
    # -------------------------
    import plotly.express as px
    
    if mode.startswith("Combined") and not resampled.empty:
        st.markdown("### üåà ALL-MAXED ‚Äî Stacked & Overlay Maker Trends")
        print(f"[ALL-MAXED] Rendering Combined Maker charts, mode={mode}")
    
        # --- Stacked Area Chart ---
        try:
            fig_area = px.area(
                resampled,
                x="ds",
                y="value",
                color="label",
                title="üìä Stacked Registrations by Maker Over Time",
                color_discrete_sequence=px.colors.qualitative.Set3,
            )
            fig_area.update_traces(
                hovertemplate="<b>%{x|%b %Y}</b><br>%{y:,.0f} registrations<br>%{fullData.name}",
                line=dict(width=0.8),
            )
            fig_area.update_layout(
                template="plotly_white",
                xaxis_title="Date",
                yaxis_title="Registrations",
                legend_title_text="Maker",
                hovermode="x unified",
                height=500,
                margin=dict(t=80, b=50, l=60, r=40)
            )
            st.plotly_chart(fig_area, use_container_width=True)
            print(f"[ALL-MAXED] ‚úÖ Stacked area chart rendered ({len(resampled)} rows)")
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Stacked area failed: {e}")
            print(f"[ALL-MAXED] ‚ö†Ô∏è Stacked area exception: {e}")
    
        # --- Overlay Line Chart ---
        try:
            fig_line = px.line(
                resampled,
                x="ds",
                y="value",
                color="label",
                markers=True,
                title="üìà Maker Trends (Overlay)",
                color_discrete_sequence=px.colors.qualitative.Bold,
            )
            fig_line.update_traces(
                line=dict(width=2),
                marker=dict(size=6),
                hovertemplate="<b>%{x|%b %Y}</b><br>%{y:,.0f} registrations<br>%{fullData.name}"
            )
            fig_line.update_layout(
                template="plotly_white",
                xaxis_title="Date",
                yaxis_title="Registrations",
                hovermode="x unified",
                legend_title_text="Maker",
                height=500,
                margin=dict(t=80, b=50, l=60, r=40)
            )
            st.plotly_chart(fig_line, use_container_width=True)
            print(f"[ALL-MAXED] ‚úÖ Overlay line chart rendered ({len(resampled)} rows)")
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Overlay lines failed: {e}")
            print(f"[ALL-MAXED] ‚ö†Ô∏è Overlay line exception: {e}")
    
    # -------------------------
    # Separate Mode (Small Multiples)
    # -------------------------
    else:
        st.markdown("### üß© ALL-MAXED ‚Äî Small Multiples (Yearly Maker Distribution)")
        print(f"[ALL-MAXED] Rendering Small Multiples for separate mode")
    
        try:
            years_sorted = sorted(resampled["year"].unique())
            print(f"[ALL-MAXED] Available years: {years_sorted}")
        except Exception as e:
            years_sorted = []
            print(f"[ALL-MAXED] ‚ö†Ô∏è Failed to sort years: {e}")
    
        sel_small = st.multiselect(
            "Select specific years for small multiples (limit 6)",
            years_sorted,
            default=years_sorted[-min(3, len(years_sorted)):] if years_sorted else [],
        )
        print(f"[ALL-MAXED] User selected years for small multiples: {sel_small}")
    
        if not sel_small:
            st.info("Select at least one year to show small multiples.")
            print(f"[ALL-MAXED] No years selected, skipping small multiples")
        else:
            for y in sel_small[:6]:
                d = resampled[resampled["year"] == y]
                if d.empty:
                    st.caption(f"‚ö†Ô∏è No data for {y}")
                    print(f"[ALL-MAXED] ‚ö†Ô∏è No data available for year {y}")
                    continue
    
                try:
                    fig_bar = px.bar(
                        d,
                        x="label",
                        y="value",
                        color="label",
                        text_auto=".2s",
                        title=f"üìä Maker Distribution ‚Äî {y}",
                        color_discrete_sequence=px.colors.qualitative.Pastel1,
                    )
                    fig_bar.update_traces(
                        textfont_size=11,
                        textangle=0,
                        hovertemplate="<b>%{x}</b><br>%{y:,.0f} registrations<br>%{fullData.name}"
                    )
                    fig_bar.update_layout(
                        showlegend=False,
                        template="plotly_white",
                        xaxis_title="Maker",
                        yaxis_title="Registrations",
                        margin=dict(t=60, b=40, l=50, r=20),
                        height=400
                    )
                    st.plotly_chart(fig_bar, use_container_width=True)
                    print(f"[ALL-MAXED] ‚úÖ Rendered bar chart for year {y} ({len(d)} rows)")
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Failed to plot {y}: {e}")
                    print(f"[ALL-MAXED] ‚ö†Ô∏è Exception plotting year {y}: {e}")

    # -------------------------
    # Optional Advanced Visuals ‚Äî Maker
    # -------------------------
    if show_heatmap:
        st.markdown("### üî• ALL-MAXED ‚Äî Maker Heatmap (Year √ó Maker)")
        print("[ALL-MAXED] Rendering heatmap")
        try:
            pivot_heat = pivot_year.copy()
            print(f"[ALL-MAXED] Pivot shape for heatmap: {pivot_heat.shape}")
            
            fig_heat = px.imshow(
                pivot_heat.T,
                labels=dict(x="Year", y="Maker", color="Registrations"),
                aspect="auto",
                color_continuous_scale="YlOrRd",
                title="Heatmap of Registrations per Maker per Year",
                text_auto=True
            )
            fig_heat.update_layout(
                template="plotly_white",
                title_font=dict(size=18, family="Arial Black"),
                xaxis=dict(tickangle=-45),
                height=450,
                margin=dict(t=60, b=60, l=60, r=40)
            )
            st.plotly_chart(fig_heat, use_container_width=True)
            st.info(f"‚úÖ Heatmap displayed ‚Äî shape: {pivot_heat.shape}")
            print("[ALL-MAXED] Heatmap rendered successfully")
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Heatmap failed: {e}")
            print(f"[ALL-MAXED] ‚ö†Ô∏è Heatmap exception: {e}")
    
    if show_radar:
        st.markdown("### üï∏Ô∏è ALL-MAXED ‚Äî Radar Chart: Maker Profiles per Year")
        print("[ALL-MAXED] Rendering radar chart")
        try:
            import plotly.graph_objects as go
            makers = list(pivot_year.columns)
            print(f"[ALL-MAXED] Makers for radar: {makers}")
            
            fig_radar = go.Figure()
            for y in sorted(pivot_year.index)[-min(4, len(pivot_year.index)):]:
                vals = pivot_year.loc[y].values
                fig_radar.add_trace(go.Scatterpolar(
                    r=vals,
                    theta=makers,
                    fill='toself',
                    name=str(y),
                    hovertemplate="<b>%{theta}</b>: %{r:,.0f} registrations<br>Year: %{fullData.name}"
                ))
                print(f"[ALL-MAXED] Added radar trace for year {y}")
            
            fig_radar.update_layout(
                polar=dict(radialaxis=dict(visible=True, showline=True, linewidth=1)),
                showlegend=True,
                template="plotly_white",
                title="Radar Comparison of Maker Patterns",
                height=500,
                legend=dict(orientation="h", yanchor="bottom", y=-0.1, xanchor="center", x=0.5)
            )
            st.plotly_chart(fig_radar, use_container_width=True)
            st.info(f"‚úÖ Radar chart displayed ‚Äî {len(pivot_year.columns)} makers")
            print("[ALL-MAXED] Radar chart rendered successfully")
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Radar chart failed: {e}")
            print(f"[ALL-MAXED] ‚ö†Ô∏è Radar exception: {e}")
    
    # -------------------------
    # üç© Donut & Sunburst (All-Maxed) ‚Äî Maker
    # -------------------------
    st.markdown("### üç© Donut & Sunburst ‚Äî Latest Available Period (All-Maxed)")
    print("[ALL-MAXED] Starting Donut & Sunburst rendering")
    
    if resampled.empty:
        st.warning("‚ö†Ô∏è No resampled data available for donut/sunburst charts.")
        print("[ALL-MAXED] Resampled dataframe is empty")
    else:
        latest_period = (
            resampled.loc[resampled["value"] > 0, "ds"].max()
            if not resampled.empty else None
        )
        print(f"[ALL-MAXED] Latest period detected: {latest_period}")
    
        if latest_period is None or pd.isna(latest_period):
            st.info("No valid non-zero data found for latest period visualization.")
            print("[ALL-MAXED] No valid latest period with non-zero values")
        else:
            d_latest = (
                resampled[resampled["ds"] == latest_period]
                .groupby("label", as_index=False)["value"]
                .sum()
                .sort_values("value", ascending=False)
            )
            print(f"[ALL-MAXED] d_latest shape: {d_latest.shape}")
    
            total_latest = d_latest["value"].sum()
            d_latest["Share_%"] = (d_latest["value"] / total_latest * 100).round(2)
            print(f"[ALL-MAXED] Total registrations for latest period: {total_latest}")
    
            if not d_latest.empty and total_latest > 0:
                # --- Donut Chart ---
                try:
                    fig_donut = px.pie(
                        d_latest,
                        names="label",
                        values="value",
                        hole=0.55,
                        color_discrete_sequence=px.colors.qualitative.Vivid,
                        title=f"Maker Split ‚Äî {latest_period.strftime('%Y-%m')} (Total: {int(total_latest):,})",
                        hover_data={"Share_%": True, "value": True},
                    )
                    fig_donut.update_traces(
                        textposition="inside",
                        textinfo="percent+label",
                        pull=[0.05] * len(d_latest),
                    )
                    fig_donut.update_layout(
                        template="plotly_white",
                        showlegend=True,
                        legend_title_text="Maker",
                        title_font=dict(size=18, family="Arial Black")
                    )
                    st.plotly_chart(fig_donut, use_container_width=True)
                    print("[ALL-MAXED] Donut chart rendered successfully")
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Donut chart failed: {e}")
                    print(f"[ALL-MAXED] Donut chart exception: {e}")
    
                # --- Sunburst Chart ---
                try:
                    sb = (
                        df_maker_all.groupby(["year", "label"], as_index=False)["value"]
                        .sum()
                        .sort_values(["year", "value"], ascending=[True, False])
                    )
                    fig_sb = px.sunburst(
                        sb,
                        path=["year", "label"],
                        values="value",
                        color="value",
                        color_continuous_scale="Sunset",
                        title="üåû Sunburst ‚Äî Year ‚Üí Maker ‚Üí Value",
                        hover_data={"value": True},
                    )
                    fig_sb.update_layout(
                        template="plotly_white",
                        title_font=dict(size=18, family="Arial Black")
                    )
                    st.plotly_chart(fig_sb, use_container_width=True)
                    print("[ALL-MAXED] Sunburst chart rendered successfully")
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Sunburst chart failed: {e}")
                    print(f"[ALL-MAXED] Sunburst exception: {e}")
    
                # --- Data summary below ---
                with st.expander("üìã Latest Period Maker Data Summary"):
                    st.dataframe(
                        d_latest.style.format({"value": "{:,}", "Share_%": "{:.2f}"}),
                        use_container_width=True,
                    )
                    print("[ALL-MAXED] Latest period data summary displayed")
            else:
                st.info("‚ö†Ô∏è Latest period has zero or empty Maker values")
                print("[ALL-MAXED] Latest period has zero/empty values")

    # --- Heatmap Section ---
    if show_heatmap:
        st.markdown("### üî• Heatmap ‚Äî Year √ó Maker (All-Maxed)")
        print("[ALL-MAXED] Starting heatmap rendering")
    
        if pivot_year.empty:
            st.info("‚ö†Ô∏è No Maker data available for heatmap.")
            print("[ALL-MAXED] pivot_year is empty")
        else:
            try:
                heat = pivot_year.copy()
                print(f"[ALL-MAXED] pivot_year shape: {heat.shape}")
    
                # --- Normalization Option ---
                heat_norm = heat.div(heat.max(axis=1), axis=0).fillna(0)
                normalize_opt = st.checkbox(
                    "Normalize heatmap (relative per year)",
                    value=True,
                    key=f"normalize_heatmap_pivot",
                )
                heat_used = heat_norm if normalize_opt else heat
    
                # --- Plotly Heatmap ---
                import plotly.graph_objects as go
                fig_h = go.Figure(
                    data=go.Heatmap(
                        z=heat_used.values,
                        x=heat_used.columns.astype(str),
                        y=heat_used.index.astype(str),
                        colorscale="Viridis",
                        hoverongaps=False,
                        texttemplate="%{z:.1f}" if normalize_opt else None,
                    )
                )
                fig_h.update_layout(
                    title=(
                        "Normalized Registrations by Maker per Year"
                        if normalize_opt
                        else "Absolute Registrations by Maker per Year"
                    ),
                    xaxis_title="Maker",
                    yaxis_title="Year",
                    template="plotly_white",
                    coloraxis_colorbar=dict(
                        title="Registrations" if not normalize_opt else "Share (0‚Äì1)"
                    ),
                    height=500,
                )
                st.plotly_chart(fig_h, use_container_width=True)
                print("[ALL-MAXED] Heatmap rendered successfully")
    
                # --- Data Table Below ---
                with st.expander("üìã View Heatmap Data Table"):
                    st.dataframe(
                        heat_used.round(2)
                        .style.format("{:,.0f}" if not normalize_opt else "{:.2f}")
                        .background_gradient(cmap="viridis"),
                        use_container_width=True,
                    )
                    print("[ALL-MAXED] Heatmap data table displayed")
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Heatmap rendering failed: {e}")
                print(f"[ALL-MAXED] Heatmap exception: {e}")

    
    # -------------------------
    # üåà RADAR ‚Äî Snapshot per Year (All-Maxed)
    # -------------------------
    if show_radar:
        st.markdown("### üåà Radar ‚Äî Maker Profile Snapshot (All-Maxed)")
        print("[ALL-MAXED] Starting radar rendering")
    
        if pivot_year.empty:
            st.info("‚ö†Ô∏è Not enough Maker data for radar visualization.")
            print("[ALL-MAXED] pivot_year is empty; radar cannot be plotted")
        else:
            try:
                # --- Select last 4 years for radar ---
                yrs_for_radar = sorted(pivot_year.index)[-min(4, len(pivot_year.index)):]
                print(f"[ALL-MAXED] Years selected for radar: {yrs_for_radar}")
    
                makers = pivot_year.columns.tolist()
                radar_df = pivot_year.copy()
    
                # --- Normalization per Maker ---
                radar_df_norm = radar_df.div(radar_df.max(axis=0), axis=1).fillna(0)
                normalize_radar = st.checkbox("Normalize radar per Maker (0‚Äì1)", value=True)
                df_radar_used = radar_df_norm if normalize_radar else radar_df
                print(f"[ALL-MAXED] Normalization applied: {normalize_radar}")
    
                # --- Plotly Radar ---
                import plotly.graph_objects as go
                fig_r = go.Figure()
                for y in yrs_for_radar:
                    vals = df_radar_used.loc[y].values.tolist()
                    print(f"[ALL-MAXED] Radar values for year {y}: {vals}")
                    fig_r.add_trace(
                        go.Scatterpolar(
                            r=vals,
                            theta=makers,
                            fill="toself",
                            name=str(y),
                            hovertemplate="<b>%{theta}</b><br>Value: %{r:.2f}<extra></extra>",
                        )
                    )
    
                fig_r.update_layout(
                    polar=dict(
                        radialaxis=dict(
                            visible=True,
                            range=[0, 1] if normalize_radar else [0, df_radar_used.values.max()],
                            showline=True,
                            linewidth=1,
                            gridcolor="lightgray",
                        )
                    ),
                    showlegend=True,
                    title="Maker Distribution Radar (Last Years)",
                    template="plotly_white",
                    height=600,
                )
                st.plotly_chart(fig_r, use_container_width=True)
                print("[ALL-MAXED] Radar chart rendered successfully")
    
                # --- Data Table ---
                with st.expander("üìã Radar Data Used"):
                    st.dataframe(
                        df_radar_used.round(2)
                        .style.format("{:,.0f}" if not normalize_radar else "{:.2f}")
                        .background_gradient(cmap="cool"),
                        use_container_width=True,
                    )
                    print("[ALL-MAXED] Radar data table displayed")
    
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Radar chart rendering failed: {e}")
                print(f"[ALL-MAXED] Radar chart exception: {e}")

    # -------------------------
    # üîÆ FORECASTING ‚Äî All-Maxed (Linear + Prophet + Auto Insights) ‚Äî Maker
    # -------------------------
    if do_forecast:
        st.markdown("## üîÆ Forecasting ‚Äî Maker (All-Maxed)")
        print("[ALL-MAXED] Starting forecasting section")
    
        # ---------------------
        # 1Ô∏è‚É£ Select maker & horizon
        # ---------------------
        makers = pivot_year.columns.tolist() if not pivot_year.empty else df_maker_all["label"].unique().tolist()
        print(f"[ALL-MAXED] Available makers for forecast: {makers}")
    
        if not makers:
            st.info("‚ö†Ô∏è No makers available for forecasting.")
            print("[ALL-MAXED] No makers found, aborting forecast")
        else:
            maker_to_forecast = st.selectbox(
                "üìä Choose maker to forecast",
                makers,
                key=f"maker_select_{section_id}"
            )
            horizon_years = st.slider(
                "Forecast horizon (years)",
                1, 10, 3,
                key=f"horizon_slider_{section_id}"
            )
            st.caption("Select a maker and choose how many future years to forecast.")
            print(f"[ALL-MAXED] Selected maker: {maker_to_forecast}, horizon: {horizon_years} years")
    
            # ---------------------
            # 2Ô∏è‚É£ Prepare time series
            # ---------------------
            if maker_to_forecast in pivot_year.columns:
                series = pivot_year[[maker_to_forecast]].reset_index().rename(
                    columns={maker_to_forecast: "y", "index": "year"}
                )
            else:
                series = pd.DataFrame(columns=["year", "y"])
            print(f"[ALL-MAXED] Prepared time series: {series.shape[0]} rows")
    
            if series.empty or series["y"].isna().all():
                st.info("‚ö†Ô∏è Insufficient data for forecasting this maker.")
                print("[ALL-MAXED] Series empty or all NaN, skipping forecast")
            else:
                series["ds"] = pd.to_datetime(series["year"].astype(str) + "-01-01")
                series = series[["ds", "y"]].dropna()
                print(f"[ALL-MAXED] Series ready for modeling: {series.shape[0]} rows")
    
                # ---------------------
                # 3Ô∏è‚É£ Linear Regression Forecast
                # ---------------------
                st.markdown("### üìà Linear Regression Forecast")
                try:
                    import numpy as np
                    from sklearn.linear_model import LinearRegression
    
                    X = np.arange(len(series)).reshape(-1, 1)
                    y = series["y"].values
                    model = LinearRegression().fit(X, y)
                    fut_idx = np.arange(len(series) + horizon_years).reshape(-1, 1)
                    preds = model.predict(fut_idx)
                    print(f"[ALL-MAXED] Linear regression predictions computed")
    
                    fut_dates = pd.date_range(
                        start=series["ds"].iloc[0],
                        periods=len(series) + horizon_years,
                        freq="YS",
                    )
                    df_fore = pd.DataFrame({"ds": fut_dates, "Linear": preds})
                    df_fore["Type"] = ["Historical"] * len(series) + ["Forecast"] * horizon_years
    
                    fig_l = px.line(
                        df_fore, x="ds", y="Linear", color="Type",
                        title=f"Linear Trend Forecast ‚Äî {maker_to_forecast}"
                    )
                    fig_l.add_scatter(
                        x=series["ds"], y=series["y"], mode="markers+lines",
                        name="Observed", line=dict(color="blue")
                    )
                    fig_l.update_layout(template="plotly_white", height=500)
                    st.plotly_chart(fig_l, use_container_width=True)
                    print("[ALL-MAXED] Linear regression chart rendered")
    
                    # KPI summary
                    last_val = series["y"].iloc[-1]
                    next_val = preds[len(series)]
                    growth = ((next_val - last_val) / last_val) * 100 if last_val else np.nan
                    st.metric(
                        "Next Year Projection",
                        f"{next_val:,.0f}",
                        f"{growth:+.1f}% vs last year"
                    )
                    print(f"[ALL-MAXED] Next year projection: {next_val}, growth: {growth:.1f}%")
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Linear regression forecast failed: {e}")
                    print(f"[ALL-MAXED] Linear regression exception: {e}")
    
                # ---------------------
                # 4Ô∏è‚É£ Prophet Forecast (if available)
                # ---------------------
                st.markdown("### üßô Prophet Forecast (Advanced, if available)")
                try:
                    from prophet import Prophet  # Import inside try
                    m = Prophet(yearly_seasonality=True, seasonality_mode="multiplicative", changepoint_prior_scale=0.05)
                    m.fit(series)
                    future = m.make_future_dataframe(periods=horizon_years, freq="Y")
                    forecast = m.predict(future)
                    print("[ALL-MAXED] Prophet forecast generated")
    
                    # Plot
                    import plotly.graph_objects as go
                    figp = go.Figure()
                    figp.add_trace(go.Scatter(x=series["ds"], y=series["y"], mode="markers+lines", name="Observed", line=dict(color="blue")))
                    figp.add_trace(go.Scatter(x=forecast["ds"], y=forecast["yhat"], mode="lines", name="Forecast (yhat)", line=dict(color="orange", width=3)))
                    figp.add_trace(go.Scatter(x=forecast["ds"], y=forecast["yhat_upper"], mode="lines", name="Upper Bound", line=dict(color="lightgray", dash="dot")))
                    figp.add_trace(go.Scatter(x=forecast["ds"], y=forecast["yhat_lower"], mode="lines", name="Lower Bound", line=dict(color="lightgray", dash="dot")))
                    figp.update_layout(title=f"Prophet Forecast ‚Äî {maker_to_forecast}", template="plotly_white", height=550, legend=dict(orientation="h", y=-0.2), xaxis_title="Year", yaxis_title="Registrations")
                    st.plotly_chart(figp, use_container_width=True)
                    print("[ALL-MAXED] Prophet chart rendered")
    
                    # Optional insight
                    fut_y = forecast.tail(horizon_years)["yhat"].mean()
                    st.success(f"üìä Prophet projects an **average of {fut_y:,.0f}** registrations/year for the next {horizon_years} years.")
                    print(f"[ALL-MAXED] Prophet average forecast: {fut_y:,.0f}")
                except ImportError:
                    st.info("üß† Prophet not installed ‚Äî only linear forecast shown.")
                    print("[ALL-MAXED] Prophet not installed")
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Prophet forecast failed: {e}")
                    print(f"[ALL-MAXED] Prophet exception: {e}")
    
                # ---------------------
                # 5Ô∏è‚É£ Display Forecast Data
                # ---------------------
                with st.expander("üìã View Forecast Data Table"):
                    try:
                        comb = df_fore.copy()
                        if "forecast" in locals():
                            comb = comb.merge(forecast[["ds", "yhat", "yhat_lower", "yhat_upper"]], on="ds", how="outer")
                        st.dataframe(comb.round(2).style.background_gradient(cmap="PuBuGn"), use_container_width=True)
                        print("[ALL-MAXED] Forecast data table displayed")
                    except Exception:
                        st.dataframe(df_fore, use_container_width=True)
                        print("[ALL-MAXED] Forecast fallback data table displayed")

    # -------------------------
    # ‚ö†Ô∏è ANOMALY DETECTION ‚Äî All-Maxed (Maker)
    # -------------------------
    if do_anomaly:
        st.markdown("## ‚ö†Ô∏è Anomaly Detection ‚Äî Maker (All-Maxed)")
        st.caption("Detects outliers and abnormal spikes/drops per maker time series using IsolationForest + backup z-score method.")
        print("[ALL-MAXED] Starting anomaly detection section")
    
        try:
            from sklearn.ensemble import IsolationForest
            import numpy as np
    
            anomalies = []
            anomaly_records = []
    
            if resampled.empty:
                st.warning("No resampled maker data available for anomaly detection.")
                print("[ALL-MAXED] Resampled data empty, skipping anomaly detection")
            else:
                makers = sorted(resampled["label"].unique())
                print(f"[ALL-MAXED] Makers to check for anomalies: {makers}")
                prog_bar = st.progress(0.0)
    
                for i, mk in enumerate(makers):
                    prog_bar.progress((i + 1) / len(makers))
                    ser = (
                        resampled[resampled["label"] == mk]
                        .set_index("ds")["value"]
                        .fillna(0)
                        .sort_index()
                    )
                    print(f"[ALL-MAXED] Processing maker: {mk}, {len(ser)} points")
    
                    if len(ser) < 8 or ser.std() == 0:
                        print(f"[ALL-MAXED] Skipping {mk}, insufficient data or zero variance")
                        continue
    
                    # --- Adaptive contamination ---
                    cont_rate = min(0.05, max(0.01, ser.std() / (ser.mean() + 1e-9) * 0.02))
                    print(f"[ALL-MAXED] Contamination rate for {mk}: {cont_rate:.4f}")
    
                    # --- IsolationForest ---
                    try:
                        iso = IsolationForest(
                            contamination=cont_rate,
                            random_state=42,
                            n_estimators=200,
                            bootstrap=True,
                        )
                        X = ser.values.reshape(-1, 1)
                        preds = iso.fit_predict(X)
                        an_idxs = ser.index[preds == -1]
                        ser_an = ser.loc[an_idxs]
                        print(f"[ALL-MAXED] {len(ser_an)} anomalies detected by IsolationForest for {mk}")
    
                        for dt, val in ser_an.items():
                            anomaly_records.append({"Maker": mk, "Date": dt, "Value": val})
    
                    except Exception as e_iso:
                        print(f"[ALL-MAXED] IsolationForest failed for {mk}: {e_iso}, using z-score fallback")
                        # --- Fallback: rolling z-score ---
                        zscores = (ser - ser.rolling(6, min_periods=2).mean()) / ser.rolling(6, min_periods=2).std()
                        ser_an = ser[np.abs(zscores) > 2.8]
                        for dt, val in ser_an.items():
                            anomaly_records.append({"Maker": mk, "Date": dt, "Value": val})
                        print(f"[ALL-MAXED] {len(ser_an)} anomalies detected by z-score fallback for {mk}")
    
                prog_bar.empty()
                print(f"[ALL-MAXED] Total anomalies collected: {len(anomaly_records)}")
    
                # -------------------------------
                # üßæ Summary + Visualization
                # -------------------------------
                if not anomaly_records:
                    st.success("‚úÖ No significant anomalies detected across makers.")
                    print("[ALL-MAXED] No anomalies found")
                else:
                    df_an = pd.DataFrame(anomaly_records)
                    df_an["Date"] = pd.to_datetime(df_an["Date"])
                    st.markdown("### üìã Detected Anomalies Summary ‚Äî Makers")
                    st.dataframe(
                        df_an.sort_values("Date", ascending=False).style.format({"Value": "{:,.0f}"}),
                        use_container_width=True,
                        height=300,
                    )
                    print(f"[ALL-MAXED] Displayed anomaly summary table ({len(df_an)} anomalies)")
    
                    # --- Maker selector for visualization ---
                    sel_mk = st.selectbox(
                        "üîç View anomalies for a specific maker", sorted(df_an["Maker"].unique())
                    )
                    ser = (
                        resampled[resampled["label"] == sel_mk]
                        .set_index("ds")["value"]
                        .fillna(0)
                        .sort_index()
                    )
                    an_dates = df_an[df_an["Maker"] == sel_mk]["Date"]
    
                    fig_a = go.Figure()
                    fig_a.add_trace(
                        go.Scatter(
                            x=ser.index,
                            y=ser.values,
                            mode="lines+markers",
                            name="Value",
                            line=dict(color="steelblue"),
                        )
                    )
                    if not an_dates.empty:
                        fig_a.add_trace(
                            go.Scatter(
                                x=an_dates,
                                y=ser.loc[an_dates],
                                mode="markers",
                                name="Anomaly",
                                marker=dict(color="red", size=10, symbol="x"),
                            )
                        )
                    fig_a.update_layout(
                        title=f"Anomalies in {sel_mk} ‚Äî Time Series Overlay",
                        template="plotly_white",
                        xaxis_title="Date",
                        yaxis_title="Registrations",
                        height=500,
                    )
                    st.plotly_chart(fig_a, use_container_width=True)
                    print(f"[ALL-MAXED] Rendered anomaly plot for maker {sel_mk}")
    
                    # Quick stats
                    st.info(f"üìä {len(df_an)} total anomalies detected across {len(makers)} makers.")
                    print(f"[ALL-MAXED] Anomaly detection complete: {len(df_an)} anomalies")
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Anomaly detection failed: {e}")
            print(f"[ALL-MAXED] Anomaly detection exception: {e}")

    # -------------------------
    # üîç CLUSTERING (KMeans) ‚Äî All-Maxed (Maker)
    # -------------------------
    if do_clustering:
        st.markdown("## üîç Clustering (KMeans) ‚Äî All-Maxed Maker Mode")
        st.caption("Groups years by their maker registration mix using KMeans with normalization, PCA view & silhouette score.")
        print("[ALL-MAXED] Starting clustering section")
    
        try:
            from sklearn.cluster import KMeans
            from sklearn.preprocessing import StandardScaler
            from sklearn.decomposition import PCA
            from sklearn.metrics import silhouette_score
            import numpy as np
    
            if pivot_year.empty:
                st.warning("No pivot_year data available for clustering.")
                print("[ALL-MAXED] pivot_year empty, skipping clustering")
            else:
                # Normalize the data
                X = pivot_year.fillna(0).values
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X)
                print(f"[ALL-MAXED] Data normalized, shape: {X_scaled.shape}")
    
                # Choose K range
                max_k = min(8, max(3, len(pivot_year) - 1))
                k = st.slider("Number of clusters (K)", 2, max_k, min(4, max_k))
                print(f"[ALL-MAXED] Selected K for clustering: {k}")
    
                # Fit KMeans
                km = KMeans(n_clusters=k, n_init="auto", random_state=42)
                labels = km.fit_predict(X_scaled)
                inertia = km.inertia_
                print(f"[ALL-MAXED] KMeans fitted, inertia: {inertia:.2f}")
    
                # Compute silhouette (safe)
                sil = silhouette_score(X_scaled, labels) if len(set(labels)) > 1 else np.nan
                print(f"[ALL-MAXED] Silhouette score: {sil:.3f}" if not np.isnan(sil) else "[ALL-MAXED] Silhouette score: n/a")
    
                df_cluster = pd.DataFrame({
                    "Year": pivot_year.index.astype(str),
                    "Cluster": labels
                })
                st.markdown("### üßæ Cluster Assignment Summary ‚Äî Makers")
                st.dataframe(df_cluster, use_container_width=True, height=300)
    
                c1, c2, c3 = st.columns(3)
                c1.metric("Clusters", str(k))
                c2.metric("Inertia (‚Üì better)", f"{inertia:.2f}")
                c3.metric("Silhouette (‚Üë better)", f"{sil:.3f}" if not np.isnan(sil) else "n/a")
    
                # --- Cluster Centers ---
                centers = pd.DataFrame(scaler.inverse_transform(km.cluster_centers_), columns=pivot_year.columns)
                st.markdown("### üß† Cluster Centers ‚Äî Approximate Maker Mix")
                st.dataframe(centers.style.format("{:,.0f}"), use_container_width=True)
                print(f"[ALL-MAXED] Cluster centers calculated for {k} clusters")
    
                # --- PCA 2D visualization ---
                try:
                    pca = PCA(n_components=2, random_state=42)
                    X_pca = pca.fit_transform(X_scaled)
                    df_pca = pd.DataFrame(X_pca, columns=["PC1", "PC2"])
                    df_pca["Year"] = pivot_year.index.astype(str)
                    df_pca["Cluster"] = labels.astype(str)
    
                    fig_pca = px.scatter(
                        df_pca,
                        x="PC1",
                        y="PC2",
                        color="Cluster",
                        symbol="Cluster",
                        hover_data=["Year"],
                        title="üìä PCA Projection ‚Äî Years clustered by Maker Mix",
                    )
                    fig_pca.update_traces(marker=dict(size=12, line=dict(width=1, color="black")))
                    fig_pca.update_layout(template="plotly_white", height=500)
                    st.plotly_chart(fig_pca, use_container_width=True)
                    print("[ALL-MAXED] PCA scatter plotted")
                except Exception as e:
                    st.warning(f"PCA visualization failed: {e}")
                    print(f"[ALL-MAXED] PCA visualization exception: {e}")
    
                # --- Radar by cluster (avg maker mix) ---
                st.markdown("### üåê Radar ‚Äî Cluster-wise Average Maker Mix")
                try:
                    fig_r = go.Figure()
                    for c in sorted(df_cluster["Cluster"].unique()):
                        cluster_mean = pivot_year.iloc[df_cluster["Cluster"] == c].mean()
                        fig_r.add_trace(go.Scatterpolar(
                            r=cluster_mean.values.tolist(),
                            theta=pivot_year.columns.tolist(),
                            fill="toself",
                            name=f"Cluster {c}",
                        ))
                    fig_r.update_layout(
                        polar=dict(radialaxis=dict(visible=True)),
                        title="Cluster-wise Average Maker Mix (Radar View)",
                        template="plotly_white",
                    )
                    st.plotly_chart(fig_r, use_container_width=True)
                    print("[ALL-MAXED] Radar chart for clusters plotted")
                except Exception as e:
                    st.warning(f"Radar view failed: {e}")
                    print(f"[ALL-MAXED] Radar chart exception: {e}")
    
                st.success(f"‚úÖ Clustering completed ‚Äî {k} groups formed, silhouette={sil:.3f if not np.isnan(sil) else 'n/a'}.")
                print(f"[ALL-MAXED] Clustering completed for K={k}, silhouette={sil}")
    
        except Exception as e:
            st.warning(f"Clustering failed: {e}")
            print(f"[ALL-MAXED] Clustering exception: {e}")

    # =====================================================
    # ü§ñ AI Narrative (All-Maxed, guarded, Maker)
    # =====================================================
    if enable_ai and do_forecast:
        st.markdown("## ü§ñ AI Narrative (Summary & Recommendations) ‚Äî All-Maxed Maker")
        print("[ALL-MAXED AI] Starting AI narrative section")
    
        try:
            # Basic safety checks
            if pivot_year is None or pivot_year.empty or df_cat_all is None or df_cat_all.empty:
                st.warning("‚ö†Ô∏è No valid data available for AI narrative.")
                print("[ALL-MAXED AI] No valid data for AI narrative, exiting section")
            else:
                print("[ALL-MAXED AI] Data checks passed, preparing AI payload")
    
                # --- UI controls for AI block ---
                st.caption(
                    "Generate a concise analyst-style summary plus 3 actionable recommendations. "
                    "AI output requires a configured provider (universal_chat or other)."
                )
                allow_send = st.checkbox(
                    "I consent to sending an anonymized summary of aggregated metrics to an AI provider",
                    value=False
                )
                preview_prompt = st.checkbox("Preview the prompt sent to the AI (truncated)", value=False)
                show_raw = st.checkbox("Show raw AI output (if available)", value=False)
    
                # Prepare compact aggregated payload
                agg = pivot_year.reset_index().copy()
                agg["year"] = agg["year"].astype(str)
                for col in agg.columns:
                    if col != "year":
                        agg[col] = agg[col].fillna(0).astype(int)
    
                small_records = agg.to_dict(orient="records")
                small_preview = json.dumps(small_records)[:3000]  # truncate for preview/prompt
                print(f"[ALL-MAXED AI] Aggregated payload prepared, {len(small_records)} records, preview chars: {len(small_preview)}")
    
                # Basic computed metrics
                total_by_year = agg.set_index("year").sum(axis=1)
                total_first = float(total_by_year.iloc[0]) if len(total_by_year) > 0 else 0.0
                total_last = float(total_by_year.iloc[-1]) if len(total_by_year) > 0 else 0.0
                years_count = max(1, len(total_by_year) - 1)
                cagr_val = (
                    ((total_last / total_first) ** (1 / years_count) - 1) * 100
                    if total_first > 0 and len(total_by_year) > 1 else 0.0
                )
                print(f"[ALL-MAXED AI] CAGR calculated: {cagr_val:.2f}%")
    
                # Top makers + latest year growth
                top_overall = df_cat_all.groupby("label")["value"].sum().sort_values(ascending=False)
                top3 = top_overall.head(3)
                latest_year = pivot_year.index.max() if not pivot_year.empty else None
                growth_per_maker = {}
                if latest_year is not None and latest_year - 1 in pivot_year.index:
                    prev = pivot_year.loc[latest_year - 1] if (latest_year - 1) in pivot_year.index else None
                    curr = pivot_year.loc[latest_year]
                    if prev is not None:
                        for maker in pivot_year.columns:
                            prev_v = float(prev.get(maker, 0))
                            curr_v = float(curr.get(maker, 0))
                            growth_per_maker[maker] = (
                                ((curr_v - prev_v) / prev_v * 100) if prev_v > 0 else (100.0 if curr_v > 0 else 0.0)
                            )
                print(f"[ALL-MAXED AI] Top 3 makers: {list(top3.index)}")
    
                # Construct system + user prompts
                system_prompt = (
                    "You are a senior transport data analyst. Provide a concise, factual summary (3-6 bullet points) of trends "
                    "in vehicle maker registrations based on the provided aggregated metrics. Then give 3 short, prioritized, "
                    "actionable recommendations for policymakers or transport planners. Use percentages where relevant. Do not invent facts."
                )
                user_context = (
                    f"Aggregated yearly totals (truncated): {small_preview}\n\n"
                    f"Top makers overall: {', '.join(top3.index.tolist())}.\n"
                )
    
                if preview_prompt:
                    st.expander("Prompt preview (truncated)", expanded=False).write({
                        "system": system_prompt,
                        "user": user_context[:4000]
                    })
                    print("[ALL-MAXED AI] Prompt preview displayed")
    
                ai_text = None
                ai_raw = None
    
                # Only attempt network/LLM call if user consented
                if allow_send:
                    try:
                        print("[ALL-MAXED AI] User consented, attempting AI provider call")
                        if "universal_chat" in globals() or "universal_chat" in locals():
                            ai_resp = universal_chat(system_prompt, user_context, stream=False, temperature=0.0, max_tokens=500, retries=2)
                            if isinstance(ai_resp, dict):
                                ai_raw = ai_resp
                                ai_text = ai_resp.get("text") or ai_resp.get("response") or ai_resp.get("output")
                            elif isinstance(ai_resp, str):
                                ai_text = ai_resp
                                ai_raw = {"text": ai_resp}
                            print("[ALL-MAXED AI] AI provider returned a response")
                        else:
                            st.info("No AI provider (`universal_chat`) found in this environment. Falling back to deterministic summary.")
                            print("[ALL-MAXED AI] No AI provider found")
                    except Exception as e:
                        st.warning(f"AI provider error or network issue: {e}")
                        print(f"[ALL-MAXED AI] AI call exception: {e}")
                        ai_text = None
    
                # Fallback deterministic narrative
                if not ai_text:
                    print("[ALL-MAXED AI] Using deterministic fallback summary")
                    bullets = []
                    bullets.append(f"Top makers overall: {', '.join(top3.index.tolist())}.")
                    if not math.isnan(cagr_val) and abs(cagr_val) > 0.01:
                        dir_word = "increased" if cagr_val > 0 else "declined"
                        bullets.append(
                            f"Total registrations {dir_word} at ~{abs(cagr_val):.2f}% CAGR between {years[0]} and {years[-1]}."
                        )
                    else:
                        bullets.append("Total registrations remained roughly flat across the selected years.")
                    notable = []
                    for maker in top3.index:
                        g = growth_per_maker.get(maker, None)
                        if g is not None:
                            notable.append(f"{maker} {('up' if g>0 else 'down')} {abs(g):.1f}% YoY (latest).")
                    if notable:
                        bullets.append("Notable recent moves: " + "; ".join(notable))
                    bullets.append("Data note: aggregated counts shown; verify monthly cadence for short-term trends.")
                    recs = [
                        "Monitor and support high-growth makers (e.g., EV, light-commercial) with targeted policy incentives.",
                        "Improve inspection and road-safety programs focused on top-volume makers to reduce incidents.",
                        "Increase data cadence (move to monthly ingestion if possible) to enable finer forecasting and earlier anomaly detection."
                    ]
                    st.markdown("### üß† Quick Narrative (deterministic fallback)")
                    for b in bullets:
                        st.markdown(f"- {b}")
                    st.markdown("**Recommendations:**")
                    for i, r in enumerate(recs, 1):
                        st.markdown(f"{i}. {r}")
                else:
                    st.markdown("### üß† AI Summary")
                    st.markdown(ai_text)
                    if show_raw and ai_raw is not None:
                        st.expander("Raw AI response (debug)", expanded=False).write(ai_raw)
                    print("[ALL-MAXED AI] AI narrative displayed")
    
                try:
                    st.session_state["_last_ai_narrative"] = ai_text or "\n".join(bullets)
                    print("[ALL-MAXED AI] Narrative cached in session_state")
                except Exception:
                    print("[ALL-MAXED AI] Failed to cache narrative in session_state")
    
        except Exception as e:
            st.error(f"üí• AI Narrative generation failed: {e}")
            print(f"[ALL-MAXED AI] Exception: {e}")

    # =====================================================
    # üß© ALL-MAXED FINAL SUMMARY + DEBUG INSIGHTS (MAKER MODE)
    # =====================================================
    st.markdown("## üß† Final Summary & Debug Insights ‚Äî ALL-MAXED Maker")
    print("[ALL-MAXED FINAL] Starting final summary section")
    
    try:
        summary_start = time.time()
    
        # ----------------------------------------------------
        # 1Ô∏è‚É£ SAFE INPUT + FALLBACKS
        # ----------------------------------------------------
        df_src = df_maker_all.copy() if "df_maker_all" in locals() else pd.DataFrame()
        freq = freq if "freq" in locals() else "Monthly"
        years = years if "years" in locals() and years else [2024, 2025]
        current_year = datetime.now().year
    
        print(f"[ALL-MAXED FINAL] Rows: {len(df_src)}, Freq: {freq}, Years: {years}")
    
        if df_src.empty:
            st.warning("‚ö†Ô∏è No valid ALL-MAXED Maker data found to summarize.")
            print("[ALL-MAXED FINAL] Data empty, stopping section")
            st.stop()
    
        # ----------------------------------------------------
        # 2Ô∏è‚É£ BASIC CLEANUP + ADD TIME FIELDS
        # ----------------------------------------------------
        if "ds" not in df_src.columns and "date" in df_src.columns:
            df_src["ds"] = pd.to_datetime(df_src["date"])
        elif "ds" not in df_src.columns:
            df_src["ds"] = pd.to_datetime(df_src["year"].astype(str) + "-01-01")
    
        df_src["year"] = df_src["ds"].dt.year
        df_src["month"] = df_src["ds"].dt.month
        df_src["maker"] = df_src["label"].astype(str)
    
        print(f"[ALL-MAXED FINAL] Added time fields, first 3 rows:\n{df_src.head(3)}")
    
        # ----------------------------------------------------
        # 3Ô∏è‚É£ RESAMPLING BASED ON FREQUENCY
        # ----------------------------------------------------
        resampled = (
            df_src.groupby(["maker", "year"])["value"].sum().reset_index()
            if freq == "Yearly"
            else df_src.copy()
        )
    
        pivot = resampled.pivot_table(
            index="ds", columns="maker", values="value", aggfunc="sum"
        ).fillna(0)
    
        pivot_year = (
            resampled.pivot_table(
                index="year", columns="maker", values="value", aggfunc="sum"
            ).fillna(0)
            if "year" in resampled.columns
            else pd.DataFrame()
        )
    
        print(f"[ALL-MAXED FINAL] Resampled shape: {resampled.shape}, Pivot year shape: {pivot_year.shape}")
    
        # ----------------------------------------------------
        # 4Ô∏è‚É£ KPI METRICS (YoY, CAGR, MoM, Maker Shares)
        # ----------------------------------------------------
        st.subheader("üíé Key Metrics & Growth (All-Maxed Maker)")
    
        if pivot_year.empty:
            st.warning("‚ö†Ô∏è No yearly data found for KPI computation.")
            print("[ALL-MAXED FINAL] Pivot year empty, stopping KPI computation")
            st.stop()
    
        # --- Compute totals and YoY
        year_totals = pivot_year.sum(axis=1).rename("TotalRegistrations").to_frame()
        year_totals["YoY_%"] = year_totals["TotalRegistrations"].pct_change() * 100
        year_totals["TotalRegistrations"] = year_totals["TotalRegistrations"].fillna(0).astype(int)
        year_totals["YoY_%"] = year_totals["YoY_%"].replace([np.inf, -np.inf], np.nan).fillna(0)
        print(f"[ALL-MAXED FINAL] Year totals:\n{year_totals}")
    
        # --- CAGR
        if len(year_totals) >= 2:
            first = float(year_totals["TotalRegistrations"].iloc[0])
            last = float(year_totals["TotalRegistrations"].iloc[-1])
            years_count = max(1, len(year_totals) - 1)
            cagr = ((last / first) ** (1 / years_count) - 1) * 100 if first > 0 else 0.0
        else:
            cagr = 0.0
        print(f"[ALL-MAXED FINAL] CAGR: {cagr:.2f}%")
    
        # --- MoM if monthly
        if freq == "Monthly":
            resampled["month_period"] = resampled["year"].astype(str) + "-" + resampled["month"].astype(str)
            month_totals = resampled.groupby("month_period")["value"].sum().reset_index()
            month_totals["MoM_%"] = month_totals["value"].pct_change() * 100
            latest_mom = (
                f"{month_totals['MoM_%'].iloc[-1]:.2f}%"
                if len(month_totals) > 1 and not np.isnan(month_totals["MoM_%"].iloc[-1])
                else "n/a"
            )
        else:
            latest_mom = "n/a"
        print(f"[ALL-MAXED FINAL] Latest MoM: {latest_mom}")
    
        # --- Maker shares
        latest_year = int(year_totals.index.max())
        latest_total = int(year_totals.loc[latest_year, "TotalRegistrations"])
        maker_share = (
            (pivot_year.loc[latest_year] / pivot_year.loc[latest_year].sum() * 100)
            .sort_values(ascending=False)
            .round(1)
        )
        print(f"[ALL-MAXED FINAL] Maker shares for {latest_year}:\n{maker_share}")
    
        # ----------------------------------------------------
        # 5Ô∏è‚É£ DISPLAY KPIs
        # ----------------------------------------------------
        c1, = st.columns(1)
        c1.metric("üìÖ Years Loaded", f"{years[0]} ‚Üí {years[-1]}", f"{len(years)} yrs")
    
        st.markdown("#### üìò Maker Share (Latest Year)")
        st.dataframe(
            pd.DataFrame({
                "Maker": maker_share.index,
                "Share_%": maker_share.values,
                "Volume": pivot_year.loc[latest_year].astype(int).values,
            }).sort_values("Share_%", ascending=False),
            use_container_width=True,
        )
    
        # ----------------------------------------------------
        # 6Ô∏è‚É£ DEEP INSIGHTS + TRENDS
        # ----------------------------------------------------
        total_all = df_src["value"].sum()
        n_makers = df_src["maker"].nunique()
        n_years = df_src["year"].nunique()
        print(f"[ALL-MAXED FINAL] Total registrations: {total_all}, Makers: {n_makers}, Years: {n_years}")
    
        # --- Top Maker
        top_maker_row = df_src.groupby("maker")["value"].sum().reset_index().sort_values("value", ascending=False).iloc[0]
        top_maker = {"maker": str(top_maker_row["maker"]), "value": float(top_maker_row["value"])}
        top_maker_share = (top_maker["value"] / total_all) * 100 if total_all > 0 else 0
    
        # --- Top Year
        top_year_row = df_src.groupby("year")["value"].sum().reset_index().sort_values("value", ascending=False).iloc[0]
        top_year = {"year": int(top_year_row["year"]), "value": float(top_year_row["value"])}
        print(f"[ALL-MAXED FINAL] Top maker: {top_maker}, Peak year: {top_year}")
    
        # --- Display metrics safely
        st.metric("üèÜ Absolute Top Maker", top_maker["maker"], f"{top_maker_share:.2f}% share")
        st.metric("üìÖ Peak Year", f"{top_year['year']}", f"{top_year['value']:,.0f} registrations")
    
        # --- Plot: Top 10 Makers
        st.write("### üßæ Top 10 Makers ‚Äî Overall")
        top_debug = df_src.groupby("maker")["value"].sum().reset_index().sort_values("value", ascending=False)
        print(f"[ALL-MAXED FINAL] Top 10 makers:\n{top_debug.head(10)}")
        fig_top10 = px.bar(
            top_debug.head(10),
            x="maker",
            y="value",
            text_auto=True,
            color="value",
            color_continuous_scale="Blues",
            title="Top 10 Makers (All Years)",
        )
        fig_top10.update_layout(template="plotly_white", margin=dict(t=50, b=40))
        st.plotly_chart(fig_top10, use_container_width=True)

        # ----------------------------------------------------
        # 9Ô∏è‚É£ EXPORT XLSX ‚Äî ALL-MAXED MAKER DASHBOARD
        # ----------------------------------------------------
        import io
        import pandas as pd
        import xlsxwriter
        
        st.markdown("### üíæ Export ALL-MAXED Maker Excel Dashboard")
        
        try:
            output = io.BytesIO()
            with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
                workbook = writer.book
        
                # ------------------ Sheet 1: Dashboard KPIs ------------------
                summary_df = pd.DataFrame({
                    "Metric": ["Years Loaded", "Total Makers", "Total Registrations", "Top Maker", "Top Maker Share (%)", 
                               "Peak Year", "Peak Year Registrations", "CAGR (%)", "Latest MoM (%)"],
                    "Value": [f"{years[0]} ‚Üí {years[-1]}", n_makers, total_all,
                              top_maker["maker"], round(top_maker_share, 2),
                              top_year["year"], int(top_year["value"]),
                              round(cagr, 2), latest_mom]
                })
                summary_df.to_excel(writer, sheet_name="Dashboard", index=False)
                ws = writer.sheets["Dashboard"]
        
                # Header format
                header_format = workbook.add_format({'bold': True, 'bg_color': '#051937', 'font_color': 'white', 'align': 'center', 'border':1})
                for col_num, value in enumerate(summary_df.columns.values):
                    ws.write(0, col_num, value, header_format)
                ws.set_column(0, 0, 35)
                ws.set_column(1, 1, 25)
        
                # Highlight top metrics
                highlight_format = workbook.add_format({'bg_color': '#00bf72', 'font_color': 'white', 'bold': True})
                ws.write(3, 1, top_maker["maker"], highlight_format)
                ws.write(5, 1, top_year["year"], highlight_format)
        
                # ------------------ Sheet 2: Yearly Pivot with Charts ------------------
                if not pivot_year.empty:
                    pivot_year.to_excel(writer, sheet_name="Yearly_Pivot")
                    ws2 = writer.sheets["Yearly_Pivot"]
        
                    # Column widths
                    for i, col in enumerate(pivot_year.columns):
                        ws2.set_column(i+1, i+1, max(len(str(col)), 15))  # +1 for index
        
                    # Conditional formatting
                    ws2.conditional_format(1,1,len(pivot_year),len(pivot_year.columns), {'type': '3_color_scale'})
        
                    # Column chart for total registrations
                    chart = workbook.add_chart({'type': 'column'})
                    chart.add_series({
                        'name': 'Total Registrations',
                        'categories': ['Yearly_Pivot', 1, 0, len(pivot_year), 0],
                        'values': ['Yearly_Pivot', 1, 1, len(pivot_year), 1],
                        'fill': {'color': '#008793'}
                    })
                    chart.set_title({'name': 'Total Registrations per Year'})
                    chart.set_x_axis({'name': 'Year'})
                    chart.set_y_axis({'name': 'Registrations'})
                    ws2.insert_chart('H2', chart, {'x_scale': 1.5, 'y_scale': 1.5})
        
                # ------------------ Sheet 3: Top Makers ------------------
                top_maker_df = df_src.groupby("maker")["value"].sum().reset_index().sort_values("value", ascending=False)
                top_maker_df.to_excel(writer, sheet_name="Top_Makers", index=False)
                ws3 = writer.sheets["Top_Makers"]
                for i, col in enumerate(top_maker_df.columns):
                    ws3.set_column(i, i, max(len(str(col)), 20))
        
                # Top 10 bar chart
                n_top = min(10, len(top_maker_df))
                chart2 = workbook.add_chart({'type': 'bar'})
                chart2.add_series({
                    'name': 'Registrations',
                    'categories': ['Top_Makers', 1, 0, n_top, 0],
                    'values': ['Top_Makers', 1, 1, n_top, 1],
                    'fill': {'color': '#004d7a'}
                })
                chart2.set_title({'name': 'Top 10 Makers'})
                chart2.set_x_axis({'name': 'Registrations'})
                chart2.set_y_axis({'name': 'Maker'})
                ws3.insert_chart('D2', chart2, {'x_scale': 2, 'y_scale': 1.5})
        
            # XLSX written automatically
            processed_data = output.getvalue()
            st.download_button(
                "üíæ Download ALL-MAXED Maker Excel Dashboard",
                processed_data,
                "ALL-MAXED_Makers_Dashboard.xlsx"
            )
            print("[ALL-MAXED FINAL] Excel export ready")
        except Exception as e:
            st.error(f"‚õî Excel export failed: {e}")
            print(f"[ALL-MAXED FINAL] Excel export exception: {e}")

    
        # ----------------------------------------------------
        # 7Ô∏è‚É£ ADVANCED DEBUG METRICS
        # ----------------------------------------------------
        volatility = df_src.groupby("year")["value"].sum().pct_change().std() * 100 if len(df_src["year"].unique()) > 2 else 0
        dominance_ratio = (top_maker["value"] / total_all) * n_makers if total_all > 0 else 0
        direction = "increased" if cagr > 0 else "declined"
        summary_time = time.time() - summary_start
        print(f"[ALL-MAXED FINAL] Volatility: {volatility:.2f}, Dominance ratio: {dominance_ratio:.2f}, Runtime: {summary_time:.2f}s")
    
        st.markdown("### ‚öôÔ∏è Debug Performance Metrics")
        st.code(
            f"""
    Years analyzed: {years}
    Makers: {n_makers}
    Rows processed: {len(df_src):,}
    Total registrations: {total_all:,.0f}
    Top maker: {top_maker['maker']} ‚Üí {top_maker['value']:,.0f} ({top_maker_share:.2f}%)
    Peak year: {int(top_year['year'])} ‚Üí {top_year['value']:,.0f}
    Dominance ratio: {dominance_ratio:.2f}
    Runtime: {summary_time:.2f}s
            """,
            language="yaml",
        )
    
        # ----------------------------------------------------
        # 8Ô∏è‚É£ SMART SUMMARY
        # ----------------------------------------------------
        if top_maker and years and top_year:
            st.success(
                f"From **{years[0]}** to **{years[-1]}**, total registrations {direction}. "
                f"**{top_maker.get('maker', 'N/A')}** leads with **{top_maker_share:.2f}%** share. "
                f"Peak year: **{top_year['year']}** with **{top_year['value']:,.0f}** registrations."
            )
            print("[ALL-MAXED FINAL] Smart summary displayed successfully")
        else:
            st.error("‚õî ALL-MAXED Maker summary failed: Missing or invalid data.")
            print("[ALL-MAXED FINAL] Smart summary failed due to incomplete data")
    
    except Exception as e:
        st.error(f"‚õî ALL-MAXED Maker summary failed: {e}")
        print(f"[ALL-MAXED FINAL] Exception occurred: {e}")
    
# -----------------------------------------------------
# üß© Safe Entry Point ‚Äî Streamlit-only Execution Guard
# -----------------------------------------------------
if __name__ == "__main__":
    import streamlit as st
    import traceback

    print("[ALL-MAXED ENTRY] Streamlit app starting...")
    
    st.markdown("# üöó ALL-MAXED MAKER ANALYTICS")
    
    try:
        print("[ALL-MAXED ENTRY] Calling all_maxed_maker_block()")
        all_maxed_maker_block()
        print("[ALL-MAXED ENTRY] all_maxed_maker_block() completed successfully")
    except Exception as e:
        st.error(f"üí• Error while rendering All-Maxed block: {e}")
        st.code(traceback.format_exc(), language="python")
        print(f"[ALL-MAXED ENTRY] Exception occurred: {e}")
        print(traceback.format_exc())

#-----------------------------

import streamlit as st
import pandas as pd
import numpy as np
import random
from datetime import datetime
import plotly.express as px

# -----------------------------
# ‚öôÔ∏è Safe defaults for params
# -----------------------------
from datetime import datetime

def safe_value(val, default):
    """Ensures value is not None, empty, or invalid."""
    if val is None or (isinstance(val, str) and val.strip() == ""):
        print(f"[SAFE_VALUE] Replacing empty or None with default: {default}")
        return default
    return val

from_year = safe_value(locals().get("from_year"), 2018)
to_year = safe_value(locals().get("to_year"), datetime.now().year)
state_code = safe_value(locals().get("state_code"), "ALL")
rto_code = safe_value(locals().get("rto_code"), "0")
vehicle_classes = safe_value(locals().get("vehicle_classes"), "ALL")
vehicle_makers = safe_value(locals().get("vehicle_makers"), "ALL")
frequency = safe_value(locals().get("freq") or locals().get("frequency"), "Monthly")
fitness_check = safe_value(locals().get("fitness_check"), "ALL")
vehicle_type = safe_value(locals().get("vehicle_type"), "ALL")

print(f"[PARAMS] from_year={from_year}, to_year={to_year}, state_code={state_code}, rto_code={rto_code}")
print(f"[PARAMS] vehicle_classes={vehicle_classes}, vehicle_makers={vehicle_makers}, frequency={frequency}")
print(f"[PARAMS] fitness_check={fitness_check}, vehicle_type={vehicle_type}")

# -----------------------------
# üîπ Build params safely
# -----------------------------
try:
    params_common1 = build_params(
        from_year=from_year,
        to_year=to_year,
        state_code=state_code,
        rto_code=rto_code,
        vehicle_classes=vehicle_classes,
        vehicle_makers=vehicle_makers,
        time_period=frequency,
        fitness_check=fitness_check,
        vehicle_type=vehicle_type
    )
    st.success("‚úÖ Parameters successfully built")
    print("[PARAMS] Parameters successfully built")
except Exception as e:
    st.error(f"‚ùå Parameter build failed: {e}")
    print(f"[PARAMS] Parameter build failed: {e}")
    st.stop()

# -----------------------------
# üîπ Safe fetch top 5 revenue
# -----------------------------
def safe_get_top5_(params):
    try:
        top5_json, url = get_json("vahandashboard/top5chartRevenueFee", params)
        df = parse_top5_revenue(top5_json)
        if df.empty or "label" not in df.columns or "value" not in df.columns:
            raise ValueError("Invalid API data")
        st.success(f"‚úÖ Top 5 revenue fetched from API ({url})")
        print(f"[TOP5] Fetched from API: {url}")
    except Exception as e:
        st.warning(f"‚ö†Ô∏è API unavailable or failed: {e}\nUsing fallback mock data.")
        print(f"[TOP5] API failed: {e}. Using fallback data.")
        states = ["MH", "DL", "KA", "TN", "UP"]
        revenues = [random.randint(500, 2000) for _ in states]
        df = pd.DataFrame({"label": states, "value": revenues})
        print(f"[TOP5] Fallback data created: {df.to_dict(orient='records')}")

    df = df.rename(columns={"label": "State", "value": "Revenue"})
    return df

# -----------------------------
# üîπ Plot Top 5 Revenue
# -----------------------------
st.markdown("## üí∞ ALL-MAXED ‚Äî Top 5 Revenue States Analytics Suite")
df_top5 = safe_get_top5_(params_common1)

print(f"[TOP5] Data for plotting:\n{df_top5}")

# Bar chart
fig_bar = px.bar(
    df_top5,
    x="State",
    y="Revenue",
    title="Top 5 Revenue States (Bar)",
    text="Revenue",
    labels={"Revenue": "Revenue (‚Çπ Cr)", "State": "State"}
)
fig_bar.update_layout(template="plotly_white")
st.plotly_chart(fig_bar, use_container_width=True)
print("[TOP5] Bar chart rendered")

# Pie chart
fig_pie = px.pie(
    df_top5,
    names="State",
    values="Revenue",
    title="Top 5 Revenue States (Pie)",
    hole=0.4
)
st.plotly_chart(fig_pie, use_container_width=True)
print("[TOP5] Pie chart rendered")

# KPI summary
total_rev = df_top5["Revenue"].sum()
top_state = df_top5.loc[df_top5["Revenue"].idxmax(), "State"]
top_value = df_top5["Revenue"].max()

st.markdown("### üíé Key Metrics")
st.write(f"- **Total Revenue:** ‚Çπ{total_rev:,} Cr")
st.write(f"- **Top State:** {top_state} with ‚Çπ{top_value:,} Cr")
print(f"[TOP5] Total Revenue: ‚Çπ{total_rev:,} Cr")
print(f"[TOP5] Top State: {top_state} with ‚Çπ{top_value:,} Cr")

# --------------------------------------------------
# üîπ Advanced Analytics ‚Äî Trend Simulation
# --------------------------------------------------
st.markdown("### üîÆ Simulated Multi-Year Trend (Safe + Maxed)")

# Generate mock revenue trend per state (2019-2025)
years = list(range(2019, 2025))
trend_data = []
for state in df_top5["State"]:
    base = df_top5.loc[df_top5["State"]==state, "Revenue"].values[0]
    for y in years:
        val = int(base * (1 + 0.08*(y-2019)) + random.randint(-50,50))
        trend_data.append({"State": state, "Year": y, "Revenue": val})

df_trend = pd.DataFrame(trend_data)
print("[TREND] Generated multi-year revenue trend:")
print(df_trend)

# Multi-year comparison chart
fig_trend = px.line(
    df_trend, x="Year", y="Revenue", color="State",
    markers=True, title="Top 5 Revenue States: Multi-Year Trend"
)
st.plotly_chart(fig_trend, use_container_width=True)
print("[TREND] Multi-year trend chart rendered")

# --------------------------------------------------
# üîπ Anomaly Detection (IsolationForest)
# --------------------------------------------------
st.markdown("### ‚ö†Ô∏è Anomaly Detection ‚Äî Revenue Trends")
try:
    def detect_anomaly(x):
        iso = IsolationForest(contamination=0.1, random_state=42)
        preds = iso.fit_predict(x.values.reshape(-1,1))
        print(f"[ANOMALY] State: {x.name}, Revenue: {x.values}, Predictions: {preds}")
        return preds

    df_trend["Anomaly"] = df_trend.groupby("State")["Revenue"].transform(detect_anomaly)

    # Map labels to readable
    df_trend["AnomalyLabel"] = df_trend["Anomaly"].map({1:"Normal",-1:"Anomaly"})
    print("[ANOMALY] Full DataFrame with anomaly labels:")
    print(df_trend)

    fig_anom = px.scatter(
        df_trend, x="Year", y="Revenue",
        color="AnomalyLabel",
        facet_col="State",
        title="Revenue Trend Anomalies by State"
    )
    st.plotly_chart(fig_anom, use_container_width=True)
except Exception as e:
    st.warning(f"Anomaly detection failed: {e}")
    print(f"[ANOMALY] Exception: {e}")

# --------------------------------------------------
# üîπ Clustering (KMeans)
# --------------------------------------------------
st.markdown("### üîç Clustering ‚Äî Revenue Patterns Across States")
try:
    pivot = df_trend.pivot(index="Year", columns="State", values="Revenue").fillna(0)
    
    # Determine number of clusters (safely)
    k = min(3, len(pivot.columns))
    km = KMeans(n_clusters=k, n_init="auto", random_state=42)
    cluster_labels = km.fit_predict(pivot.values)
    
    pivot["Cluster"] = cluster_labels
    
    # Print cluster assignments
    print("[CLUSTER] Cluster labels per year:")
    print(pivot[["Cluster"]])
    
    # Print cluster centers
    print("[CLUSTER] Cluster centers:")
    print(km.cluster_centers_)
    
    st.dataframe(pivot)
    
    # Optional: simple scatter visualization of clusters
    pivot_reset = pivot.reset_index()
    fig_clusters = px.scatter(
        pivot_reset, x="Year", y=pivot_reset.columns[1],  # example: first state column
        color=pivot_reset["Cluster"].astype(str),
        title="KMeans Clustering ‚Äî Revenue Patterns by Year",
        labels={"color": "Cluster"}
    )
    st.plotly_chart(fig_clusters, use_container_width=True)
    
except Exception as e:
    st.warning(f"Clustering unavailable: {e}")
    print(f"[CLUSTER] Exception: {e}")

st.markdown("---")
st.success("‚úÖ ALL-MAXED Top 5 Revenue States Dashboard Ready!")

# =====================================================
# üß© ALL-MAXED FINAL SUMMARY + EXCEL EXPORT ‚Äî STATES
# =====================================================
import io
import time
import xlsxwriter
from sklearn.ensemble import IsolationForest
from sklearn.cluster import KMeans

st.markdown("## üß† Final Summary & Debug Insights ‚Äî ALL-MAXED States")
print("[ALL-MAXED FINAL] Starting final summary & export section")

try:
    start_time = time.time()

    # -------------------------
    # Safety: ensure df_top5 & df_trend exist
    # -------------------------
    if "df_top5" not in locals() or df_top5 is None or df_top5.empty:
        print("[ALL-MAXED FINAL] df_top5 missing ‚Äî creating fallback")
        df_top5 = pd.DataFrame({"State": ["MH","DL","KA","TN","UP"], "Revenue":[1000,800,700,650,600]})

    if "df_trend" not in locals() or df_trend is None or df_trend.empty:
        print("[ALL-MAXED FINAL] df_trend missing ‚Äî generating fallback multi-year trend")
        yrs = list(range(from_year if 'from_year' in locals() else 2019, (to_year if 'to_year' in locals() else datetime.now().year)+1))
        rows = []
        for s, v in zip(df_top5["State"], df_top5["Revenue"]):
            base = int(v)
            for y in yrs:
                rows.append({"State": s, "Year": int(y), "Revenue": max(0, int(base * (1 + 0.06 * (y-yrs[0])) + random.randint(-50,50)))})
        df_trend = pd.DataFrame(rows)

    # -------------------------
    # Add anomaly column (IsolationForest) if not present
    # -------------------------
    if "Anomaly" not in df_trend.columns:
        try:
            print("[ALL-MAXED FINAL] Running IsolationForest anomaly detection")
            def _detect(series, cont=0.10):
                iso = IsolationForest(contamination=cont, random_state=42)
                preds = iso.fit_predict(series.values.reshape(-1,1))
                return preds
            df_trend["Anomaly"] = df_trend.groupby("State")["Revenue"].transform(lambda x: _detect(x, cont=0.10))
            df_trend["AnomalyLabel"] = df_trend["Anomaly"].map({1:"Normal", -1:"Anomaly"})
            print("[ALL-MAXED FINAL] Anomaly detection finished")
        except Exception as e:
            print("[ALL-MAXED FINAL] Anomaly detection failed:", e)
            df_trend["Anomaly"] = 1
            df_trend["AnomalyLabel"] = "Normal"

    # -------------------------
    # Yearly pivot (for KPI calculations)
    # -------------------------
    pivot_year = df_trend.pivot_table(index="Year", columns="State", values="Revenue", aggfunc="sum").fillna(0)
    if pivot_year.empty:
        print("[ALL-MAXED FINAL] pivot_year empty ‚Äî aborting KPI calc gracefully")
        st.warning("‚ö†Ô∏è Not enough time-series data to compute yearly KPIs.")
    # total per year
    year_totals = pivot_year.sum(axis=1).rename("TotalRevenue").to_frame()
    year_totals["YoY_%"] = year_totals["TotalRevenue"].pct_change() * 100
    year_totals["TotalRevenue"] = year_totals["TotalRevenue"].fillna(0).astype(int)
    year_totals["YoY_%"] = year_totals["YoY_%"].replace([np.inf, -np.inf], np.nan).fillna(0)

    # CAGR
    if len(year_totals) >= 2:
        first = float(year_totals["TotalRevenue"].iloc[0])
        last = float(year_totals["TotalRevenue"].iloc[-1])
        years_count = max(1, len(year_totals)-1)
        cagr = ((last / first) ** (1 / years_count) - 1) * 100 if first > 0 else 0.0
    else:
        cagr = 0.0

    # MoM if we have monthly frequency in original data (best-effort)
    latest_mom = "n/a"
    if "Month" in df_trend.columns or (frequency if 'frequency' in locals() else None) == "Monthly":
        # attempt to compute month-over-month using df_trend if it had months; otherwise leave n/a
        try:
            # if df_trend had monthly rows with 'Month' column
            if "Month" in df_trend.columns:
                month_totals = df_trend.groupby(["Year","Month"])["Revenue"].sum().reset_index()
                month_totals["period"] = month_totals["Year"].astype(str) + "-" + month_totals["Month"].astype(str)
                month_totals["MoM_%"] = month_totals["Revenue"].pct_change() * 100
                latest_mom = f"{month_totals['MoM_%'].iloc[-1]:.2f}%" if len(month_totals)>1 else "n/a"
        except Exception:
            latest_mom = "n/a"

    # Top state and shares (latest year)
    latest_year = int(year_totals.index.max()) if not year_totals.empty else int(df_trend["Year"].max())
    latest_total = int(year_totals.loc[latest_year, "TotalRevenue"]) if latest_year in year_totals.index else int(df_trend[df_trend["Year"]==latest_year]["Revenue"].sum())
    state_shares = (pivot_year.loc[latest_year] / pivot_year.loc[latest_year].sum() * 100).sort_values(ascending=False).round(1) if latest_year in pivot_year.index else pd.Series(dtype=float)
    if not state_shares.empty:
        top_state = state_shares.idxmax()
        top_state_value = int(pivot_year.loc[latest_year, top_state])
    else:
        top_state = df_top5.loc[df_top5["Revenue"].idxmax(), "State"]
        top_state_value = int(df_top5["Revenue"].max())

    # Display KPIs
    st.subheader("üíé Key Metrics ‚Äî States")
    c1, c2, c3 = st.columns(3)
    c1.metric("Total Revenue (latest year)", f"‚Çπ{latest_total:,}")
    c2.metric("CAGR (period)", f"{cagr:.2f}%")
    c3.metric("Latest MoM", latest_mom)
    st.markdown(f"- **Top State (latest year):** {top_state} ‚Üí ‚Çπ{top_state_value:,}")
    if not state_shares.empty:
        st.markdown("#### State shares (latest year)")
        shares_df = pd.DataFrame({"State": state_shares.index, "Share_%": state_shares.values, "Revenue": pivot_year.loc[latest_year].astype(int).values}).reset_index(drop=True)
        st.dataframe(shares_df, use_container_width=True)

    # -------------------------
    # KMeans clustering on year√óstate pivot
    # -------------------------
    try:
        print("[ALL-MAXED FINAL] Running KMeans clustering")
        pivot_for_cluster = pivot_year.copy()
        k = min(3, pivot_for_cluster.shape[1]) if pivot_for_cluster.shape[1] > 0 else 1
        km = KMeans(n_clusters=k, n_init="auto", random_state=42)
        cluster_labels = km.fit_predict(pivot_for_cluster.T.values)  # cluster states by their yearly pattern (transpose)
        cluster_df = pd.DataFrame({"State": pivot_for_cluster.columns, "Cluster": cluster_labels})
        st.markdown("### üîç KMeans ‚Äî State pattern clusters")
        st.dataframe(cluster_df.sort_values("Cluster"), use_container_width=True)
        print("[ALL-MAXED FINAL] KMeans centers:", km.cluster_centers_)
    except Exception as e:
        print("[ALL-MAXED FINAL] KMeans failed:", e)
        cluster_df = pd.DataFrame({"State": pivot_year.columns if not pivot_year.empty else df_top5["State"].tolist(), "Cluster": 0})

    # -------------------------
    # üßÆ Debug metrics & 2-Year Smart Summary
    # -------------------------
    try:
        years_sorted = sorted(df_trend["Year"].unique())
        if len(years_sorted) >= 2:
            prev_year, curr_year = years_sorted[-2], years_sorted[-1]
            df_prev = df_trend[df_trend["Year"] == prev_year]
            df_curr = df_trend[df_trend["Year"] == curr_year]
    
            rows_processed = len(df_trend)
            total_prev = int(df_prev["Revenue"].sum())
            total_curr = int(df_curr["Revenue"].sum())
            yoy_change = ((total_curr - total_prev) / total_prev * 100) if total_prev else 0.0
    
            top_prev_state = df_prev.loc[df_prev["Revenue"].idxmax(), "State"]
            top_prev_value = int(df_prev["Revenue"].max())
    
            top_curr_state = df_curr.loc[df_curr["Revenue"].idxmax(), "State"]
            top_curr_value = int(df_curr["Revenue"].max())
    
            n_states = df_trend["State"].nunique()
            volatility = (
                df_trend.groupby("Year")["Revenue"].sum().pct_change().std() * 100
                if len(years_sorted) > 2 else 0.0
            )
    
            dominance_prev = (top_prev_value / total_prev) * n_states if total_prev else 0.0
            dominance_curr = (top_curr_value / total_curr) * n_states if total_curr else 0.0
    
            run_time = time.time() - start_time
    
            st.markdown("### ‚öôÔ∏è Debug Performance Metrics (2-Year Focus)")
            st.code(
    f"""Years analyzed: [{prev_year}, {curr_year}]
    Rows processed: {rows_processed:,}
    Total revenue ({prev_year}): ‚Çπ{total_prev:,}
    Total revenue ({curr_year}): ‚Çπ{total_curr:,}
    YoY Change: {yoy_change:+.2f}%
    Top state {prev_year}: {top_prev_state} ‚Üí ‚Çπ{top_prev_value:,}
    Top state {curr_year}: {top_curr_state} ‚Üí ‚Çπ{top_curr_value:,}
    Dominance ratio {prev_year}: {dominance_prev:.2f}
    Dominance ratio {curr_year}: {dominance_curr:.2f}
    Volatility (YoY sd%): {volatility:.2f}
    Runtime: {run_time:.2f}s
    """, language="yaml")
    
            summary_direction = "increased" if yoy_change > 0 else "declined" if yoy_change < 0 else "remained stable"
            st.success(
                f"Between **{prev_year} ‚Üí {curr_year}**, total revenue **{summary_direction} by {abs(yoy_change):.2f}%**.  "
                f"Top state: **{top_curr_state}** (‚Çπ{top_curr_value:,})."
            )
        else:
            st.info("‚ÑπÔ∏è Not enough data for 2-year comparison ‚Äî only one year available.")
    except Exception as e:
        st.error(f"‚ùå 2-year debug summary failed: {e}")
        print("[2-YEAR SUMMARY] Exception:", e)

    # -------------------------
    # ‚úÖ Build Excel workbook in-memory (fixed .save issue)
    # -------------------------
    st.markdown("### üíæ Export ALL-MAXED States Excel Dashboard")
    try:
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
            workbook = writer.book
    
            # Sheet 1: Top5
            df_top5.to_excel(writer, sheet_name="Top5", index=False)
            ws = writer.sheets["Top5"]
            ws.set_column(0, 0, 12)
            ws.set_column(1, 1, 15)
    
            # Sheet 2: Trend
            df_trend.to_excel(writer, sheet_name="Trend", index=False)
            ws2 = writer.sheets["Trend"]
            ws2.set_column(0, 0, 12)
            ws2.set_column(1, 1, 12)
            ws2.set_column(2, 2, 15)
    
            # Sheet 3: Yearly pivot
            if not pivot_year.empty:
                pivot_year.to_excel(writer, sheet_name="Yearly_Pivot")
                ws3 = writer.sheets["Yearly_Pivot"]
                ws3.conditional_format(
                    1, 1,
                    1 + pivot_year.shape[0],
                    1 + pivot_year.shape[1],
                    {'type': '3_color_scale'}
                )
    
                # optional chart sheet
                totals = year_totals.reset_index()
                totals.to_excel(writer, sheet_name="Yearly_Totals", index=False)
                ws_tot = writer.sheets["Yearly_Totals"]
                chart = workbook.add_chart({'type': 'column'})
                chart.add_series({
                    'name': 'TotalRevenue',
                    'categories': ['Yearly_Totals', 1, 0, len(totals), 0],
                    'values': ['Yearly_Totals', 1, 1, len(totals), 1],
                })
                chart.set_title({'name': 'Total Revenue per Year'})
                ws3.insert_chart('H2', chart, {'x_scale': 1.3, 'y_scale': 1.1})
    
            # Sheet 4: Clusters
            cluster_df.to_excel(writer, sheet_name="Clusters", index=False)
            ws4 = writer.sheets["Clusters"]
            ws4.set_column(0, 0, 15)
            ws4.set_column(1, 1, 10)
    
            # Sheet 5: Anomalies
            df_trend.to_excel(writer, sheet_name="Anomalies", index=False)
            ws5 = writer.sheets["Anomalies"]
            ws5.set_column(0, 0, 12)
            ws5.set_column(1, 1, 10)
            ws5.set_column(2, 2, 18)
    
            # Sheet 6: Summary / KPIs
            summary_df = pd.DataFrame({
                "Metric": ["Years Loaded", "Total States", "Total Revenue (sum)",
                           "Top State", "Top State Revenue", "CAGR (%)",
                           "Latest MoM", "Runtime_s"],
                "Value": [
                    f"{int(df_trend['Year'].min())} ‚Üí {int(df_trend['Year'].max())}",
                    n_states, total_revenue_all, top_state, top_state_value,
                    round(cagr, 2), latest_mom, round(run_time, 2)
                ]
            })
            summary_df.to_excel(writer, sheet_name="Summary", index=False)
            ws6 = writer.sheets["Summary"]
            ws6.set_column(0, 0, 36)
            ws6.set_column(1, 1, 22)
    
        # ‚úÖ No need to call writer.save()
        processed_data = output.getvalue()
        st.download_button(
            label="üíæ Download ALL-MAXED States Excel Dashboard",
            data=processed_data,
            file_name=f"ALL-MAXED_States_Dashboard_{int(df_trend['Year'].min())}_{int(df_trend['Year'].max())}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        )
        print("[ALL-MAXED FINAL] Excel export ready ‚úÖ")
    
    except Exception as e:
        st.error(f"‚õî Excel export failed: {e}")
        print("[ALL-MAXED FINAL] Excel export exception:", e)

except Exception as e:
    st.error(f"‚õî ALL-MAXED final failed: {e}")
    print("[ALL-MAXED FINAL] Exception occurred:", e)


# ================================================================
# üöÄ VAHAN ALL-MAXED ‚Äî Unified Trend + Growth + Revenue Analytics
# ================================================================

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import json, random, math, logging
from colorama import Fore

# ================================================================
# ‚öôÔ∏è PARAMETER SAFETY + BUILD
# ================================================================

def safe_value(val, default):
    """Ensure no parameter is blank or None."""
    if val is None or (isinstance(val, str) and not val.strip()):
        return default
    return val

def build_params(**kwargs):
    """Mock builder ‚Äî merges & cleans parameters."""
    params = {}
    for k, v in kwargs.items():
        params[k] = safe_value(v, "ALL" if isinstance(v, str) else v)
    return params

from_year = safe_value(locals().get('from_year'), 2018)
to_year = safe_value(locals().get('to_year'), datetime.now().year)
state_code = safe_value(locals().get('state_code'), 'ALL')
rto_code = safe_value(locals().get('rto_code'), '0')
vehicle_classes = safe_value(locals().get('vehicle_classes'), 'ALL')
vehicle_makers = safe_value(locals().get('vehicle_makers'), 'ALL')
frequency = safe_value(locals().get('frequency'), 'Monthly')
fitness_check = safe_value(locals().get('fitness_check'), 'ALL')
vehicle_type = safe_value(locals().get('vehicle_type'), 'ALL')

params = build_params(
    from_year=from_year,
    to_year=to_year,
    state_code=state_code,
    rto_code=rto_code,
    vehicle_classes=vehicle_classes,
    vehicle_makers=vehicle_makers,
    time_period=frequency,
    fitness_check=fitness_check,
    vehicle_type=vehicle_type
)

st.write("üîç **Params Sent to API:**")
st.json(params)

# ‚úÖ Also print to console for debugging
print(Fore.CYAN + "[DEBUG] Parameters sent to API:" + Fore.RESET)
print(params)

def normalize_trend(tr_json):
    """Normalize trend data structure."""
    if not tr_json:
        print("[DEBUG] Input trend JSON is empty. Returning empty DataFrame.")
        return pd.DataFrame()

    if "data" in tr_json:
        df = pd.DataFrame(tr_json["data"])
        print(f"[DEBUG] Loaded {len(df)} rows from 'data' key.")
    elif isinstance(tr_json, list):
        df = pd.DataFrame(tr_json)
        print(f"[DEBUG] Loaded {len(df)} rows from list input.")
    else:
        df = pd.DataFrame()
        print("[DEBUG] Input JSON unrecognized. Returning empty DataFrame.")

    if "date" not in df.columns:
        df["date"] = pd.date_range("2020-01-01", periods=len(df), freq="M")
        print("[DEBUG] 'date' column missing. Added default monthly range starting 2020-01-01.")

    if "value" not in df.columns:
        df["value"] = np.random.randint(300000, 800000, len(df))
        print("[DEBUG] 'value' column missing. Filled with random integers.")

    print(f"[DEBUG] normalize_trend returning DataFrame with {len(df)} rows and columns: {df.columns.tolist()}")
    return df

def safe_get_trend(params):
    try:
        tr_json, tr_url = get_json("vahandashboard/vahanyearwiseregistrationtrend", params)
        print(f"[DEBUG] Fetched trend JSON from URL: {tr_url}")
        df = normalize_trend(tr_json)
        print(f"[DEBUG] Trend DataFrame shape: {df.shape}")
    except Exception as e:
        print(f"[DEBUG] API fetch failed: {e}. Using fallback mock data.")
        tr_url = "MOCK://trend"
        np.random.seed(42)
        months = pd.date_range("2020-01-01", "2026-12-01", freq="MS")
        df = pd.DataFrame({
            "date": months,
            "value": (
                (np.sin(np.arange(len(months)) / 6) * 0.1 + 1.05)
                * np.linspace(400000, 950000, len(months))
                + np.random.randint(-50000, 50000, len(months))
            ).astype(int)
        })
        print(f"[DEBUG] Fallback DataFrame shape: {df.shape}")

    df["year"] = df["date"].dt.year
    df["month"] = df["date"].dt.month

    print(f"[DEBUG] Added 'year' and 'month' columns. Final shape: {df.shape}")
    return df, tr_url

with st.spinner("üì° Fetching trend series..."):
    df_tr, tr_url = safe_get_trend(params)
    print(f"[DEBUG] Trend URL used: {tr_url}")
    print(f"[DEBUG] Trend DataFrame head:\n{df_tr.head()}")
    print(f"[DEBUG] Trend DataFrame shape: {df_tr.shape}")

if df_tr.empty:
    st.error("‚ùå No trend data found.")
    print("[DEBUG] df_tr is empty ‚Äî nothing to plot.")
else:
    st.success(f"‚úÖ Trend data fetched ({len(df_tr)} records)")
    st.line_chart(df_tr.set_index("date")["value"])
    print(f"[DEBUG] Plotted trend chart for {len(df_tr)} records")

st.subheader("üìà Multi-Year Comparison")
years = sorted(df_tr["year"].unique())
selected_years = st.multiselect("Select years to compare", years, default=years[-2:])
print(f"[DEBUG] All available years: {years}")
print(f"[DEBUG] User selected years: {selected_years}")

pivot = (
    df_tr.groupby([df_tr["date"].dt.strftime("%b"), "year"])["value"]
    .sum()
    .unstack()
    .fillna(0)
)
print(f"[DEBUG] Pivot table head:\n{pivot.head()}")
print(f"[DEBUG] Pivot table columns: {pivot.columns.tolist()}")

# --- Ensure column types are strings for Plotly
pivot.columns = pivot.columns.astype(str)
selected_years_str = [str(y) for y in selected_years if str(y) in pivot.columns]
print(f"[DEBUG] Selected years (str) in pivot: {selected_years_str}")

if not selected_years_str:
    st.warning("‚ö†Ô∏è No matching years found for comparison plot.")
    print("[DEBUG] No years available for plotting.")
else:
    fig = px.line(
        pivot,
        x=pivot.index,
        y=selected_years_str,
        markers=True,
        title="Year-over-Year Registration Comparison"
    )
    st.plotly_chart(fig, use_container_width=True)
    print(f"[DEBUG] Plotted comparison chart for years: {selected_years_str}")

# ================================================================
# üí∞ REVENUE TREND MOCK
# ================================================================

def safe_get_revenue_trend(params):
    np.random.seed(42)
    now = datetime.now()
    years = list(range(2019, now.year + 1))
    df = pd.DataFrame({
        "period": [f"FY{y}-{str(y+1)[-2:]}" for y in years for _ in range(4)],
        "year": np.repeat(years, 4),
        "value": np.concatenate([
            np.linspace(random.randint(500, 800), random.randint(900, 1200), 4)
            for _ in years
        ])
    })
    return df

df_rev = safe_get_revenue_trend(params)

print(f"[DEBUG] Revenue trend head:\n{df_rev.head()}")
print(f"[DEBUG] Revenue trend years: {df_rev['year'].unique()}")
print(f"[DEBUG] Revenue trend periods: {df_rev['period'].unique()}")

st.subheader("üí∞ Revenue Trend Comparison")
fig_rev = px.line(df_rev, x="period", y="value", color="year", markers=True)
st.plotly_chart(fig_rev, use_container_width=True)

print(f"[DEBUG] Plotted revenue trend chart for years: {df_rev['year'].unique()}")


# ================================================================
# üîÆ FORECASTING + ANOMALY + CLUSTERING
# ================================================================

try:
    from sklearn.linear_model import LinearRegression
    from sklearn.ensemble import IsolationForest
    from sklearn.cluster import KMeans
except ImportError:
    import subprocess, sys
    subprocess.run([sys.executable, "-m", "pip", "install", "scikit-learn"])
    from sklearn.linear_model import LinearRegression
    from sklearn.ensemble import IsolationForest
    from sklearn.cluster import KMeans

st.subheader("üîÆ Forecasting (Linear Regression)")
series = df_tr.groupby("year")["value"].sum().reset_index()
print(f"[DEBUG] Series for forecasting:\n{series}")

X = np.arange(len(series)).reshape(-1, 1)
lr = LinearRegression().fit(X, series["value"])
future_idx = np.arange(len(series) + 5).reshape(-1, 1)
preds = lr.predict(future_idx)
future_years = list(range(series["year"].iloc[0], series["year"].iloc[0] + len(preds)))
df_pred = pd.DataFrame({"year": future_years, "pred": preds})
print(f"[DEBUG] Forecasted values:\n{df_pred}")

fig_lin = px.line(df_pred, x="year", y="pred", title="Linear Forecast", markers=True)
fig_lin.add_scatter(x=series["year"], y=series["value"], mode="lines+markers", name="Actual")
st.plotly_chart(fig_lin, use_container_width=True)

# -----------------------------
st.subheader("‚ö†Ô∏è Anomaly Detection")
iso = IsolationForest(contamination=0.03, random_state=42)
df_tr["anomaly"] = iso.fit_predict(df_tr[["value"]])
print(f"[DEBUG] Anomalies detected:\n{df_tr[['date','value','anomaly']].tail()}")

fig_a = px.scatter(df_tr, x="date", y="value",
                   color=df_tr["anomaly"].map({1: "Normal", -1: "Anomaly"}))
st.plotly_chart(fig_a, use_container_width=True)

# -----------------------------
st.subheader("üîç Clustering (Monthly Patterns)")

# Pivot (year vs month)
month_pivot = (
    df_tr.pivot_table(index="year", columns="month", values="value", aggfunc="sum")
    .fillna(0)
)
st.write(f"[DEBUG] Pivot for clustering shape: {month_pivot.shape}")

num_years = len(month_pivot)

if num_years < 2:
    st.info("üìâ Only one year of data ‚Äî clustering skipped.")
    st.dataframe(month_pivot)

elif num_years == 2:
    st.info("‚öôÔ∏è Only two years available ‚Äî using K=2 automatically.")
    try:
        km = KMeans(n_clusters=2, random_state=42)
        month_pivot["Cluster"] = km.fit_predict(month_pivot)
        st.dataframe(month_pivot)
        st.write(f"[DEBUG] Cluster assignments:\n{month_pivot[['Cluster']]}")
    except Exception as e:
        st.error(f"‚ùå Clustering failed: {e}")

else:
    # Safe slider only when we have ‚â• 3 years
    max_k = min(10, num_years)
    default_k = min(3, max_k)
    k = st.slider(
        "Select K (clusters)",
        min_value=2,
        max_value=max_k,
        value=default_k,
        step=1,
        help="Choose number of clusters (‚â§ number of years)"
    )

    try:
        km = KMeans(n_clusters=k, random_state=42)
        month_pivot["Cluster"] = km.fit_predict(month_pivot)
        st.dataframe(month_pivot)
        st.write(f"[DEBUG] Cluster assignments:\n{month_pivot[['Cluster']]}")
    except Exception as e:
        st.error(f"‚ùå Clustering failed: {e}")

# ================================================================
# üî• HEATMAP
# ================================================================

st.subheader("üî• Heatmap ‚Äî Month √ó Year")

# Pivot table for heatmap
heat = df_tr.pivot_table(index="year", columns="month", values="value", aggfunc="sum").fillna(0)

# DEBUG: print the pivot table
print("[DEBUG] Heatmap pivot table:")
print(heat)

# Create heatmap
fig_h = go.Figure(
    data=go.Heatmap(
        z=heat.values,
        x=heat.columns,
        y=heat.index,
        colorscale="Viridis",
        hovertemplate="Year: %{y}<br>Month: %{x}<br>Registrations: %{z}<extra></extra>"
    )
)
fig_h.update_layout(
    title="Heatmap of Registrations",
    xaxis_title="Month",
    yaxis_title="Year"
)
st.plotly_chart(fig_h, use_container_width=True)


# ================================================================
# ‚úÖ SUMMARY
# ================================================================

st.markdown("---")

total_records = len(df_tr)
duration_start = df_tr['year'].min()
duration_end = df_tr['year'].max()
peak_value = df_tr['value'].max()

# DEBUG prints
print(f"[DEBUG] Total Records: {total_records}")
print(f"[DEBUG] Duration: {duration_start} ‚Üí {duration_end}")
print(f"[DEBUG] Peak Value: {peak_value:,}")

st.markdown(f"**Total Records:** {total_records:,}")
st.markdown(f"**Duration:** {duration_start} ‚Üí {duration_end}")
st.markdown(f"**Peak Value:** {peak_value:,.0f}")
st.success("‚úÖ All modules executed successfully ‚Äî VAHAN ALL-MAXED ready!")


# ---------- Forecasting & Anomalies -------------------------------------------
# --- Ensure required variables exist ---
import pandas as pd
import numpy as np
import streamlit as st
import math

# -------------------------
# Permanently enable ML
# -------------------------
enable_ml = True

# -------------------------
# Ensure historical data exists
# -------------------------
if "df_tr" not in globals() or df_tr is None or df_tr.empty:
    # Create minimal synthetic timeseries
    dates = pd.date_range("2023-01-01", periods=12, freq="M")
    values = np.random.randint(1000, 5000, size=len(dates))
    df_tr = pd.DataFrame({"date": dates, "value": values})

print(f"[DEBUG] df_tr head:\n{df_tr.head()}")
print(f"[DEBUG] Total rows: {len(df_tr)}")
print(f"[DEBUG] Date range: {df_tr['date'].min()} ‚Üí {df_tr['date'].max()}")
print(f"[DEBUG] Value stats: min={df_tr['value'].min()}, max={df_tr['value'].max()}, mean={df_tr['value'].mean():.2f}")

freq_map = {"M": "MS", "Y": "YS"}
frequency = "M"

def lazy(pkg):
    try:
        __import__(pkg)
        print(f"[DEBUG] Package '{pkg}' is available.")
        return True
    except ImportError:
        print(f"[DEBUG] Package '{pkg}' not installed.")
        return None

try:
    from prophet import Prophet
    prophet_mod = Prophet
    print("[DEBUG] Prophet module loaded successfully.")
except Exception:
    prophet_mod = None
    print("[DEBUG] Prophet module not available; fallback only.")

# ---------- Forecasting & Anomaly Detection ----------
if enable_ml and not df_tr.empty:
    st.subheader("üìä Forecasting & Anomaly Detection ‚Äî ALLL-MAXED")

    fc_col1, fc_col2 = st.columns([2,3])
    with fc_col1:
        method = st.selectbox(
            "Forecast method",
            ["Naive seasonality", "SARIMAX", "Prophet", "RandomForest", "XGBoost"],
            key="forecast_method_maxed"
        )
        horizon = st.number_input("Forecast horizon (periods)", 1, 60, 12, key="forecast_horizon_maxed")
    with fc_col2:
        st.info("Auto-shows all visuals & stats. Methods run only if their packages are available.")

    ts = df_tr.set_index("date")["value"].astype(float)
    freq = freq_map.get(frequency, "M")

    print(f"[DEBUG] Forecast selected: {method}, horizon: {horizon}")
    print(f"[DEBUG] Time series head:\n{ts.head()}")

    # ---- FORECAST ----
    if st.button("Run Forecast (ALLL-MAXED)", key="run_forecast_allmaxed"):
        st.markdown("### üîÆ Forecast Results")
        fc = None
        idx = pd.date_range(start=ts.index[-1] + pd.offsets.MonthBegin(1), periods=horizon, freq=freq)

        # Forecast logic
        if method == "Naive seasonality":
            last = ts[-12:] if len(ts) >= 12 else ts
            preds = np.tile(last.values, int(np.ceil(horizon / len(last))))[:horizon]
            fc = pd.Series(preds, index=idx)
            print(f"[DEBUG] Naive seasonality forecast values:\n{fc}")

        elif method == "SARIMAX" and lazy("statsmodels"):
            from statsmodels.tsa.statespace.sarimax import SARIMAX
            model = SARIMAX(ts, order=(1,1,1), seasonal_order=(1,1,1,12))
            res = model.fit(disp=False)
            fc = pd.Series(res.get_forecast(steps=horizon).predicted_mean, index=idx)
            print(f"[DEBUG] SARIMAX forecast head:\n{fc.head()}")

        elif method == "Prophet" and lazy("prophet") and prophet_mod:
            pdf = ts.reset_index().rename(columns={"date": "ds", "value": "y"})
            m = prophet_mod()
            m.fit(pdf)
            future = m.make_future_dataframe(periods=horizon, freq="M")
            fc = m.predict(future).set_index("ds")["yhat"].tail(horizon)
            print(f"[DEBUG] Prophet forecast head:\n{fc.head()}")

        elif method == "RandomForest" and lazy("sklearn"):
            from sklearn.ensemble import RandomForestRegressor
            df_feat = pd.DataFrame({"y": ts})
            for l in range(1,13):
                df_feat[f"lag_{l}"] = df_feat["y"].shift(l)
            df_feat = df_feat.dropna()
            X = df_feat.drop(columns=["y"]).values
            y_arr = df_feat["y"].values
            model = RandomForestRegressor(n_estimators=200, random_state=42).fit(X, y_arr)
            last = df_feat.drop(columns=["y"]).iloc[-1].values
            preds, cur = [], last.copy()
            for _ in range(horizon):
                p = model.predict(cur.reshape(1,-1))[0]
                preds.append(p)
                cur = np.roll(cur, 1)
                cur[0] = p
            fc = pd.Series(preds, index=idx)
            print(f"[DEBUG] RandomForest forecast head:\n{fc.head()}")

        elif method == "XGBoost" and lazy("xgboost"):
            import xgboost as xgb
            df_feat = pd.DataFrame({"y": ts})
            for l in range(1,13):
                df_feat[f"lag_{l}"] = df_feat["y"].shift(l)
            df_feat = df_feat.dropna()
            X = df_feat.drop(columns=["y"])
            y_arr = df_feat["y"]
            dtrain = xgb.DMatrix(X, label=y_arr)
            bst = xgb.train({"objective": "reg:squarederror"}, dtrain, num_boost_round=200)
            last = X.iloc[-1].values
            preds, cur = [], last.copy()
            for _ in range(horizon):
                dcur = xgb.DMatrix(cur.reshape(1,-1))
                p = bst.predict(dcur)[0]
                preds.append(p)
                cur = np.roll(cur,1)
                cur[0] = p
            fc = pd.Series(preds, index=idx)
            print(f"[DEBUG] XGBoost forecast head:\n{fc.head()}")

        if fc is not None and not fc.empty:
            combined = pd.concat([ts, fc])
            st.line_chart(combined)
            st.metric("Forecast Mean", round(fc.mean(), 2))
            st.metric("Forecast Std", round(fc.std(), 2))
            st.metric("Forecast Start", str(fc.index[0].date()))
            st.metric("Forecast End", str(fc.index[-1].date()))
            with st.expander("üìà Forecast Data Summary"):
                st.dataframe(fc.describe().to_frame("Forecast Summary"))

    # ---- ANOMALY DETECTION ----
    st.markdown("### ‚ö†Ô∏è Anomaly Detection")
    a_method = st.selectbox(
        "Anomaly method",
        ["Z-score", "IQR", "IsolationForest"],
        key="anom_method_allmaxed"
    )

    if st.button("Run Anomaly Detection (ALLL-MAXED)", key="run_anom_allmaxed"):
        if a_method == "Z-score":
            z = (ts - ts.mean()) / ts.std()
            anoms = z.abs() > 3
        elif a_method == "IQR":
            q1, q3 = ts.quantile(0.25), ts.quantile(0.75)
            iqr = q3 - q1
            anoms = (ts < q1 - 1.5*iqr) | (ts > q3 + 1.5*iqr)
        elif a_method == "IsolationForest" and lazy("sklearn"):
            from sklearn.ensemble import IsolationForest
            iso = IsolationForest(random_state=0).fit(ts.values.reshape(-1,1))
            preds = iso.predict(ts.values.reshape(-1,1))
            anoms = preds == -1
        else:
            anoms = pd.Series(False, index=ts.index)

        out = ts[anoms]
        print(f"[DEBUG] Anomalies detected: {out.shape[0]}")
        print(f"[DEBUG] Anomaly values:\n{out}")
        st.metric("Anomalies Detected", out.shape[0])
        st.line_chart(ts)
        if not out.empty:
            st.scatter_chart(out)
            with st.expander("üîç Anomaly Data"):
                st.dataframe(out)

# ---------------------------------------------------

# # ---------- RAG / LLM + vector DB (FAISS or fallback) -------------------------------------
# if enable_rag:
#     st.subheader("üß† RAG (Retrieval-Augmented Generation) + Vector Index")

#     docs = []

#     # Collect all docs from current dataframes
#     if not df_cat.empty:
#         docs += (df_cat["label"].astype(str) + " :: " + df_cat["value"].astype(str)).tolist()
#     if not df_mk.empty:
#         docs += (df_mk["label"].astype(str) + " :: " + df_mk["value"].astype(str)).tolist()
#     if not df_tr.empty:
#         docs += [f"{idx.strftime('%Y-%m-%d')} :: {int(v)}" for idx, v in df_tr["value"].items()]

#     if not docs:
#         st.info("‚ÑπÔ∏è No documents available for RAG ‚Äî please fetch data first.")
#     else:
#         st.success(f"‚úÖ Built in-memory corpus with **{len(docs)}** entries")

#         # ---- Embedding step ----
#         with st.spinner("Generating embeddings..."):
#             try:
#                 # Try real embeddings if available
#                 try:
#                     from sentence_transformers import SentenceTransformer
#                     model = SentenceTransformer("all-MiniLM-L6-v2")
#                     emb = model.encode(docs, show_progress_bar=False, convert_to_numpy=True).astype("float32")
#                     st.caption("üß© Using SentenceTransformer embeddings (MiniLM).")
#                 except ImportError:
#                     rng = np.random.default_rng(42)
#                     emb = np.stack([rng.normal(size=768) for _ in docs]).astype("float32")
#                     st.caption("‚öôÔ∏è Using random embeddings (demo mode).")

#                 # ---- Build FAISS or fallback ----
#                 try:
#                     import faiss
#                     faiss.normalize_L2(emb)
#                     index = faiss.IndexFlatIP(emb.shape[1])
#                     index.add(emb)
#                     st.success("üìö FAISS index built successfully.")

#                     def query_rag(query, topk=5):
#                         qv = emb[:1] if not query else model.encode([query]).astype("float32")
#                         faiss.normalize_L2(qv)
#                         D, I = index.search(qv, topk)
#                         return [docs[i] for i in I[0] if i < len(docs)]

#                 except Exception as e:
#                     st.warning(f"‚ö†Ô∏è FAISS not available ‚Äî using naive cosine fallback. ({e})")
#                     import numpy as np
#                     from numpy.linalg import norm

#                     def query_rag(query, topk=5):
#                         if not query:
#                             return docs[:topk]
#                         qv = emb[0] if emb is not None else np.random.normal(size=768)
#                         sims = np.dot(emb, qv) / (norm(emb, axis=1) * norm(qv) + 1e-9)
#                         topk_idx = np.argsort(sims)[::-1][:topk]
#                         return [docs[i] for i in topk_idx]

#             except Exception as e:
#                 st.error(f"‚ùå Error building embeddings or index: {e}")
#                 query_rag = lambda q, topk=5: docs[:topk]

#         # ---- RAG Query UI ----
#         st.markdown("### üîç Ask a Question")
#         q = st.text_input("Enter your question")
#         if st.button("Run RAG Query") and q:
#             with st.spinner("Retrieving relevant information..."):
#                 hits = query_rag(q)
#                 if llm_key:
#                     st.markdown(f"**LLM Answer (mock)** for: `{q}`")
#                     st.info(f"ü§ñ Would call LLM API here with top-{len(hits)} docs.")
#                 else:
#                     st.markdown("**Retrieved Documents (Top 5)**")
#                     st.write("\n---\n".join(hits))



# # ---------- NLP tools hooks ----------------------------------------------------
# if enable_nlp:
#     st.subheader('NLP tools (nltk / spacy)')
#     st.info('This will lazy-load NLP packages and run simple tokenization / NER demo')
#     if nltk is None or spacy is None:
#         st.warning('nltk or spacy not installed; install them to run NLP demos')
#     else:
#         st.write('Running simple demo...')
#         # simple tokenization demo
#         text = st.text_area('Text for NLP demo', 'The quick brown fox jumps over the lazy dog. New Delhi saw 10000 registrations in Jan 2024.')
#         import nltk
#         nltk.download('punkt', quiet=True)
#         from nltk.tokenize import word_tokenize
#         toks = word_tokenize(text)
#         st.write('Tokens:', toks)
#         import spacy
#         nlp = spacy.load('en_core_web_sm')
#         doc = nlp(text)
#         st.write('Entities:', [(ent.text, ent.label_) for ent in doc.ents])

# # =====================================================
# # üß† NLP ANALYZER ‚Äî ALL-MAXED ULTIMATE FUSION LAB (SAFE)
# # =====================================================
# enable_nlp = st.checkbox("üó£Ô∏è Enable NLP Analyzer (ALL-MAXED ULTIMATE SAFE)", False, key="nlp_toggle")

# if enable_nlp:
#     import pandas as pd
#     import numpy as np
#     import io, base64, re
#     import matplotlib.pyplot as plt
#     import plotly.express as px
#     from collections import Counter

#     st.markdown("## üß† NLP Analyzer ‚Äî ALL-MAXED ULTIMATE (SAFE MODE)")

#     # --- Try importing optional NLP libs safely
#     def safe_import(module_name, install_name=None):
#         try:
#             return __import__(module_name)
#         except ModuleNotFoundError:
#             install_name = install_name or module_name
#             st.warning(f"‚ö†Ô∏è `{module_name}` not found ‚Äî attempting auto-install...")
#             import subprocess, sys
#             try:
#                 subprocess.run(
#                     [sys.executable, "-m", "pip", "install", install_name, "--quiet"],
#                     check=True
#                 )
#                 return __import__(module_name)
#             except Exception as e:
#                 st.error(f"‚ùå Failed to import {module_name}: {e}")
#                 return None

#     nltk = safe_import("nltk")
#     spacy = safe_import("spacy")
#     seaborn = safe_import("seaborn")
#     from wordcloud import WordCloud
#     from openpyxl import Workbook
#     from openpyxl.utils.dataframe import dataframe_to_rows

#     if nltk:
#         nltk.download("punkt", quiet=True)
#         nltk.download("averaged_perceptron_tagger", quiet=True)
#         nltk.download("vader_lexicon", quiet=True)
#         from nltk.tokenize import word_tokenize
#         from nltk.sentiment import SentimentIntensityAnalyzer
#         sia = SentimentIntensityAnalyzer()
#     else:
#         word_tokenize = lambda x: x.split()
#         sia = None

#     # --- Input Section
#     model_choice = st.selectbox("Choose NLP Engine", ["spaCy", "NLTK", "None (Fallback)"], index=0)
#     text_input = st.text_area("üìù Enter text for full analysis:",
#                               "Maharashtra saw record EV registrations in 2024.")
#     if not text_input.strip():
#         st.stop()

#     # --- Load spaCy if available
#     if spacy:
#         try:
#             nlp = spacy.load("en_core_web_sm")
#         except OSError:
#             from spacy.cli import download
#             download("en_core_web_sm")
#             nlp = spacy.load("en_core_web_sm")
#     else:
#         nlp = None

#     # --- Core NLP Analysis
#     toks = word_tokenize(text_input) if nltk else text_input.split()
#     pos_tags = nltk.pos_tag(toks) if nltk else []
#     sent_score = sia.polarity_scores(text_input) if sia else {"compound": 0, "pos": 0, "neg": 0, "neu": 1}

#     if nlp:
#         doc = nlp(text_input)
#         ents = [(ent.text, ent.label_) for ent in doc.ents]
#         keywords = [tok.text.lower() for tok in doc if tok.is_alpha and not tok.is_stop]
#     else:
#         ents = []
#         keywords = [t.lower() for t in toks if len(t) > 3]

#     top_kw = Counter(keywords).most_common(15)
#     st.metric("Sentiment (compound)", f"{sent_score['compound']:.3f}")
#     st.write("üîë Keywords:", top_kw)
#     st.write("üß© Entities:", ents if ents else "‚Äî None ‚Äî")

#     # --- WordCloud
#     wc = WordCloud(width=600, height=300, background_color="white").generate(" ".join(keywords))
#     fig, ax = plt.subplots()
#     ax.imshow(wc, interpolation="bilinear")
#     ax.axis("off")
#     st.pyplot(fig)

#     # --- Summary DataFrame
#     df_summary = pd.DataFrame([{
#         "Tokens": len(toks),
#         "Sentiment": sent_score["compound"],
#         "Pos": sent_score["pos"],
#         "Neg": sent_score["neg"],
#         "Neu": sent_score["neu"],
#         "TopKeywords": ", ".join([k for k, _ in top_kw])
#     }])
#     st.dataframe(df_summary)

#     # --- Charts
#     if seaborn:
#         import seaborn as sns
#         fig, ax = plt.subplots()
#         sns.heatmap(df_summary[["Sentiment", "Pos", "Neg", "Neu"]].T, cmap="RdYlGn", annot=True, ax=ax)
#         st.pyplot(fig)

#     fig_kw = px.bar(pd.DataFrame(top_kw, columns=["Keyword", "Frequency"]),
#                     x="Keyword", y="Frequency", title="Keyword Frequency")
#     st.plotly_chart(fig_kw, use_container_width=True)

#     # --- Export
#     wb = Workbook()
#     ws = wb.active
#     ws.title = "Summary"
#     for r in dataframe_to_rows(df_summary, index=False, header=True):
#         ws.append(r)
#     buf = io.BytesIO()
#     wb.save(buf)
#     buf.seek(0)

#     st.download_button("üì• Download Excel",
#                        data=buf,
#                        file_name="NLP_AllMaxed_Safe.xlsx",
#                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

#     st.download_button("üß† Download JSON",
#                        data=df_summary.to_json(orient="records", indent=2),
#                        file_name="nlp_allmaxed_safe.json",
#                        mime="application/json")

#     st.success("‚úÖ NLP Analyzer fully loaded ‚Äî all maxed (safe mode)!")


# # ---------- Exports & comparisons ------------------------------------------------
# st.subheader('Exports & Comparisons')

# # Safe download for trend
# if "df_tr" in globals() and not df_tr.empty:
#     st.download_button('Download Trend CSV', df_tr.reset_index().to_csv(index=False), 'trend.csv')

# # Safe download for categories
# if "df_cat_all" in globals() and not df_cat_all.empty:
#     st.download_button('Download Categories CSV', df_cat_all.to_csv(index=False), 'categories.csv')

# # Safe download for makers
# if "df_maker_all" in globals() and not df_maker_all.empty:
#     st.download_button('Download Makers CSV', df_maker_all.to_csv(index=False), 'makers.csv')



# # ---------- Footer & next steps ------------------------------------------------
# st.markdown('---')
# st.caption('Ultra  V2 ‚Äî build: ' + datetime.utcnow().strftime('%Y-%m-%d %H:%M UTC'))


# # ---------- Exports & comparisons ------------------------------------------------
# st.subheader('üíæ Exports & Comparisons')

# if not df_tr.empty:
#     st.download_button('Download Trend CSV', df_tr.reset_index().to_csv(index=False), 'trend.csv')
# if 'df_cat' in globals() and not df_cat.empty:
#     st.download_button('Download Categories CSV', df_cat.to_csv(index=False), 'categories.csv')
# if 'df_mk' in globals() and not df_mk.empty:
#     st.download_button('Download Makers CSV', df_mk.to_csv(index=False), 'makers.csv')

# =====================================================
# üöÄ ALL-MAXED Final Dashboard Footer with Debug
# =====================================================
try:
    import time
    from datetime import datetime

    # Track runtime if app_start_time exists
    runtime = ""
    if "app_start_time" in globals():
        try:
            runtime_secs = time.time() - app_start_time
            runtime = f" ‚Ä¢ Runtime: {runtime_secs:,.1f}s"
            print(f"[ALL-MAXED DEBUG] App runtime: {runtime_secs:.1f}s")
        except Exception as e:
            print(f"[ALL-MAXED DEBUG] Runtime computation failed: {e}")
            runtime = ""

    # Count loaded DataFrames
    df_count = sum(1 for v in globals().values() if isinstance(v, pd.DataFrame))
    df_text = f" ‚Ä¢ DataFrames: {df_count}" if df_count else ""
    print(f"[ALL-MAXED DEBUG] Number of DataFrames loaded: {df_count}")

    # Current UTC timestamp
    footer_ts = datetime.utcnow().strftime("%Y-%m-%d %H:%M UTC")
    print(f"[ALL-MAXED DEBUG] Footer timestamp (UTC): {footer_ts}")

    # Render ALL-MAXED gradient footer
    st.markdown(f"""
    <div style="
        text-align:center;
        padding:16px;
        border-radius:14px;
        background:linear-gradient(90deg,#051937,#004d7a,#008793,#00bf72,#a8eb12);
        color:white;
        margin-top:18px;
        box-shadow:0 0 12px rgba(0,0,0,0.4);
    ">
        <h3 style="margin:0;">üöó Parivahan Analytics ‚Äî ALL-MAXED DASHBOARD</h3>
        <div style="opacity:0.95;font-size:14px;">
            Snapshot: <code>{footer_ts}</code>{runtime}{df_text} ‚Ä¢ Built with ‚ù§Ô∏è & ‚öôÔ∏è Streamlit ‚Ä¢ ALLLL MAXED ‚úÖ
        </div>
    </div>
    """, unsafe_allow_html=True)

    st.caption("üí° Tip: Use the exported CSVs or ZIP snapshot for polished reports and full archival backups.")

    # Optional celebratory balloons
    try:
        st.balloons()
        print("[ALL-MAXED DEBUG] Balloons triggered!")
    except Exception as e:
        print(f"[ALL-MAXED DEBUG] Balloons failed: {e}")

    print("[ALL-MAXED DEBUG] Dashboard fully loaded and ALLLL MAXED ‚úÖ")

except Exception as e:
    st.warning(f"Footer rendering failed: {e}")
    print(f"[ALL-MAXED DEBUG] Footer exception: {e}")
